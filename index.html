<!DOCTYPE html>
<html lang="en">
    <head>
        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-116076474-3"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-116076474-3');
        </script>
        <title>Yung-Sung Chuang</title>
        <meta http-equiv="content-type" content="text/html; charset=UTF-8">
        <meta charset="utf-8">
        <meta property="og:url" content="https://people.csail.mit.edu/yungsung/" />
	    <meta property="og:title" content="Yung-Sung Chuang" />
	    <meta property="og:image" content="img/Yung-Sung.jpg" />
	    <meta http-equiv="X-UA-Compatible" content="IE=edge">
	    <meta name="author" content="Yung-Sung Chuang">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
        <link rel="shortcut icon" type="image/png" href="favicon.ico"/>

        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
        <link rel="stylesheet" href="css/style.css">
        <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
        <link href='https://fonts.googleapis.com/css?family=EB+Garamond' rel='stylesheet' type='text/css'>
        <style>
            /* Add some padding on document's body to prevent the content
            to go underneath the header and footer */
            body{        
                padding-top: 50px;
                padding-bottom: 0px;
            }
            html {
                scroll-padding-top: 70px; /* height of sticky header */
            }
            .container{
                width: 80%;
                margin: 0 auto; /* Center the DIV horizontally */
            }
            .fixed-header, .fixed-footer{
                width: 100%;
                position: fixed;        
                /* background: rgb(69, 142, 226); */
                background: rgb(126, 103, 77);
                padding: 7px 0;
                color: #fff;
                font-size: calc(6px + 0.78125vw);
            }
            .fixed-header{
                top: 0;
            }
            .fixed-footer{
                bottom: 0;
            }    
            /* Some more styles to beutify this example (85, 129, 212) */
            a {
                color: rgb(148, 95, 37);
            }
            a:hover {
                color: rgb(9, 67, 17);
            }
            .emph {
                color: rgb(23, 111, 23) !important;
            }
            nav a{
                color: #fff;
                text-decoration: none;
                padding: 7px 1.7vw;
                display: inline-block;
            }
            h1 span {
                font-size: 20pt;
            }
            h1 span a{
                font-size: 12pt;
            }
            /* set the font color of <a href...> to be brown */
        </style>
    </head>
    <body>
        <div class="fixed-header" style="z-index:1;">
            <div class="container">
                <nav>
                    <a href="#">Home</a>
                    <a href=#talks>Talks</a>
                    <a href=#publications>Publications</a>
                    <a href=#honors>Honors</a>
                    <a href=#services>Services</a>
                    <a href="https://voidism.github.io/">Blog</a>
                    <!-- <a href=#services>Services</a> -->
                    <!-- <a href="https://www.csail.mit.edu/person/yung-sung-chuang" style="float:right;"><i>@csail.mit</i></a> -->
                </nav>
            </div>
        </div>
        <div class="container mt-5" style="z-index:0;">
            <div class="row">
                <div class="col-lg-4 col-md-6 order-1 order-xs-1 order-sm-1 order-md-0 order-lg-0">
                <!-- <div class="col-lg-4 col-md-6 order-0 order-xs-0 order-sm-0 order-md-1 order-lg-1"> -->
                    <div class="card mb-3">
                        <img class="card-img-top" src="img/Yung-Sung.jpg" alt="Yung-Sung Chuang">
                        <div class="card-body">
                            <h5 class="card-title">
                                <b>Yung-Sung Chuang</b>
                            </h5>
                            <p class="card-text">
                                MIT EECS PhD Student @ CSAIL
                            </br>
                            Office: 32-G436
                        </p>
                    </div>
                </div>
            </div>
            <!-- <div class="col-lg-8 col-md-6 order-1 order-xs-1 order-sm-1 order-md-0 order-lg-0"> -->
                <div class="col-lg-8 col-md-6 order-0 order-xs-0 order-sm-0 order-md-1 order-lg-1">
                    <!-- <p>▶ [<a href=#publications>Publications</a>] [<a href=#talks>Talks</a>] [<a href=#projects>Projects</a>] [<a href=#honors>Honors</a>] [<a href=#services>Services</a>] </p> -->
                    
                    <div class="row mb-3">
                        <div class="col">
                            <h1>Yung-Sung Chuang <span>(莊永松) <a href="#" onclick="$('#my_name').toggle();return false;"><i>How to pronounce?</i></a></span></h1>
                        </div>
                    </div>
                    <div id="my_name" class="abstract" style="display:none;">
                        <p style="color:gray">
                            My first name Yung-Sung (永松) should be pronounced as "yong song". My last name Chuang (莊) should be pronounced as "jwang". In my first name, Yung means "forever" and Sung means "pine tree", so it has the meaning of "long-lasting" or "longevity".
                            </br>
                            <audio controls>
                                <source src="files/name-tts.mp3" type="audio/mpeg">
                            </audio>
                        </p>
                    </div>
                    <p>
                        Hi! I'm a fourth-year PhD student in <a href="https://www.eecs.mit.edu/" target="_blank">Electrical Engineering and Computer Science</a> at
                        <a href="https://www.mit.edu" target="_blank">Massachusetts Institute of Technology</a>, where I work with <a href="https://www.csail.mit.edu/person/jim-glass"
                        target="_blank">Jim Glass</a> at <a href="https://www.csail.mit.edu/" target="_blank">CSAIL</a>.
                    </p>
                    <p>
                        My research primarily focuses on natural language processing and large language models (LLMs), 
                        with a particular interest in improving their factuality and reliability. 
                        In <a href="https://arxiv.org/abs/2309.03883" target="_blank">DoLa</a>, 
                        we proposed a decoding strategy that enhances LLM factuality by contrasting the knowledge 
                        across different transformer layers.
                        In <a href="https://arxiv.org/abs/2407.07071" target="_blank">Lookback Lens</a>, 
                        we introduced a method that detects and mitigates contextual hallucinations in LLMs 
                        using attention maps. 
                        Most recently, in <a href="https://arxiv.org/abs/2502.09604" target="_blank">SelfCite</a>, 
                        we developed a self-supervised framework that enables LLMs to generate fine-grained, sentence-level citations by leveraging context ablation as a reward signal.
                    </p>
                    <p>
                        I also explore retrieval-based approaches to strengthen LLM by grounding answers
                        in real documents. For instance, in <a href="https://arxiv.org/abs/2305.17080" target="_blank">Expand, Rerank, and Retrieve</a>, 
                        we proposed query reranking to achieve more accurate
                        retrieval results for open-domain QA. In <a href="https://arxiv.org/abs/2204.10298" target="_blank">DiffCSE</a>, 
                        we built a contrastive learning method based on the differences between 
                        similar sentences to further boost the quality of sentence embeddings.
                    </p>
                    <p>
                        I was fortunate to intern at FAIR Meta, Microsoft, and MIT-IBM Watson AI Lab.
                        Before joining MIT, I was an undergraduate student in Electrical Engineering
                        at National Taiwan University, where I worked with 
                        <a href="https://speech.ee.ntu.edu.tw/~hylee/index.html">Hung-Yi Lee</a>,
                        <a href="https://www.csie.ntu.edu.tw/~yvchen/">Yun-Nung (Vivian) Chen</a>,
                        and <a href="https://speech.ee.ntu.edu.tw/previous_version/lslNew.htm">Lin-shan Lee</a>.
                        Here is my <a href="https://people.csail.mit.edu/yungsung/files/CV.pdf">Curriculum Vitae</a>.
                    </p>
                </div>
            </div>
            <div class="row">
                <div class="col">
                    <p>
                        Email: yungsung [AT] mit.edu
                    </p>
                    <p>
                        Links:
                        [<a href="files/CV.pdf" target="_blank">CV</a>] [<a href="https://twitter.com/YungSungChuang" target="_blank">Twitter</a>] [<a href="https://github.com/voidism" target="_blank">Github</a>] [<a href="https://scholar.google.com/citations?hl=en&user=3ar1DOwAAAAJ" target="_blank">Google Scholar</a>] [<a href="https://dblp.org/pers/hd/c/Chuang:Yung=Sung" target="_blank">DBLP</a>] [<a href="https://voidism.github.io/" target="_blank">Blog</a>] [<a href="https://www.linkedin.com/in/yschuang">Linkedin</a>] [<a href="https://www.instagram.com/yungsung.chuang/">Instagram</a>]
                    </p>
                </div>
            </div>
            <div class="row">
                <div class="col">
                    <h2>Recent News</h2>
                    <ul>
                        <li>
                          (05/2025) Start my summer internship at FAIR, Meta with <a href="https://scholar.google.com/citations?user=UjpbO6IAAAAJ&hl=en" target="_blank">Luke Zettlemoyer</a>. <br/>
                        </li>
                        <li>
                          (04/2025) My recent talk at MIT EI Seminar <a href="https://youtu.be/oEFRH80LndM?si=uM_lb2sUm1NVmezT">"Reducing Hallucinations in LLMs via Decoding, Detection, and Citation"</a> is on Youtube! 
                        </li>
                        <li>
                          (05/2024) Start my summer internship at FAIR, Meta with <a href="https://howardhsu.github.io/" target="_blank">Hu Xu</a>, <a href="https://swdanielli.github.io/" target="_blank">Daniel Li</a>, <a href="https://scottyih.org/" target="_blank">Scott Yih</a>. See our <a href="https://arxiv.org/abs/2507.22062">MetaCLIP 2 paper</a>. <br/>
                        </li>
                        <li>
                          (06/2023) Start my summer internship at Microsoft with <a href="https://scholar.google.com/citations?user=TS1RoxAAAAAJ&hl=en" target="_blank">Pengcheng He</a>, <a href="https://sites.google.com/view/yujia" target="_blank">Yujia Xie</a>. See our <a href="https://arxiv.org/abs/2309.03883">DoLa paper</a>. <br/>
                        </li>
                        <li>
                          (06/2022) Start my summer internship at MIT-IBM Watson AI Lab with <a href="https://mitibmwatsonailab.mit.edu/people/yang-zhang/" target="_blank">Yang Zhang</a>, <a href="https://mitibmwatsonailab.mit.edu/people/shiyu-chang/" target="_blank">Shiyu Chang</a>, <a href="https://people.csail.mit.edu/yoonkim/" target="_blank">Yoon Kim</a>, <a href="https://scholar.google.com/citations?user=uEpr4C4AAAAJ&hl=en" target="_blank">Kaizhi Qian</a> <br/>
                        </li>
                        <!-- <li>
                            (04/2022) Our paper <a href="https://arxiv.org/abs/2204.10298" target="_blank">"DiffCSE: Difference-based Contrastive Learning for Sentence Embeddings"</a> is accepted by NAACL 2022 as a oral paper! <br/>
                        </li> -->
                        <!-- <li>
                            (03/2022) I am serving as a reviewer for <a href="https://nips.cc/Conferences/2022" target="_blank">NeurIPS 2022</a>. <br/>
                        </li>
                        <li>
                            (03/2022) I am serving on the Program Committee for <a href="https://www.workshopononlineabuse.com/" target="_blank">The Sixth Workshop on Online Abuse and Harms in NAACL 2022</a>. <br/>
                        </li>
                        <li>
                            (01/2022) I am serving as a reviewer for <a href="https://icml.cc/Conferences/2022" target="_blank">ICML 2022</a>. <br/>
                        </li> -->
                        <!-- <li>
                            (09/2021) Our paper <a href="https://arxiv.org/abs/2106.05933" target="_blank">"PARP: Prune, Adjust and Re-Prune for Self-Supervised Speech Recognition"</a> is accepted by NeurIPS 2021 as a spotlight paper! <br/>
                        </li> -->
                        <li>
                            (09/2021) Start my PhD life in Cambridge, MA!
                        </li>
                        <!-- <li>
                            (06/2021) I am serving as a reviewer for <a href="https://iclr.cc/Conferences/2022" target="_blank">ICLR 2022</a>. <br/>
                        </li>
                        <li>
                            (04/2021) I am serving as a reviewer for <a href="https://nips.cc/Conferences/2021" target="_blank">NeurIPS 2021</a>. <br/>
                        </li> -->
                    </ul>
                </div>
            </div>
            <hr>
            <div class="row" id="talks">
                <div class="col">
                    <h2>Talks</h2>
                    <iframe width="560" height="315" src="https://www.youtube.com/embed/oEFRH80LndM?si=nuyL3AsfZxjw4Tf3" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
                    <p>
                        <b>Reducing Hallucinations in LLMs via Decoding, Detection, and Citation</b>
                        <br/>
                        <i>MIT CSAIL: EI Seminar, April 24, 2025</i>
                    </p>
                    <p><i>For my other talks at the conferences, please see <a href="https://www.youtube.com/@yung-sung">https://www.youtube.com/@yung-sung</a></i></p>
                </div>
            </div>
                    
            <div class="row" id="publications">
                <div class="col">
                    <h2>Publications</h2>
                    <p><i>For a full list of papers, see my <a href="https://scholar.google.com/citations?hl=en&user=3ar1DOwAAAAJ" target="_blank">Google Scholar</a>.</i></p>
                    <h3>2025</h3>
                    <ul class="pl">
                        <li>
                            <a href="https://arxiv.org/abs/2502.09604" target="_blank">
                                <b>SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models</b>
                            </a>
                            <br/>
                            <b>Yung-Sung Chuang</b>,
                            <a href="https://bencw99.github.io/" target="_blank">Benjamin Cohen-Wang</a>,
                            <a href="https://www.szj.io/" target="_blank">Shannon Zejiang Shen</a>,
                            <a href="https://zhaofengwu.github.io/" target="_blank">Zhaofeng Wu</a>,
                            <a href="https://howardhsu.github.io/" target="_blank">Hu Xu</a>,
                            <a href="https://victorialin.org/" target="_blank">Xi Victoria Lin</a>,
                            <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>,
                            <a href="https://swdanielli.github.io/" target="_blank">Shang-Wen Li</a>,
                            <a href="https://scottyih.org/" target="_blank">Wen-tau Yih</a>
                            <br/>
                            To appear in <b>ICML</b>, 2025.
                            <br/>
                            [<a href="https://selfcite.github.io" target="_blank">blog</a>]
                            [<a href="bibs/chuang2025selfcite.bib" target="_blank" type="text/plain">bib</a>]
                            [<a href="#" onclick="$('#selfcite_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://arxiv.org/pdf/2502.09604.pdf" target="_blank">pdf</a>]
                            [<a href="https://github.com/facebookresearch/SelfCite" target="_blank">code</a>]
                            <div id="selfcite_abstract" class="abstract" style="display:none;">
                                <p>
                                    We introduce <i>SelfCite</i>, a novel self-supervised approach that aligns LLMs to generate high-quality, fine-grained, sentence-level citations for the statements in their generated responses. Instead of only relying on costly and labor-intensive annotations, SelfCite leverages a reward signal provided by the LLM itself through context ablation: If a citation is necessary, removing the cited text from the context should prevent the same response; if sufficient, retaining the cited text alone should preserve the same response. This reward can guide the inference-time best-of-N sampling strategy to improve citation quality significantly, as well as be used in preference optimization to directly fine-tune the models for generating better citations. The effectiveness of SelfCite is demonstrated by increasing citation F1 up to 5.3 points on the LongBench-Cite benchmark across five long-form question answering tasks.
                                </p>
                            </div>
                        </li>
                    </ul>

                    <ul class="pl">
                        <li>
                            <a href="https://arxiv.org/abs/2507.22062" target="_blank">
                                <b>MetaCLIP 2: A Worldwide Scaling Recipe</b>
                            </a>
                            <br/>
                            <b>Yung-Sung Chuang</b>,
                            Yang Li,
                            Dong Wang,
                            <a href="https://scholar.google.com/citations?user=P7ma7pAAAAAJ&hl=en" target="_blank">Ching-Feng Yeh</a>,
                            Kehan Lyu,
                            Ramya Raghavendra,
                            <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>,
                            Lifei Huang,
                            <a href="https://scholar.google.com/citations?user=lMkTx0EAAAAJ&hl=en" target="_blank">Jason Weston</a>,
                            <a href="https://www.cs.washington.edu/people/faculty/lsz" target="_blank">Luke Zettlemoyer</a>,
                            <a href="https://xinleic.xyz/" target="_blank">Xinlei Chen</a>,
                            <a href="https://liuzhuang13.github.io/" target="_blank">Zhuang Liu</a>,
                            <a href="https://www.sainingxie.com/" target="_blank">Saining Xie</a>,
                            <a href="https://scottyih.org/" target="_blank">Wen-tau Yih</a>,
                            <a href="https://swdanielli.github.io/" target="_blank">Shang-Wen Li</a>,
                            <a href="https://howardhsu.github.io/" target="_blank">Hu Xu</a>
                            <br/>
                            arXiv preprint, 2025.
                            <br/>

                            [<a href="bibs/chuang2025metaclip2.bib" target="_blank" type="text/plain">bib</a>]
                            [<a href="#" onclick="$('#metaclip2_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://arxiv.org/pdf/2507.22062.pdf" target="_blank">pdf</a>]
                            [<a href="https://github.com/facebookresearch/MetaCLIP" target="_blank">code</a>]
                            <div id="metaclip2_abstract" class="abstract" style="display:none;">
                                <p>
                                    Contrastive Language-Image Pretraining (CLIP) is a popular foundation model, supporting from zero-shot classification, retrieval to encoders for multimodal large language models (MLLMs). Although CLIP is successfully trained on billion-scale image-text pairs from the English world, scaling CLIP's training further to learning from the worldwide web data is still challenging: (1) no curation method is available to handle data points from non-English world; (2) the English performance from existing multilingual CLIP is worse than its English-only counterpart, i.e., "curse of multilinguality" that is common in LLMs. Here, we present Meta CLIP 2, the first recipe training CLIP from scratch on worldwide web-scale image-text pairs. To generalize our findings, we conduct rigorous ablations with minimal changes that are necessary to address the above challenges and present a recipe enabling mutual benefits from English and non-English world data. In zero-shot ImageNet classification, Meta CLIP 2 ViT-H/14 surpasses its English-only counterpart by 0.8% and mSigLIP by 0.7%, and surprisingly sets new state-of-the-art without system-level confounding factors (e.g., translation, bespoke architecture changes) on multilingual benchmarks, such as CVQA with 57.4%, Babel-ImageNet with 50.2% and XM3600 with 64.3% on image-to-text retrieval.
                                </p>
                            </div>
                        </li>
                    </ul>

                    <ul class="pl">
                        <li>
                            <a href="https://arxiv.org/abs/2504.13752" target="_blank">
                                <b>Learning to Attribute with Attention</b>
                            </a>
                            <br/>
                            <a href="https://bencw99.github.io/" target="_blank">Benjamin Cohen-Wang</a>,
                            <b>Yung-Sung Chuang</b>,
                            <a href="https://madry.mit.edu/" target="_blank">Aleksander Madry</a>
                            <br/>
                            arXiv preprint arXiv:2504.13752, 2025.
                            <br/>
                            [<a href="bibs/cohenwang2025learning.bib" target="_blank" type="text/plain">bib</a>]
                            [<a href="#" onclick="$('#at2_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://arxiv.org/pdf/2504.13752.pdf" target="_blank">pdf</a>]
                            [<a href="https://github.com/MadryLab/AT2" target="_blank">code</a>]
                            <div id="at2_abstract" class="abstract" style="display:none;">
                                <p>
                                    Given a sequence of tokens generated by a language model, we may want to identify the preceding tokens that influence the model to generate this sequence. Performing such token attribution is expensive; a common approach is to ablate preceding tokens and directly measure their effects. To reduce the cost of token attribution, we revisit attention weights as a heuristic for how a language model uses previous tokens. Naive approaches to attribute model behavior with attention (e.g., averaging attention weights across attention heads to estimate a token's influence) have been found to be unreliable. To attain faithful attributions, we propose treating the attention weights of different attention heads as features. This way, we can learn how to effectively leverage attention weights for attribution (using signal from ablations). Our resulting method, Attribution with Attention (AT2), reliably performs on par with approaches that involve many ablations, while being significantly more efficient. To showcase the utility of AT2, we use it to prune less important parts of a provided context in a question answering setting, improving answer quality.
                                </p>
                            </div>
                        </li>
                    </ul>

                    <h3>2024</h3>

                    <ul class="pl">
                        <li>
                            <a href="https://arxiv.org/abs/2407.07071" target="_blank">
                                <b>Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps</b>
                            </a>
                            <br/>
                            <b>Yung-Sung Chuang</b>
                            <a href="https://linlu-qiu.github.io/" target="_blank">Linlu Qiu</a>,
                            <a href="https://chengyuhsieh.github.io/" target="_blank">Cheng-Yu Hsieh</a>,
                            <a href="https://www.ranjaykrishna.com/index.html" target="_blank">Ranjay Krishna</a>,
                            <a href="https://people.csail.mit.edu/yoonkim/" target="_blank">Yoon Kim</a>,
                            <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
                            <br/>
                            In <a href="https://2024.emnlp.org" target="_blank"><b>The Conference on Empirical Methods in Natural Language Processing</b></a>, 2024.
                            <br/>
                            [<a href="bibs/chuang2024lookback.bib" target="_blank" type="text/plain">bib</a>]
                            [<a href="#" onclick="$('#lookback_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://arxiv.org/pdf/2407.07071.pdf" target="_blank">pdf</a>]
                            [<a href="https://github.com/voidism/Lookback-Lens" target="_blank">code</a>]
                            <div id="lookback_abstract" class="abstract" style="display:none;">
                                <p>
                                    When asked to summarize articles or answer questions given a passage, large language models (LLMs) can hallucinate details and respond with unsubstantiated answers that are inaccurate with respect to the input context. This paper describes a simple approach for detecting such contextual hallucinations. We hypothesize that contextual hallucinations are related to the extent to which an LLM attends to information in the provided context versus its own generations. Based on this intuition, we propose a simple hallucination detection model whose input features are given by the ratio of attention weights on the context versus newly generated tokens (for each attention head). We find that a linear classifier based on these lookback ratio features is as effective as a richer detector that utilizes the entire hidden states of an LLM or a text-based entailment model. The lookback ratio-based detector -- Lookback Lens -- is found to transfer across tasks and even models, allowing a detector that is trained on a 7B model to be applied (without retraining) to a larger 13B model. We further apply this detector to mitigate contextual hallucinations, and find that a simple classifier-guided decoding approach is able to reduce the amount of hallucination, for example by 9.6% in the XSum summarization task.
                                </p>
                            </div>
                        </li>
                    </ul>

                    <ul class="pl">
                        <li>
                            <a href="https://arxiv.org/abs/2309.03883" target="_blank">
                                <b>DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models</b>
                            </a>
                            <br/>
                            <b>Yung-Sung Chuang</b>,
                            <a href="https://sites.google.com/view/yujia" target="_blank">Yujia Xie</a>,
                            <a href="https://luohongyin.github.io/" target="_blank">Hongyin Luo</a>,
                            <a href="https://people.csail.mit.edu/yoonkim/" target="_blank">Yoon Kim</a>,
                            <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>,
                            <a href="https://scholar.google.com/citations?user=TS1RoxAAAAAJ&hl=en" target="_blank">Pengcheng He</a>
                            <br/>
                            In <a href="https://iclr.cc/" target="_blank">
                                <b>
                                    The Twelfth International Conference on Learning Representations (ICLR)</b></a>, 2024.
                            <br/>
                            [<a href="bibs/chuang2023dola.bib" target="_blank" type="text/plain">bib</a>]
                            [<a href="#" onclick="$('#chuang2023dola_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://browse.arxiv.org/pdf/2309.03883.pdf" target="_blank">pdf</a>]
                            [<a href="https://github.com/voidism/DoLa" target="_blank">code</a>]
                            <div id="chuang2023dola_abstract" class="abstract" style="display:none;">
                                <p>
                                    Despite their impressive capabilities, large language models (LLMs) are prone to hallucinations, i.e., generating content that deviates from facts seen during pretraining. We propose a simple decoding strategy for reducing hallucinations with pretrained LLMs that does not require conditioning on retrieved external knowledge nor additional fine-tuning. Our approach obtains the next-token distribution by contrasting the differences in logits obtained from projecting the later layers versus earlier layers to the vocabulary space, exploiting the fact that factual knowledge in an LLMs has generally been shown to be localized to particular transformer layers. We find that this Decoding by Contrasting Layers (DoLa) approach is able to better surface factual knowledge and reduce the generation of incorrect facts. DoLa consistently improves the truthfulness across multiple choices tasks and open-ended generation tasks, for example improving the performance of LLaMA family models on TruthfulQA by 12-17% absolute points, demonstrating its potential in making LLMs reliably generate truthful facts.
                                </p>
                            </div>
                        </li>
                    </ul>
                    

                    <ul class="pl">
                        <li>
                            <a href="https://arxiv.org/abs/2309.15678" target="_blank">
                                <b>Curiosity-driven Red-teaming for Large Language Models</b>
                            </a>
                            <br/>
                            <a href="https://williamd4112.github.io/" target="_blank">Zhang-Wei Hong</a>,
                            <a href="https://idanshen.github.io/" target="_blank">Idan Shenfeld</a>,
                            <a href="https://zswang666.github.io/" target="_blank">Tsun-Hsuan Wang</a>,
                            <b>Yung-Sung Chuang</b>,
                            <a href="https://mitibmwatsonailab.mit.edu/people/aldo-pareja/" target="_blank">Aldo Pareja</a>,
                            <a href="https://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>,
                            <a href="https://akashgit.github.io/" target="_blank">Akash Srivastava</a>,
                            <a href="https://people.csail.mit.edu/pulkitag/" target="_blank">Pulkit Agrawal</a>
                            <br/>
                            In <a href="https://iclr.cc/ICLR2024/" target="_blank">
                                <b>The Twelfth International Conference on Learning Representations (ICLR)</b></a>, 2024.
                            <br/>
                            [<a href="bibs/hong2024curiosity.bib" target="_blank" type="text/plain">bib</a>]
                            [<a href="#" onclick="$('#hong2024curiosity_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://arxiv.org/pdf/2309.15678.pdf" target="_blank">pdf</a>]
                            [<a href="https://github.com/Improbable-AI/curiosity_redteam" target="_blank">code</a>]
                            <div id="hong2024curiosity_abstract" class="abstract" style="display:none;">
                                <p>
                                    Large language models (LLMs) hold great potential for many natural language applications but risk generating incorrect or toxic content. To probe when an LLM generates unwanted content, the current paradigm is to recruit a red team of human testers to design input prompts that elicit undesirable responses from LLMs. However, relying solely on human testers is expensive and time-consuming. Recent works automate red teaming by training a separate red team LLM with reinforcement learning (RL) to generate test cases that maximize the chance of eliciting undesirable responses from the target LLM. Current RL methods only generate a small number of effective test cases, leading to low coverage of the span of prompts that elicit undesirable responses from the target LLM. We propose a method of curiosity-driven red teaming (CRT) that optimizes for novelty to increase the coverage of test cases while maintaining or increasing their effectiveness compared to existing methods. CRT successfully provokes toxic responses from the LLaMA2 model, even after it has been heavily fine-tuned using human preferences to avoid toxic outputs. Code is available at https://github.com/Improbable-AI/curiosity_redteam.
                                </p>
                            </div>
                        </li>
                    </ul>
                    
                    <ul class="pl">
                        <li>
                            <a href="https://arxiv.org/abs/2310.03991" target="_blank">
                                <b>Semstamp: A semantic watermark with paraphrastic robustness for text generation</b>
                            </a>
                            <br/>
                            <a href="https://scholar.google.com/citations?user=H5-QJloAAAAJ&hl=en" target="_blank">Abe Bohan Hou</a>,
                            <a href="https://jackz.io/" target="_blank">Jingyu Zhang</a>,
                            <a href="https://cloudygoose.github.io/" target="_blank">Tianxing He</a>,
                            <a href="https://yichenzw.com/" target="_blank">Yichen Wang</a>,
                            <b>Yung-Sung Chuang</b>,
                            <a href="#" target="_blank">Hongwei Wang</a>,
                            <a href="#" target="_blank">Lingfeng Shen</a>,
                            <a href="#" target="_blank">Benjamin Van Durme</a>,
                            <a href="https://danielkhashabi.com/" target="_blank">Daniel Khashabi</a>,
                            <a href="https://homes.cs.washington.edu/~yuliats/" target="_blank">Yulia Tsvetkov</a>
                            <br/>
                            In <a href="https://2024.naacl.org/" target="_blank">
                                <b>Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</b></a>, 2024.
                            <br/>
                            [<a href="bibs/hou2024semstamp.bib" target="_blank" type="text/plain">bib</a>]
                            [<a href="#" onclick="$('#hou2024semstamp_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://arxiv.org/pdf/2310.03991.pdf" target="_blank">pdf</a>]
                            [<a href="https://github.com/bohanhou14/SemStamp" target="_blank">code</a>]
                            <div id="hou2024semstamp_abstract" class="abstract" style="display:none;">
                                <p>
                                    Existing watermarking algorithms are vulnerable to paraphrase attacks because of their token-level design. To address this issue, we propose SemStamp, a robust sentence-level semantic watermarking algorithm based on locality-sensitive hashing (LSH), which partitions the semantic space of sentences. The algorithm encodes and LSH-hashes a candidate sentence generated by an LLM, and conducts sentence-level rejection sampling until the sampled sentence falls in watermarked partitions in the semantic embedding space. A margin-based constraint is used to enhance its robustness. To show the advantages of our algorithm, we propose a "bigram" paraphrase attack using the paraphrase that has the fewest bigram overlaps with the original sentence. This attack is shown to be effective against the existing token-level watermarking method. Experimental results show that our novel semantic watermark algorithm is not only more robust than the previous state-of-the-art method on both common and bigram paraphrase attacks, but also is better at preserving the quality of generation.
                                </p>
                            </div>
                        </li>
                    </ul>
                    

                    <ul class="pl">
                        <li>
                            <a href="https://arxiv.org/abs/2309.10814" target="_blank">
                                <b>Natural Language Embedded Programs for Hybrid Language Symbolic Reasoning</b>
                            </a>
                            <br/>
                            <a href="#" target="_blank">Tianhua Zhang</a>,
                            <a href="#" target="_blank">Jiaxin Ge</a>,
                            <a href="https://luohongyin.github.io/" target="_blank">Hongyin Luo</a>,
                            <b>Yung-Sung Chuang</b>,
                            <a href="#" target="_blank">Mingye Gao</a>,
                            <a href="https://yuangongnd.github.io/" target="_blank">Yuan Gong</a>,
                            <a href="#" target="_blank">Xixin Wu</a>,
                            <a href="https://people.csail.mit.edu/yoonkim/" target="_blank">Yoon Kim</a>,
                            <a href="#" target="_blank">Helen Meng</a>,
                            <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
                            <br/>
                            In <a href="https://2024.naacl.org/" target="_blank">
                                <b>
                                    Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL): Findings</b></a>, 2024.
                            <br/>
                            [<a href="bibs/zhang2023natural.bib" target="_blank" type="text/plain">bib</a>]
                            [<a href="#" onclick="$('#zhang2023natural_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://browse.arxiv.org/pdf/2309.10814.pdf" target="_blank">pdf</a>]
                            [<a href="https://github.com/luohongyin/LangCode" target="_blank">code</a>]
                            <div id="zhang2023natural_abstract" class="abstract" style="display:none;">
                                <p>
                                    How can we perform computations over natural language representations to solve tasks that require symbolic and numeric reasoning? We propose natural language embedded programs (NLEP) as a unifying framework for addressing math/symbolic reasoning, natural language understanding, and instruction following tasks. Our approach prompts a language model to generate full Python programs that define functions over data structures which contain natural language representations of structured knowledge. A Python interpreter then executes the generated code and prints the output. Despite using a task-general prompt, we find that this approach can improve upon strong baselines across a range of different tasks including math and symbolic reasoning, text classification, question answering, and instruction following. We further find the generated programs are often interpretable and enable post-hoc verification of the intermediate reasoning steps.
                                </p>
                            </div>
                        </li>
                    </ul>

                    

                    <ul class="pl">
                        <li>
                            <a href="https://aclanthology.org/2024.findings-eacl.151/" target="_blank">
                                <b>Joint Inference of Retrieval and Generation for Passage Re-ranking</b>
                            </a>
                            <br/>
                            <a href="https://people.csail.mit.edu/weifang/" target="_blank">Wei Fang</a>,
                            <b>Yung-Sung Chuang</b>,
                            <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
                            <br/>
                            In <a href="https://2024.eacl.org/" target="_blank">
                                <b>The 18th Conference of the European Chapter of the Association for Computational Linguistics: Findings</b></a>, 2024.
                            <br/>
                            [<a href="bibs/fang2024joint.bib" target="_blank" type="text/plain">bib</a>]
                            [<a href="#" onclick="$('#fang2024joint_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://aclanthology.org/2024.findings-eacl.151/" target="_blank">pdf</a>]
                            <div id="fang2024joint_abstract" class="abstract" style="display:none;">
                                <p>
                                    Passage retrieval is a crucial component of modern open-domain question answering (QA) systems, providing information for downstream QA components to generate accurate and transparent answers. This study focuses on passage re-ranking, proposing a simple yet effective method, Joint Passage Re-ranking (JPR), that optimizes the mutual information between query and passage distributions, integrating both cross-encoders and generative models in the re-ranking process. Experimental results demonstrate that JPR outperforms conventional re-rankers and language model scorers in both open-domain QA retrieval settings and diverse retrieval benchmarks under zero-shot settings.
                                </p>
                            </div>
                        </li>
                    </ul>
                    

                    <ul class="pl">
                        <li>
                            <a href="https://ieeexplore.ieee.org/document/10477541" target="_blank">
                                <b>Joint Dual Learning With Mutual Information Maximization for Natural Language Understanding and Generation in Dialogues</b>
                            </a>
                            <br/>
                            <a href="https://www.shangyusu.com/" target="_blank">Shang-Yu Su</a>,
                            <b>Yung-Sung Chung</b>,
                            <a href="https://www.csie.ntu.edu.tw/~yvchen/" target="_blank">Yun-Nung Chen</a>
                            <br/>
                            In <a href="https://ieeexplore.ieee.org/xpl/conhome/10388490/proceeding" target="_blank">
                                <b>IEEE/ACM Transactions on Audio, Speech, and Language Processing (Volume: 32)</b></a>, pages 2445 - 2452, March 2024.
                            <br/>
                            [<a href="bibs/su2024joint.bib" target="_blank" type="text/plain">bib</a>]
                            [<a href="#" onclick="$('#su2024joint_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://ieeexplore.ieee.org/document/10477541" target="_blank">pdf</a>]
                            <div id="su2024joint_abstract" class="abstract" style="display:none;">
                                <p>
                                    Modular conversational systems heavily rely on the performance of their natural language understanding (NLU) and natural language generation (NLG) components. NLU focuses on extracting core semantic concepts from input texts, while NLG constructs coherent sentences based on these extracted semantics. Inspired by information theory in digital communication, we introduce a one-way communication model that mirrors human conversations, comprising two distinct phases: (1) the conversion of thoughts into messages, similar to NLG, and (2) the comprehension of received messages, similar to NLU. This paper presents a novel algorithm that trains NLU and NLG collaboratively by concatenating their models and maximizing mutual information between inputs and outputs. This approach efficiently facilitates the transmission of semantics, leading to enhanced learning performance for both components. Our experimental results, based on three benchmark datasets, consistently demonstrate significant improvements for both NLU and NLG tasks, highlighting the practical promise of our proposed method.
                                </p>
                            </div>
                        </li>
                    </ul>
                    
                    <ul class="pl">
                        <li>
                            <a href="https://arxiv.org/abs/2401.13463" target="_blank">
                                <b>SpeechDPR: End-to-End Spoken Passage Retrieval for Open-Domain Spoken Question Answering</b>
                            </a>
                            <br/>
                            <a href="https://scholar.google.com.tw/citations?user=DoibyQwAAAAJ" target="_blank">Chyi-Jiunn Lin</a>,
                            <a href="https://daniellin94144.github.io/" target="_blank">Guan-Ting Lin</a>,
                            <b>Yung-Sung Chuang</b>,
                            <a href="#" target="_blank">Wei-Lun Wu</a>,
                            <a href="https://swdanielli.github.io/" target="_blank">Shang-Wen Li</a>,
                            <a href="http://www.cs.toronto.edu/~asamir/" target="_blank">Abdelrahman Mohamed</a>,
                            <a href="https://speechlab.csie.ntu.edu.tw/~tlkagk/" target="_blank">Hung-yi Lee</a>,
                            <a href="https://speech.ee.ntu.edu.tw/previous_version/lslNew.htm" target="_blank">Lin-shan Lee</a>
                            <br/>
                            In <a href="https://2024.ieeeicassp.org/" target="_blank">
                                <b>
                                IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</b></a>, 2024.
                            <br/>
                            [<a href="bibs/lin2024speechdpr.bib" target="_blank" type="text/plain">bib</a>]
                            [<a href="#" onclick="$('#lin2024speechdpr_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://arxiv.org/pdf/2401.13463.pdf" target="_blank">pdf</a>]
                            <div id="lin2024speechdpr_abstract" class="abstract" style="display:none;">
                                <p>
                                    Spoken Question Answering (SQA) is essential for machines to reply to user's question by finding the answer span within a given spoken passage. SQA has been previously achieved without ASR to avoid recognition errors and Out-of-Vocabulary (OOV) problems. However, the real-world problem of Open-domain SQA (openSQA), in which the machine needs to first retrieve passages that possibly contain the answer from a spoken archive, was never considered. This paper proposes the first known end-to-end framework, Speech Dense Passage Retriever (SpeechDPR), for the retrieval component of the openSQA problem. SpeechDPR learns a sentence-level semantic representation by distilling knowledge from the cascading model of unsupervised ASR (UASR) and text dense retriever (TDR). No manually transcribed speech data is needed. Initial experiments showed performance comparable to the cascading model of UASR and TDR, and significantly better when UASR was poor, verifying that this approach is more robust to speech recognition errors.
                                </p>
                            </div>
                        </li>
                    </ul>
                    
                    

                    <ul class="pl">
                        <li>
                            <a href="https://ieeexplore.ieee.org/abstract/document/10502279" target="_blank">
                                <b>A Large-Scale Evaluation of Speech Foundation Models</b>
                            </a>
                            <br/>
                            <a href="#" target="_blank">Shu-wen Yang</a>,
                            <a href="#" target="_blank">Heng-Jui Chang</a>,
                            <a href="#" target="_blank">Zili Huang</a>,
                            <a href="#" target="_blank">Andy T Liu</a>,
                            <a href="#" target="_blank">Cheng-I Lai</a>,
                            <a href="#" target="_blank">Haibin Wu</a>,
                            <a href="#" target="_blank">Jiatong Shi</a>,
                            <a href="#" target="_blank">Xuankai Chang</a>,
                            <a href="#" target="_blank">Hsiang-Sheng Tsai</a>,
                            <a href="#" target="_blank">Wen-Chin Huang</a>,
                            <a href="#" target="_blank">Tzu-hsun Feng</a>,
                            <a href="#" target="_blank">Po-Han Chi</a>,
                            <a href="#" target="_blank">Yist Y Lin</a>,
                            <b>Yung-Sung Chuang</b>,
                            <a href="#" target="_blank">Tzu-Hsien Huang</a>,
                            <a href="#" target="_blank">Wei-Cheng Tseng</a>,
                            <a href="#" target="_blank">Kushal Lakhotia</a>,
                            <a href="#" target="_blank">Shang-Wen Li</a>,
                            <a href="#" target="_blank">Abdelrahman Mohamed</a>,
                            <a href="#" target="_blank">Shinji Watanabe</a>,
                            <a href="#" target="_blank">Hung-yi Lee</a>
                            <br/>
                            In <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6570655" target="_blank">
                                <b>IEEE/ACM Transactions on Audio, Speech, and Language Processing</b></a>, page(s) 1 - 16, April 2024.
                            <br/>
                            [<a href="bibs/yang2024largescale.bib" target="_blank" type="text/plain">bib</a>]
                            [<a href="#" onclick="$('#yang2024largescale_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://ieeexplore.ieee.org/abstract/document/10502279" target="_blank">pdf</a>]
                            <div id="yang2024largescale_abstract" class="abstract" style="display:none;">
                                <p>
                                    The foundation model paradigm leverages a shared foundation model to achieve state-of-the-art (SOTA) performance for various tasks, requiring minimal downstream-specific data collection and modeling. This approach has proven crucial in the field of Natural Language Processing (NLP). However, the speech processing community lacks a similar setup to explore the paradigm systematically. To bridge this gap, we establish the Speech processing Universal PERformance Benchmark (SUPERB). SUPERB represents an ecosystem designed to evaluate foundation models across a wide range of speech processing tasks, facilitating the sharing of results on an online leaderboard and fostering collaboration through a community-driven benchmark database that aids in new development cycles. We present a unified learning framework for solving the speech processing tasks in SUPERB with the frozen foundation model followed by task-specialized lightweight prediction heads. Combining our results with community submissions, we verify that the framework is simple yet effective, as the best-performing foundation model shows competitive generalizability across most SUPERB tasks. Finally, we conduct a series of analyses to offer an in-depth understanding of SUPERB and speech foundation models, including information flows across tasks inside the models and the statistical significance and robustness of the benchmark.
                                </p>
                            </div>
                        </li>
                    </ul>
                    
                    <h3>2023</h3>

                    <ul class="pl">
                        <li>
                            <a href="https://arxiv.org/abs/2310.07654" target="_blank">
                                <b>Audio-Visual Neural Syntax Acquisition</b>
                            </a>
                            <br/>
                            <a href="https://people.csail.mit.edu/clai24/" target="_blank">Cheng-I Jeff Lai</a>,
                            <a href="https://home.ttic.edu/~freda/" target="_blank">Freda Shi</a>,
                            <a href="https://jasonppy.github.io/" target="_blank">Puyuan Peng</a>,
                            <a href="https://people.csail.mit.edu/yoonkim/" target="_blank">Yoon Kim</a>,
                            <a href="https://home.ttic.edu/~kgimpel/" target="_blank">Kevin Gimpel</a>,
                            <a href="https://code-terminator.github.io/" target="_blank">Shiyu Chang</a>,
                            <b>Yung-Sung Chuang</b>,
                            <a href="https://scholar.google.com/citations?user=eVc2TGkAAAAJ&hl=en" target="_blank">Saurabhchand Bhati</a>,
                            <a href="https://mitibmwatsonailab.mit.edu/people/david-cox/" target="_blank">David Cox</a>,
                            <a href="https://www.cs.utexas.edu/~harwath/" target="_blank">David Harwath</a>,
                            <a href="https://mitibmwatsonailab.mit.edu/people/yang-zhang/" target="_blank">Yang Zhang</a>,
                            <a href="https://home.ttic.edu/~klivescu/" target="_blank">Karen Livescu</a>,
                            <a href="https://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
                            <br/>
                            In <a href="https://asru2023.org/" target="_blank">
                                <b>2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</b>
                            </a>.
                            <br/>
                            [<a href="bibs/lai2023audiovisual.bib" target="_blank" type="text/plain">bib</a>]
                            [<a href="#" onclick="$('#lai2023audiovisual_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://arxiv.org/pdf/2310.07654.pdf" target="_blank">pdf</a>]
                            <div id="lai2023audiovisual_abstract" class="abstract" style="display:none;">
                                <p>
                                    We study phrase structure induction from visually-grounded speech. The core idea is to first segment the speech waveform into sequences of word segments, and subsequently induce phrase structure using the inferred segment-level continuous representations. We present the Audio-Visual Neural Syntax Learner (AV-NSL) that learns phrase structure by listening to audio and looking at images, without ever being exposed to text. By training on paired images and spoken captions, AV-NSL exhibits the capability to infer meaningful phrase structures that are comparable to those derived by naturally-supervised text parsers, for both English and German. Our findings extend prior work in unsupervised language acquisition from speech and grounded grammar induction, and present one approach to bridge the gap between the two topics.
                                </p>
                            </div>
                        </li>
                    </ul>

                    
                    <ul class="pl">
                        <li>
                            <a href="https://openreview.net/forum?id=yt3hnXNPfr" target="_blank">
                                <b>On Robustness-Accuracy Characterization of Large Language Models using Synthetic Datasets</b>
                            </a>
                            <br/>
                            <a href="https://ireneko.github.io/" target="_blank">Ching-Yun Ko</a>,
                            <a href="https://sites.google.com/site/pinyuchenpage" target="_blank">Pin-Yu Chen</a>,
                            <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-daspa" target="_blank">Payel Das</a>,
                            <b>Yung-Sung Chuang</b>,
                            <a href="http://www.mit.edu/~dluca/" target="_blank">Luca Daniel</a>
                            <br/>
                            In <a href="https://openreview.net/group?id=ICML.cc/2023/Workshop/ES-FoMO" target="_blank">
                                <b>Workshop on Efficient Systems for Foundation Models@ ICML2023</b>
                            </a>, July 2023.
                            <br/>
                            [<a href="bibs/ko2023on.bib" target="_blank" type="text/plain">bib</a>]
                            [<a href="#" onclick="$('#ko2023robust_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://openreview.net/pdf?id=yt3hnXNPfr" target="_blank">pdf</a>]
                            <div id="ko2023robust_abstract" class="abstract" style="display:none;">
                                <p>
                                    Despite the impressive capability of large language models (LLMs) in solving different downstream tasks, new concerns about proper performance evaluation have been raised, especially for test-data leakage caused by accidentally including them during pretraining, or by indirectly exposing them through API calls for evaluation. Motivated by these, in this paper, we propose a new evaluation workflow that generates steerable synthetic language datasets and proxy tasks for benchmarking the performance of pertained LLMs on sentence classification tasks. This approach allows for better characterization of the joint analysis on the robustness and accuracy of LLMs without risking sensitive information leakage. Verified on various pretrained LLMs, the proposed approach demonstrates promising high correlation with real downstream performance.
                                </p>
                            </div>
                        </li>
                    </ul>
                    
                    
                    <ul class="pl">
                        <li>
                            <a href="https://aclanthology.org/2023.repl4nlp-1.24/" target="_blank">
                                <b>Revealing the Blind Spot of Sentence Encoder Evaluation by HEROS</b>
                            </a>
                            <br/>
                            <a href="https://d223302.github.io/" target="_blank">Cheng-Han Chiang</a>,
                            <b>Yung-Sung Chuang</b>,
                            <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>,
                            <a href="https://speechlab.csie.ntu.edu.tw/~tlkagk/" target="_blank">Hung-yi Lee</a>
                            <br/>
                            In <a href="https://www.aclweb.org/anthology/volumes/2023.repl4nlp-1/" target="_blank">
                                <b>Proceedings of the 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)</b>
                            </a>, 2023.
                            <br/>
                            [<a href="bibs/chiang2023revealing.bib" target="_blank" type="text/plain">bib</a>]
                            [<a href="#" onclick="$('#chiang2023heros_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://aclanthology.org/2023.repl4nlp-1.24.pdf" target="_blank">pdf</a>]
                            [<a href="https://huggingface.co/datasets/dcml0714/Heros" target="_blank">data</a>]
                            <div id="chiang2023heros_abstract" class="abstract" style="display:none;">
                                <p>
                                    Existing sentence textual similarity benchmark datasets only use a single number to summarize how similar the sentence encoder's decision is to humans'. However, it is unclear what kind of sentence pairs a sentence encoder (SE) would consider similar. Moreover, existing SE benchmarks mainly consider sentence pairs with low lexical overlap, so it is unclear how the SEs behave when two sentences have high lexical overlap. We introduce a high-quality SE diagnostic dataset, HEROS. HEROS is constructed by transforming an original sentence into a new sentence based on certain rules to form a <i>minimal pair</i>, and the minimal pair has high lexical overlaps. The rules include replacing a word with a synonym, an antonym, a typo, a random word, and converting the original sentence into its negation. Different rules yield different subsets of HEROS. By systematically comparing the performance of over 60 supervised and unsupervised SEs on HEROS, we reveal that most unsupervised sentence encoders are insensitive to negation. We find the datasets used to train the SE are the main determinants of what kind of sentence pairs an SE considers similar. We also show that even if two SEs have similar performance on STS benchmarks, they can have very different behavior on HEROS. Our result reveals the blind spot of traditional STS benchmarks when evaluating SEs.
                                </p>
                            </div>
                        </li>
                    </ul>

                    <ul class="pl">
                        <li>
                            <a href="https://arxiv.org/abs/2305.17080" target="_blank">
                                <b>Expand, Rerank, and Retrieve: Query Reranking for Open-Domain Question Answering</b>
                            </a>
                            <br/>
                            <b>Yung-Sung Chuang</b>,
                            <a href="https://people.csail.mit.edu/weifang/" target="_blank">Wei Fang</a>,
                            <a href="https://swdanielli.github.io/" target="_blank">Shang-Wen Li</a>,
                            <a href="http://scottyih.org/" target="_blank">Wen-tau Yih</a>,
                            <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
                            <br/>
                            In <a href="https://2023.aclweb.org/" target="_blank">
                                <b>
                                The 61st Annual Meeting of the Association for Computational Linguistics: Findings</b></a>, 2023.
                            <br/>
                            [<a href="bibs/chuang2023expand.bib" target="_blank" type="text/plain">bib</a>]
                            [<a href="#" onclick="$('#chuang2023expand_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://arxiv.org/pdf/2305.17080.pdf" target="_blank">pdf</a>]
                            [<a href="https://github.com/voidism/EAR" target="_blank">code</a>]
                            <div id="chuang2023expand_abstract" class="abstract" style="display:none;">
                                <p>
                                    We propose EAR, a query Expansion And Reranking approach for improving passage retrieval, with the application to open-domain question answering. EAR first applies a query expansion model to generate a diverse set of queries, and then uses a query reranker to select the ones that could lead to better retrieval results. Motivated by the observation that the best query expansion often is not picked by greedy decoding, EAR trains its reranker to predict the rank orders of the gold passages when issuing the expanded queries to a given retriever. By connecting better the query expansion model and retriever, EAR significantly enhances a traditional sparse retrieval method, BM25. Empirically, EAR improves top-5/20 accuracy by 3-8 and 5-10 points in in-domain and out-of-domain settings, respectively, when compared to a vanilla query expansion model, GAR, and a dense retrieval model, DPR.
                                </p>
                            </div>
                        </li>
                    </ul>

                    <ul class="pl">
                        <li>
                            <a href="https://arxiv.org/abs/2305.15225" target="_blank">
                                <b>SAIL: Search-Augmented Instruction Learning</b>
                            </a>
                            <br/>
                            <a href="https://luohongyin.github.io/" target="_blank">Hongyin Luo</a>,
                            <b>Yung-Sung Chuang</b>,
                            <a href="https://yuangongnd.github.io/" target="_blank">Yuan Gong</a>, Tianhua Zhang, <a href="https://people.csail.mit.edu/yoonkim/" target="_blank">Yoon Kim</a>, Xixin Wu, Danny Fox, Helen Meng, <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
                            <br/>
                            In <a href="https://aclanthology.org/volumes/2023.findings-emnlp/" target="_blank">
                                <b>
                                Findings of the Association for Computational Linguistics: EMNLP</b></a>, 2023.
                            <br/>
                            [<a href="bibs/luo2023sail.bib" target="_blank" type="text/plain">bib</a>]
                            [<a href="#" onclick="$('#luo2023sail_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://arxiv.org/pdf/2305.15225.pdf" target="_blank">pdf</a>]
                            [<a href="https://github.com/luohongyin/SAIL" target="_blank">code</a>]
                            [<a href="https://huggingface.co/spaces/luohy/SAIL-7B" target="_blank">demo</a>]
                            [<a href="https://openlsr.org/sail-7b" target="_blank">web</a>]
                            <div id="luo2023sail_abstract" class="abstract" style="display:none;">
                                <p>
                                    Large language models (LLMs) have been significantly improved by instruction fine-tuning, but still lack transparency and the ability to utilize up-to-date knowledge and information. In this work, we propose search-augmented instruction learning (SAIL), which grounds the language generation and instruction following abilities on complex search results generated by in-house and external search engines. With an instruction tuning corpus, we collect search results for each training case from different search APIs and domains, and construct a new search-grounded training set containing \textit{(instruction, grounding information, response)} triplets. We then fine-tune the LLaMA-7B model on the constructed training set. Since the collected results contain unrelated and disputing languages, the model needs to learn to ground on trustworthy search results, filter out distracting passages, and generate the target response. The search result-denoising process entails explicit trustworthy information selection and multi-hop reasoning, since the retrieved passages might be informative but not contain the instruction-following answer. Experiments show that the fine-tuned SAIL-7B model has a strong instruction-following ability, and it performs significantly better on transparency-sensitive tasks, including open-ended question answering and fact checking.
                                </p>
                            </div>
                        </li>
                    </ul>
                    
                    
                    <ul class="pl">
                        <li>
                            <a href="https://arxiv.org/abs/2304.03728" target="_blank">
                                <b>Interpretable Unified Language Checking</b>
                            </a>
                            <br/>
                            <a href="https://luohongyin.github.io/" target="_blank">Hongyin Luo</a>,
                            Tianhua Zhang, 
                            <b>Yung-Sung Chuang</b>,
                            <a href="https://people.csail.mit.edu/weifang/" target="_blank">Wei Fang</a>, Luc Gaitskell, Thomas Hartvigsen, Xixin Wu, Danny Fox, Helen Meng, <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
                            <br/>
                            In <b>arXiv preprint</b>, 2023.
                            <br/>
                            [<a href="bibs/zhang2023interpretable.bib" target="_blank" type="text/plain">bib</a>]
                            [<a href="#" onclick="$('#zhang2023interpretable_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://arxiv.org/pdf/2304.03728.pdf" target="_blank">pdf</a>]
                            [<a href="https://github.com/luohongyin/UniLC" target="_blank">code</a>]
                            <div id="zhang2023interpretable_abstract" class="abstract" style="display:none;">
                                <p>
                                    Despite recent concerns about undesirable behaviors generated by large language models (LLMs), including non-factual, biased, and hateful language, we find LLMs are inherent multi-task language checkers based on their latent representations of natural and social knowledge. We present an interpretable, unified, language checking (UniLC) method for both human and machine-generated language that aims to check if language input is factual and fair. While fairness and fact-checking tasks have been handled separately with dedicated models, we find that LLMs can achieve high performance on a combination of fact-checking, stereotype detection, and hate speech detection tasks with a simple, few-shot, unified set of prompts. With the ``1/2-shot'' multi-task language checking method proposed in this work, the GPT3.5-turbo model outperforms fully supervised baselines on several language tasks. The simple approach and results suggest that based on strong latent knowledge representations, an LLM can be an adaptive and explainable tool for detecting misinformation, stereotypes, and hate speech.
                                </p>
                            </div>
                        </li>
                    </ul>
                    
                    <ul class="pl">
                        <li>
                            <a href="https://ieeexplore.ieee.org/abstract/document/10094821" target="_blank">
                                <b>C2KD: Cross-Lingual Cross-Modal Knowledge Distillation for Multilingual Text-Video Retrieval</b>
                            </a>
                            <br/>
                            <a href="http://people.csail.mit.edu/roudi/" target="_blank">Andrew Rouditchenko</a>,
                            <b>Yung-Sung Chuang</b>,
                            Nina Shvetsova,
                            Samuel Thomas,
                            Rogerio Feris,
                            Brian Kingsbury,
                            Leonid Karlinsky,
                            David Harwath,
                            Hilde Kuehne,
                            James Glass
                            <br/>
                            In <a href="https://2023.ieeeicassp.org/" target="_blank">
                                <b>
                                IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</b></a>, 2023.
                            <br/>
                            [<a href="bibs/roudi2023c2kd.bib" target="_blank" type="text/plain">bib</a>]
                            [<a href="#" onclick="$('#roudi2023c2kd_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://arxiv.org/pdf/2210.03625.pdf" target="_blank">pdf</a>]
                            [<a href="https://github.com/roudimit/c2kd" target="_blank">code & dataset</a>]
                            <div id="roudi2023c2kd_abstract" class="abstract" style="display:none;">
                                <p>
                                    Multilingual text-video retrieval methods have improved significantly in recent years, but the performance for languages other than English still lags. We propose a Cross-Lingual Cross-Modal Knowledge Distillation method to improve multilingual text-video retrieval. Inspired by the fact that English text-video retrieval outperforms other languages, we train a student model using input text in different languages to match the cross-modal predictions from teacher models using input text in English. We propose a cross entropy based objective which forces the distribution over the student’s text-video similarity scores to be similar to those of the teacher models. We introduce a new multilingual video dataset, Multi-YouCook2, by translating the English captions in the YouCook2 video dataset to 8 other languages. Our method improves multilingual text-video retrieval performance on Multi-YouCook2 and several other datasets such as Multi-MSRVTT and VATEX. We also conducted an analysis on the effectiveness of different multilingual text models as teachers.
                                </p>
                            </div>
                        </li>
                    </ul>
                    
                    <ul class="pl">
                        <li>
                            <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Lu_Visual_Language_Pretrained_Multiple_Instance_Zero-Shot_Transfer_for_Histopathology_Images_CVPR_2023_paper.html" target="_blank">
                                <b>Visual Language Pretrained Multiple Instance Zero-Shot Transfer for Histopathology Images</b>
                            </a>
                            <br/>
                            <a href="https://mingylu.me/" target="_blank">Ming Y Lu</a>,
                            Bowen Chen, 
                            Andrew Zhang, 
                            Drew FK Williamson, 
                            Richard J Chen, 
                            Tong Ding, 
                            Long Phi Le, 
                            <b>Yung-Sung Chuang</b>, 
                            Faisal Mahmood
                            <br/>
                            In <a href="https://2023.thecvf.com/" target="_blank">
                                <b>
                                Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</b></a>, 2023.
                            <br/>
                            [<a href="bibs/lu2022visual.bib" target="_blank" type="text/plain">bib</a>]
                            [<a href="#" onclick="$('#lu2022visual').toggle();return false;">abstract</a>]
                            [<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Visual_Language_Pretrained_Multiple_Instance_Zero-Shot_Transfer_for_Histopathology_Images_CVPR_2023_paper.pdf" target="_blank">pdf</a>]
                            [<a href="https://openaccess.thecvf.com/content/CVPR2023/supplemental/Lu_Visual_Language_Pretrained_CVPR_2023_supplemental.pdf" target="_blank">supplementary material</a>]
                            <div id="lu2022visual_abstract" class="abstract" style="display:none;">
                                <p>
                                    Contrastive visual language pretraining has emerged as a powerful method for either training new language-aware image encoders or augmenting existing pretrained models with zero-shot visual recognition capabilities. However, existing works typically train on large datasets of image-text pairs and have been designed to perform downstream tasks involving only small to medium sized-images, neither of which are applicable to the emerging field of computational pathology where there are limited publicly available paired image-text datasets and each image can span up to 100,000 x 100,000 pixels in dimensions. In this paper we present MI-Zero, a simple and intuitive framework for unleashing the zero-shot transfer capabilities of contrastively aligned image and text models to gigapixel histopathology whole slide images, enabling multiple downstream diagnostic tasks to be carried out by pretrained encoders without requiring any additional labels. MI-Zero reformulates zero-shot transfer under the framework of multiple instance learning to overcome the computational challenge of inference on extremely large images. We used over 550k pathology reports and other available in-domain text corpora to pretrain our text encoder. By effectively leveraging strong pretrained encoders, our best model pretrained on over 33k histopathology image-caption pairs achieves an average median zero-shot accuracy of 70.2% across three different real-world cancer subtyping tasks. Our code is available at: https://github. com/mahmoodlab/MI-Zero.
                                </p>
                            </div>
                        </li>
                    </ul>
                    
                    <h3>2022</h3>
                    <ul class="pl">
                        <li>
                            <a href="https://aclanthology.org/2022.aacl-tutorials.2/" target="_blank">
                                <b>Recent Advances in Pre-trained Language Models: Why Do They Work and How Do They Work</b>
                            </a>
                            <br/>
                            <a href="https://d223302.github.io/" target="_blank">Cheng-Han Chiang</a>,
                            <b>Yung-Sung Chuang</b>, 
                            <a href="https://speech.ee.ntu.edu.tw/~hylee/" target="_blank">Hung-Yi Lee</a>
                            <br/>
                            In <a href="https://aclanthology.org/2022.aacl-tutorials.0/" target="_blank">
                                <b>
                                Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing: Tutorial Abstracts</b></a>, 2022.
                            <br/>
                            [<a href="bibs/chiang2022recent.bib" target="_blank" type="text/plain">bib</a>]
                            [<a href="#" onclick="$('#chiang2022recent_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://aclanthology.org/2022.aacl-tutorials.2.pdf" target="_blank">pdf</a>]
                            [<a href="https://d223302.github.io/AACL2022-Pretrain-Language-Model-Tutorial/" target="_blank">tutorial page</a>]
                            [<a href="https://www.youtube.com/watch?v=thr4-hgLhi8" target="_blank">video</a>]
                            <div id="chiang2022recent_abstract" class="abstract" style="display:none;">
                                <p>
                                    Pre-trained language models (PLMs) are language models that are pre-trained on large-scaled corpora in a self-supervised fashion. These PLMs have fundamentally changed the natural language processing community in the past few years. In this tutorial, we aim to provide a broad and comprehensive introduction from two perspectives: why those PLMs work, and how to use them in NLP tasks. The first part of the tutorial shows some insightful analysis on PLMs that partially explain their exceptional downstream performance. The second part first focuses on emerging pre-training methods that enable PLMs to perform diverse downstream tasks and then illustrates how one can apply those PLMs to downstream tasks under different circumstances. These circumstances include fine-tuning PLMs when under data scarcity, and using PLMs with parameter efficiency. We believe that attendees of different backgrounds would find this tutorial informative and useful.
                                </p>
                            </div>
                        </li>
                    </ul>
                    
                    <ul class="pl">
                        <li>
                            <a href="https://arxiv.org/abs/2110.01147" target="_blank">
                                <b>On the interplay between sparsity, naturalness, intelligibility, and prosody in speech synthesis</b>
                            </a>
                            <br/>
                            <a href="http://people.csail.mit.edu/clai24/" target="_blank">Cheng-I Jeff Lai</a>, 
                            <a href="http://www.cs.columbia.edu/~ecooper/" target="_blank">Erica Cooper</a>, 
                            <a href="https://mitibmwatsonailab.mit.edu/people/yang-zhang/" target="_blank">Yang Zhang</a>, 
                            <a href="https://mitibmwatsonailab.mit.edu/people/shiyu-chang/" target="_blank">Shiyu Chang</a>, 
                            <a href="https://scholar.google.com/citations?user=uEpr4C4AAAAJ&hl=en" target="_blank">Kaizhi Qian</a>, 
                            <a href="https://github.com/yilunliao" target="_blank">Yi-Lun Liao</a>, 
                            <b>Yung-Sung Chuang</b>, 
                            <a href="https://scholar.google.com/citations?user=LIiCDa0AAAAJ&hl=en" target="_blank">Alexander H. Liu</a>, 
                            <a href="https://scholar.google.com/citations?user=nRrdjtwAAAAJ&hl=en" target="_blank">Junichi Yamagishi</a>, 
                            <a href="https://mitibmwatsonailab.mit.edu/people/david-cox/" target="_blank">David Cox</a>, 
                            <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>.
                            <br/>
                            In <a href="https://2022.ieeeicassp.org/" target="_blank">
                                <b>ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</b></a>, 2022. <a style="color:#e22222"></a>
                            <br/>
                            [<a href="bibs/lai2022on.bib" target="_blank" type="text/plain">bib</a>]
                            [<a href="#" onclick="$('#lai2022on_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://arxiv.org/pdf/2110.01147.pdf" target="_blank">pdf</a>]
                            [<a href="https://people.csail.mit.edu/clai24/prune-tts/" target="_blank">code</a>]
                            [<a href="https://www.youtube.com/watch?v=od7cK1Yz6oQ" target="_blank">video</a>]
                            <div id="lai2022on_abstract" class="abstract" style="display:none;">
                                <p>
                                    Are end-to-end text-to-speech (TTS) models over-parametrized? To what extent can these models be pruned, and what happens to their synthesis capabilities? This work serves as a starting point to explore pruning both spectrogram prediction networks and vocoders. We thoroughly investigate the tradeoffs between sparsity and its subsequent effects on synthetic speech. Additionally, we explore several aspects of TTS pruning: amount of finetuning data versus sparsity, TTS-Augmentation to utilize unspoken text, and combining knowledge distillation and pruning. Our findings suggest that not only are end-to-end TTS models highly prunable, but also, perhaps surprisingly, pruned TTS models can produce synthetic speech with equal or higher naturalness and intelligibility, with similar prosody. All of our experiments are conducted on publicly available models, and findings in this work are backed by large-scale subjective tests and objective measures. Code and pruned models are made available to facilitate future research on efficiency in TTS.
                                </p>
                            </div>
                        </li>
                    </ul>                             
                    <ul class="pl">
                        <li>
                            <a href="https://arxiv.org/abs/2204.10298" target="_blank">
                                <b>DiffCSE: Difference-based Contrastive Learning for Sentence Embeddings</b>
                            </a>
                            <br/>
                            <b>Yung-Sung Chuang</b>, 
                            <a href="http://super-ms.mit.edu/rumen.html" target="_blank">Rumen Dangovski</a>, 
                            <a href="http://people.csail.mit.edu/hyluo/" target="_blank">Hongyin Luo</a>, 
                            <a href="https://mitibmwatsonailab.mit.edu/people/yang-zhang/" target="_blank">Yang Zhang</a>, 
                            <a href="https://code-terminator.github.io/" target="_blank">Shiyu Chang</a>, 
                            <a href="http://www.mit.edu/~soljacic/marin.html" target="_blank">Marin Soljačić</a>, 
                            <a href="https://swdanielli.github.io/" target="_blank">Shang-Wen Li</a>, 
                            <a href="https://scottyih.org/" target="_blank">Scott Wen-tau Yih</a>, 
                            <a href="https://people.csail.mit.edu/yoonkim/" target="_blank">Yoon Kim</a>, 
                            <a href="https://www.csail.mit.edu/person/jim-glass" target="_blank">James Glass</a>.
                            <br/>
                            In <a href="https://2022.naacl.org/" target="_blank">
                                <b>
                                  Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</b></a>, 2022. <a class="emph"><b> (Oral Paper) </b></a>
                            <br/>
                            [<a href="bibs/chuang2022diffcse.bib" target="_blank" type="text/plain">bib</a>]
                            [<a href="#" onclick="$('#chuang2022diffcse_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://arxiv.org/pdf/2204.10298.pdf" target="_blank">pdf</a>]
                            [<a href="https://github.com/voidism/DiffCSE" target="_blank">code</a>]
                            [<a href="https://www.youtube.com/watch?v=9vx-HyzcXtU" target="_blank">video</a>]
                            <!-- [<a href="https://superbbenchmark.org/" target="_blank">leaderboard</a>] -->
                            <div id="chuang2022diffcse_abstract" class="abstract" style="display:none;">
                                <p>
                                    We propose DiffCSE, an unsupervised contrastive learning framework for learning sentence embeddings. DiffCSE learns sentence embeddings that are sensitive to the difference between the original sentence and an edited sentence, where the edited sentence is obtained by stochastically masking out the original sentence and then sampling from a masked language model. We show that DiffSCE is an instance of equivariant contrastive learning (Dangovski et al., 2021), which generalizes contrastive learning and learns representations that are insensitive to certain types of augmentations and sensitive to other "harmful" types of augmentations. Our experiments show that DiffCSE achieves state-of-the-art results among unsupervised sentence representation learning methods, outperforming unsupervised SimCSE by 2.3 absolute points on semantic textual similarity tasks.
                                </p>
                            </div>
                        </li>
                    </ul>
                    <ul class="pl">
                        <li>
                            <a href="https://arxiv.org/abs/2203.04911" target="_blank">
                                <b>DUAL: Discrete Spoken Unit Adaptive Learning for Textless Spoken Question Answering</b>
                            </a>
                            <br/>
                            <a href="https://daniellin94144.github.io/" target="_blank">Guan-Ting Lin</a>, 
                            <b>Yung-Sung Chuang</b>, 
                            <a href="https://github.com/voidful" target="_blank">Ho-Lam Chung</a>, 
                            <a href="https://scholar.google.com.tw/citations?user=R1mNI8QAAAAJ" target="_blank">Shu-wen Yang</a>, 
                            <a href="#">Hsuan-Jui Chen</a>, 
                            <a href="https://www.amazon.science/author/shuyan-dong" target="_blank">Shuyan Dong</a>, 
                            <a href="https://swdanielli.github.io/" target="_blank">Shang-Wen Li</a>, 
                            <a href="http://www.cs.toronto.edu/~asamir/" target="_blank">Abdelrahman Mohamed</a>, 
                            <a href="https://speech.ee.ntu.edu.tw/~hylee/index.html" target="_blank">Hung-yi Lee</a>, 
                            <a href="https://speech.ee.ntu.edu.tw/previous_version/lslNew.htm" target="_blank">Lin-shan Lee</a>.
                            <br/>
                            In <a href="https://www.interspeech2022.org/" target="_blank">
                                <b>
                                    Interspeech</b></a>, 2022.
                            <br/>
                            [<a href="bibs/lin2022dual.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#lin2022dual_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://arxiv.org/pdf/2203.04911.pdf" target="_blank">pdf</a>]
                            [<a href="https://github.com/daniellin94144/dual-textless-sqa" target="_blank">code</a>]
                            <!-- [<a href="https://www.youtube.com/watch?v=zd9fiVvej0k" target="_blank">video</a>] -->
                            <!-- [<a href="https://superbbenchmark.org/" target="_blank">leaderboard</a>] -->
                            <div id="lin2022dual_abstract" class="abstract" style="display:none;">
                                <p>
                                    Spoken Question Answering (SQA) is to find the answer from a spoken document given a question, which is crucial for personal assistants when replying to the queries from the users. Existing SQA methods all rely on Automatic Speech Recognition (ASR) transcripts. Not only does ASR need to be trained with massive annotated data that are time and cost-prohibitive to collect for low-resourced languages, but more importantly, very often the answers to the questions include name entities or out-of-vocabulary words that cannot be recognized correctly. Also, ASR aims to minimize recognition errors equally over all words, including many function words irrelevant to the SQA task. Therefore, SQA without ASR transcripts (textless) is always highly desired, although known to be very difficult. This work proposes Discrete Spoken Unit Adaptive Learning (DUAL), leveraging unlabeled data for pre-training and fine-tuned by the SQA downstream task. The time intervals of spoken answers can be directly predicted from spoken documents. We also release a new SQA benchmark corpus, NMSQA, for data with more realistic scenarios. We empirically showed that DUAL yields results comparable to those obtained by cascading ASR and text QA model and robust to real-world data. Our code and model will be open-sourced.
                                </p>
                            </div>
                        </li>
                    </ul>
                    <h3>2021</h3>
                    <ul class="pl">
                        <li>
                            <a href="https://arxiv.org/abs/2106.05933" target="_blank">
                                <b>PARP: Prune, Adjust and Re-Prune for Self-Supervised Speech Recognition</b>
                            </a>
                            <br/>
                            <a href="http://people.csail.mit.edu/clai24/" target="_blank">Cheng-I Jeff Lai</a>, 
                            <a href="https://mitibmwatsonailab.mit.edu/people/yang-zhang/" target="_blank">Yang Zhang</a>, <a href="https://alexander-h-liu.github.io/" target="_blank">Alexander H. Liu</a>, <a href="https://code-terminator.github.io/" target="_blank">Shiyu Chang</a>, <a href="https://tw.linkedin.com/in/yilunliao" target="_blank">Yi-Lun Liao</a>, 
                            <b>Yung-Sung Chuang</b>, 
                            <a href="https://scholar.google.com/citations?user=uEpr4C4AAAAJ&hl=en" target="_blank">Kaizhi Qian</a>, <a href="http://people.csail.mit.edu/sameerk/" target="_blank">Sameer Khurana</a>, <a href="https://mitibmwatsonailab.mit.edu/people/david-cox/" target="_blank">David Cox</a>, <a href="https://www.csail.mit.edu/person/jim-glass" target="_blank">James Glass</a>.
                            <br/>
                            In <a href="https://proceedings.neurips.cc/paper/2021/hash/b17c0907e67d868b4e0feb43dbbe6f11-Abstract.html" target="_blank">
                                <b>
                                    Thirty-fifth Conference on Neural Information Processing Systems (NeurIPS)</b></a>, 2021. <a class="emph"><b>(Spotlight Paper)</b></a>
                            <br/>
                            [<a href="bibs/lai2021parp.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#lai2021parp_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://proceedings.neurips.cc/paper/2021/file/b17c0907e67d868b4e0feb43dbbe6f11-Paper.pdf" target="_blank">pdf</a>]
                            <!-- [<a href="https://github.com/s3prl/s3prl" target="_blank">code</a>] -->
                            <!-- [<a href="https://www.youtube.com/watch?v=zd9fiVvej0k" target="_blank">video</a>] -->
                            <!-- [<a href="https://superbbenchmark.org/" target="_blank">leaderboard</a>] -->
                            <div id="lai2021parp_abstract" class="abstract" style="display:none;">
                                <p>
                                    Recent work on speech self-supervised learning (speech SSL) demonstrated the benefits of scale in learning rich and transferable representations for Automatic Speech Recognition (ASR) with limited parallel data. It is then natural to investigate the existence of sparse and transferrable subnetworks in pre-trained speech SSL models that can achieve even better low-resource ASR performance. However, directly applying widely adopted pruning methods such as the Lottery Ticket Hypothesis (LTH) is suboptimal in the computational cost needed. Moreover, contrary to what LTH predicts, the discovered subnetworks yield minimal performance gain compared to the original dense network. In this work, we propose Prune-Adjust- Re-Prune (PARP), which discovers and finetunes subnetworks for much better ASR performance, while only requiring a single downstream finetuning run. PARP is inspired by our surprising observation that subnetworks pruned for pre-training tasks only needed to be slightly adjusted to achieve a sizeable performance boost in downstream ASR tasks. Extensive experiments on low-resource English and multi-lingual ASR show (1) sparse subnetworks exist in pre-trained speech SSL, and (2) the computational advantage and performance gain of PARP over baseline pruning methods. On the 10min Librispeech split without LM decoding, PARP discovers subnetworks from wav2vec 2.0 with an absolute 10.9%/12.6% WER decrease compared to the full model. We demonstrate PARP mitigates performance degradation in cross-lingual mask transfer, and investigate the possibility of discovering a single subnetwork for 10 spoken languages in one run.
                                </p>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <a href="https://arxiv.org/abs/2105.01051" target="_blank">
                                <b>SUPERB: Speech processing Universal PERformance Benchmark</b>
                            </a>
                            <br/>
                            <a href="https://scholar.google.com.tw/citations?user=R1mNI8QAAAAJ" target="_blank">Shu-wen Yang</a>, <a href="https://scholar.google.com/citations?user=SiyicoEAAAAJ&hl=zh-TW" target="_blank">Po-Han Chi*</a>, <b>Yung-Sung Chuang*</b>, 
                            <a href="http://people.csail.mit.edu/clai24/" target="_blank">Cheng-I Jeff Lai*</a>, 
                            <a href="https://ai.facebook.com/people/kushal-lakhotia/" target="_blank">Kushal Lakhotia*</a>, <a href="https://scholar.google.com/citations?user=0lrZq9MAAAAJ&hl=en" target="_blank">Yist Y. Lin*</a>, 
                            <a href="https://andi611.github.io/" target="_blank">Andy T. Liu*</a>, <a href="http://shijt.site/" target="_blank">Jiatong Shi*</a>, <a href="https://www.lti.cs.cmu.edu/people/222227473/xuankai-chang" target="_blank">Xuankai Chang</a>, <a href="https://github.com/DanielLin94144" target="_blank">Guan-Ting Lin</a>, 
                            <a href="https://tw.linkedin.com/in/tzu-hsien-huang-1686651b6" target="_blank">Tzu-Hsien Huang</a>, Wei-Cheng Tseng, <a href="https://tw.linkedin.com/in/ko-tik-lee-4747291a2/en?trk=people-guest_people_search-card" target="_blank">Ko-tik Lee</a>, <a href="https://scholar.google.com.tw/citations?user=qJ5zXNIAAAAJ" target="_blank">Da-Rong Liu</a>, 
                            <a href="https://dblp.org/pid/210/0905.html" target="_blank">Zili Huang</a>, <a href="https://www.amazon.science/author/shuyan-dong" target="_blank">Shuyan Dong</a>, <a href="https://swdanielli.github.io/" target="_blank">Shang-Wen Li</a>, 
                            <a href="https://sites.google.com/view/shinjiwatanabe" target="_blank">Shinji Watanabe</a>, 
                            <a href="http://www.cs.toronto.edu/~asamir/" target="_blank">Abdelrahman Mohamed</a>, <a href="https://speech.ee.ntu.edu.tw/~hylee/index.html" target="_blank">Hung-Yi Lee</a>.
                            <br/>
                            In <a href="https://www.interspeech2021.org/" target="_blank">
                                <b>
                                    Interspeech</b></a>, 2021.
                            <br/>
                            [<a href="bibs/yang2021superb.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#yang2020superb_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://arxiv.org/pdf/2105.01051.pdf" target="_blank">pdf</a>]
                            [<a href="https://github.com/s3prl/s3prl" target="_blank">code</a>]
                            [<a href="https://www.youtube.com/watch?v=zd9fiVvej0k" target="_blank">video</a>]
                            [<a href="https://superbbenchmark.org/" target="_blank">leaderboard</a>]
                            <div id="yang2020superb_abstract" class="abstract" style="display:none;">
                                <p>
                                    Self-supervised learning (SSL) has proven vital for advancing research in natural language processing (NLP) and computer vision (CV). The paradigm pretrains a shared model on large volumes of unlabeled data and achieves state-of-the-art (SOTA) for various tasks with minimal adaptation. However, the speech processing community lacks a similar setup to systematically explore the paradigm. To bridge this gap, we introduce Speech processing Universal PERformance Benchmark (SUPERB). SUPERB is a leaderboard to benchmark the performance of a shared model across a wide range of speech processing tasks with minimal architecture changes and labeled data. Among multiple usages of the shared model, we especially focus on extracting the representation learned from SSL due to its preferable re-usability. We present a simple framework to solve SUPERB tasks by learning task-specialized lightweight prediction heads on top of the frozen shared model. Our results demonstrate that the framework is promising as SSL representations show competitive generalizability and accessibility across SUPERB tasks. We release SUPERB as a challenge with a leaderboard and a benchmark toolkit to fuel the research in representation learning and general speech processing.
                                </p>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <a href="https://arxiv.org/abs/2105.04840" target="_blank">
                                <b>Investigating the Reordering Capability in CTC-based Non-Autoregressive End-to-End Speech Translation</b>
                            </a>
                            <br/>
                            <a href="https://scholar.google.com.hk/citations?user=CYDgtRoAAAAJ" target="_blank">Shun-Po Chuang*</a>, <b>Yung-Sung Chuang*</b>, <a href="https://aclanthology.org/people/c/chih-chiang-chang/" target="_blank">Chih-Chiang Chang*</a>,
                            <a href="https://speech.ee.ntu.edu.tw/~hylee/index.html" target="_blank">Hung-Yi Lee</a>.
                            <br/>
                            In <a href="https://aclanthology.org/volumes/2021.findings-acl/" target="_blank">
                                <b>
                                    The Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Findings</b></a>, 2021.
                            <br/>
                            [<a href="bibs/chuang2021investigating.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#chuang2021investigating_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://aclanthology.org/2021.findings-acl.92.pdf" target="_blank">pdf</a>]
                            [<a href="https://github.com/voidism/NAR-ST" target="_blank">code</a>]
                            [<a href="https://www.youtube.com/watch?v=I80xwWfswl8" target="_blank">video</a>]
                            <!-- [<a href="https://leaderboard.allenai.org/quoref" target="_blank">leaderboard</a>] -->
                            <div id="chuang2021investigating_abstract" class="abstract" style="display:none;">
                                <p>
                                    We study the possibilities of building a non-autoregressive speech-to-text translation model using connectionist temporal classification (CTC), and use CTC-based automatic speech recognition as an auxiliary task to improve the performance. CTC's success on translation is counter-intuitive due to its monotonicity assumption, so we analyze its reordering capability. Kendall's tau distance is introduced as the quantitative metric, and gradient-based visualization provides an intuitive way to take a closer look into the model. Our analysis shows that transformer encoders have the ability to change the word order and points out the future research direction that worth being explored more on non-autoregressive speech translation.
                                </p>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <a href="https://arxiv.org/abs/2106.07240" target="_blank">
                                <b>Mitigating Biases in Toxic Language Detection through Invariant Rationalization</b>
                            </a>
                            <br/>
                            <b>Yung-Sung Chuang</b>, <a href="https://www.linkedin.com/in/mingye94" target="_blank">Mingye Gao</a>,
                            <a href="http://people.csail.mit.edu/hyluo/" target="_blank">Hongyin Luo</a>,
                            <a href="https://www.csail.mit.edu/person/jim-glass" target="_blank">James Glass</a>,
                            <a href="https://speech.ee.ntu.edu.tw/~hylee/index.html" target="_blank">Hung-Yi Lee</a>,
                            <a href="https://www.csie.ntu.edu.tw/~yvchen/" target="_blank">Yun-Nung Chen</a>,
                            <a href="https://swdanielli.github.io/" target="_blank">Shang-Wen Li</a>.
                            <br/>
                            In <a href="https://aclanthology.org/events/woah-2021/" target="_blank">
                                <b>
                                    The 5th Workshop on Online Abuse and Harms at The Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing
                                    (WOAH@ACL-IJCNLP)</b></a>, 2021.
                            <br/>
                            [<a href="bibs/chuang2021mitigating.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#chuang2021mitigating_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://aclanthology.org/2021.woah-1.12.pdf" target="_blank">pdf</a>]
                            [<a href="https://github.com/voidism/invrat_debias" target="_blank">code</a>]
                            [<a href="https://www.youtube.com/watch?v=bAX8oL_ywpQ" target="_blank">video</a>]
                            <!-- [<a href="https://leaderboard.allenai.org/quoref" target="_blank">leaderboard</a>] -->
                            <div id="chuang2021mitigating_abstract" class="abstract" style="display:none;">
                                <p>
                                    Automatic detection of toxic language plays an essential role in protecting social media users, especially minority groups, from verbal abuse. However, biases toward some attributes, including gender, race, and dialect, exist in most training datasets for toxicity detection. The biases make the learned models unfair and can even exacerbate the marginalization of people. Considering that current debiasing methods for general natural language understanding tasks cannot effectively mitigate the biases in the toxicity detectors, we propose to use invariant rationalization (InvRat), a game-theoretic framework consisting of a rationale generator and a predictor, to rule out the spurious correlation of certain syntactic patterns (e.g., identity mentions, dialect) to toxicity labels. We empirically show that our method yields lower false positive rate in both lexical and dialectal attributes than previous debiasing methods.
                                </p>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <a href="https://arxiv.org/abs/2010.13826" target="_blank">
                                <b>Semi-Supervised Spoken Language Understanding via Self-Supervised Speech and Language Model Pretraining</b>
                            </a>
                            <br/>
                            <a href="http://people.csail.mit.edu/clai24/" target="_blank">Cheng-I Lai</a>,
                            <b>Yung-Sung Chuang</b>,
                            <a href="https://speech.ee.ntu.edu.tw/~hylee/index.html" target="_blank">Hung-Yi Lee</a>,
                            <a href="https://swdanielli.github.io/" target="_blank">Shang-Wen Li</a>,
                            <a href="https://www.csail.mit.edu/person/jim-glass" target="_blank">James Glass</a>.
                            <br/>
                            In <a href="https://2021.ieeeicassp.org/" target="_blank">
                                <b>
                                    IEEE International Conference on Acoustics, Speech and Signal Processing
                                    (ICASSP)</b></a>, 2021.
                            <br/>
                            [<a href="bibs/lai2020semi.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#lai2020semi_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://arxiv.org/pdf/2010.13826.pdf" target="_blank">pdf</a>]
                            [<a href="https://github.com/jefflai108/Semi-Supervsied-Spoken-Language-Understanding-PyTorch" target="_blank">code</a>]
                            [<a href="https://www.youtube.com/watch?v=5GOIPp9hWkY" target="_blank">video</a>]
                            <!-- [<a href="https://leaderboard.allenai.org/quoref" target="_blank">leaderboard</a>] -->
                            <div id="lai2020semi_abstract" class="abstract" style="display:none;">
                                <p>
                                    Much recent work on Spoken Language Understanding (SLU) is limited in at least one of three ways: models were trained on oracle text input and neglected ASR errors, models were trained to predict only intents without the slot values, or models were trained on a large amount of in-house data. In this paper, we propose a clean and general framework to learn semantics directly from speech with semi-supervision from transcribed or untranscribed speech to address these issues. Our framework is built upon pretrained end-to-end (E2E) ASR and self-supervised language models, such as BERT, and fine-tuned on a limited amount of target SLU data. We study two semi-supervised settings for the ASR component: supervised pretraining on transcribed speech, and unsupervised pretraining by replacing the ASR encoder with self-supervised speech representations, such as wav2vec. In parallel, we identify two essential criteria for evaluating SLU models: environmental noise-robustness and E2E semantics evaluation. Experiments on ATIS show that our SLU framework with speech as input can perform on par with those using oracle text as input in semantics understanding, even though environmental noise is present and a limited amount of labeled semantics data is available for training.
                                </p>
                            </div>
                        </li>
                    </ul>
                    <h3>2020</h3>
                    <ul class="pl">
                        <li>
                            <a href="https://arxiv.org/abs/2010.02123" target="_blank">
                                <b>Lifelong Language Knowledge Distillation</b>
                            </a>
                            <br/>
                            <b>Yung-Sung Chuang</b>, <a href="https://www.shangyusu.com/" target="_blank">Shang-Yu Su</a>,
                            <a href="https://www.csie.ntu.edu.tw/~yvchen/" target="_blank">Yun-Nung Chen</a>.
                            <br/>
                            In <a href="https://2020.emnlp.org/" target="_blank">
                                <b>
                                    The Conference on Empirical Methods in Natural Language Processing (EMNLP)</b></a>, 2020.
                            <br/>
                            [<a href="bibs/chuang2020lifelong.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#chuang2020lifelong_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://aclanthology.org/2020.emnlp-main.233.pdf" target="_blank">pdf</a>]
                            [<a href="https://github.com/voidism/L2KD" target="_blank">code</a>]
                            [<a href="https://slideslive.com/38938863/lifelong-language-knowledge-distillation" target="_blank">video</a>]
                            <!-- [<a href="https://leaderboard.allenai.org/quoref" target="_blank">leaderboard</a>] -->
                            <div id="chuang2020lifelong_abstract" class="abstract" style="display:none;">
                                <p>
                                    It is challenging to perform lifelong language learning (LLL) on a stream of different tasks without any performance degradation comparing to the multi-task counterparts. To address this issue, we present Lifelong Language Knowledge Distillation (L2KD), a simple but efficient method that can be easily applied to existing LLL architectures in order to mitigate the degradation. Specifically, when the LLL model is trained on a new task, we assign a teacher model to first learn the new task, and pass the knowledge to the LLL model via knowledge distillation. Therefore, the LLL model can better adapt to the new task while keeping the previously learned knowledge. Experiments show that the proposed L2KD consistently improves previous state-of-the-art models, and the degradation comparing to multi-task models in LLL tasks is well mitigated for both sequence generation and text classification tasks.
                                </p>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <a href="https://arxiv.org/abs/2010.04246" target="_blank">
                                <b>Dual Inference for Improving Language Understanding and Generation</b>
                            </a>
                            <br/>
                            <b>Yung-Sung Chuang*</b>, <a href="https://www.shangyusu.com/" target="_blank">Shang-Yu Su*</a>,
                            <a href="https://www.csie.ntu.edu.tw/~yvchen/" target="_blank">Yun-Nung Chen</a>.
                            <br/>
                            In <a href="https://2020.emnlp.org/" target="_blank">
                                <b>
                                    The Conference on Empirical Methods in Natural Language Processing: Findings (EMNLP)</b></a>, 2020.
                            <br/>
                            [<a href="bibs/su2020dual.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#su2020dual_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://aclanthology.org/2020.findings-emnlp.443.pdf" target="_blank">pdf</a>]
                            [<a href="https://github.com/MiuLab/DuaLUG" target="_blank">code</a>]
                            <!-- [<a href="https://slideslive.com/38938863/lifelong-language-knowledge-distillation" target="_blank">video</a>] -->
                            <!-- [<a href="https://leaderboard.allenai.org/quoref" target="_blank">leaderboard</a>] -->
                            <div id="su2020dual_abstract" class="abstract" style="display:none;">
                                <p>
                                    Natural language understanding (NLU) and Natural language generation (NLG) tasks hold a strong dual relationship, where NLU aims at predicting semantic labels based on natural language utterances and NLG does the opposite. The prior work mainly focused on exploiting the duality in model training in order to obtain the models with better performance. However, regarding the fast-growing scale of models in the current NLP area, sometimes we may have difficulty retraining whole NLU and NLG models. To better address the issue, this paper proposes to leverage the duality in the inference stage without the need of retraining. The experiments on three benchmark datasets demonstrate the effectiveness of the proposed method in both NLU and NLG, providing the great potential of practical usage.
                                </p>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <a href="https://arxiv.org/abs/1910.11559" target="_blank">
                                <b>SpeechBERT: An Audio-and-text Jointly Learned Language Model for End-to-end Spoken Question Answering</b>
                            </a>
                            <br/>
                            <b>Yung-Sung Chuang</b>, <a href="https://liangtaiwan.github.io/" target="_blank">Chi-Liang Liu</a>,
                            <a href="https://speech.ee.ntu.edu.tw/~hylee/index.html" target="_blank"> Hung-Yi Lee</a>, <a href="https://speech.ee.ntu.edu.tw/previous_version/lslNew.htm" target="_blank">Lin-shan Lee</a>.
                            <br/>
                            In <a href="http://www.interspeech2020.org/" target="_blank">
                                <b>
                                    Interspeech</b></a>, 2020. <a class="emph"><b>(Interspeech 2020 Travel Grant)</b></a>
                            <br/>
                            [<a href="bibs/chuang2020speechbert.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#chuang2020speechbert_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://www.isca-speech.org/archive/pdfs/interspeech_2020/chuang20b_interspeech.pdf" target="_blank">pdf</a>]
                            <!-- [<a href="https://github.com/MiuLab/DuaLUG" target="_blank">code</a>] -->
                            [<a href="https://www.youtube.com/watch?v=7mf7nSh8dGE" target="_blank">video</a>]
                            <!-- [<a href="https://leaderboard.allenai.org/quoref" target="_blank">leaderboard</a>] -->
                            <div id="chuang2020speechbert_abstract" class="abstract" style="display:none;">
                                <p>
                                    While various end-to-end models for spoken language understanding tasks have been explored recently, this paper is probably the first known attempt to challenge the very difficult task of end-to-end spoken question answering (SQA). Learning from the very successful BERT model for various text processing tasks, here we proposed an audio-and-text jointly learned SpeechBERT model. This model outperformed the conventional approach of cascading ASR with the following text question answering (TQA) model on datasets including ASR errors in answer spans, because the end-to-end model was shown to be able to extract information out of audio data before ASR produced errors. When ensembling the proposed end-to-end model with the cascade architecture, even better performance was achieved. In addition to the potential of end-to-end SQA, the SpeechBERT can also be considered for many other spoken language understanding tasks just as BERT for many text processing tasks.
                                </p>
                            </div>
                        </li>
                    </ul>
                    <h3>2019</h3>
                    <ul class="pl">
                        <li>
                            <a href="https://arxiv.org/abs/1910.01462" target="_blank">
                                <b>Towards Understanding of Medical Randomized Controlled Trials by Conclusion Generation</b>
                            </a>
                            <br/>
                            <a href="https://lipolysis.github.io/" target="_blank">Alexander Te-Wei Shieh*</a>, <b>Yung-Sung Chuang*</b>, <a href="https://www.shangyusu.com/" target="_blank">Shang-Yu Su</a>,
                            <a href="https://www.csie.ntu.edu.tw/~yvchen/" target="_blank">Yun-Nung Chen</a>.
                            <br/>
                            In <a href="https://2019.emnlp.org/" target="_blank">
                                <b>
                                    Proceedings of the Tenth International Workshop on Health Text Mining and Information Analysis (LOUHI 2019) at The 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing</b></a>, 2019.
                            <br/>
                            [<a href="bibs/shieh2019towards.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#shieh2019towards_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://aclanthology.org/D19-6214.pdf" target="_blank">pdf</a>]
                            [<a href="https://github.com/MiuLab/RCT-Gen" target="_blank">code</a>]
                            <!-- [<a href="https://slideslive.com/38938863/lifelong-language-knowledge-distillation" target="_blank">video</a>] -->
                            <!-- [<a href="https://leaderboard.allenai.org/quoref" target="_blank">leaderboard</a>] -->
                            <div id="shieh2019towards_abstract" class="abstract" style="display:none;">
                                <p>
                                    Randomized controlled trials (RCTs) represent the paramount evidence of clinical medicine. Using machines to interpret the massive amount of RCTs has the potential of aiding clinical decision-making. We propose a RCT conclusion generation task from the PubMed 200k RCT sentence classification dataset to examine the effectiveness of sequence-to-sequence models on understanding RCTs. We first build a pointer-generator baseline model for conclusion generation. Then we fine-tune the state-of-the-art GPT-2 language model, which is pre-trained with general domain data, for this new medical domain task. Both automatic and human evaluation show that our GPT-2 fine-tuned models achieve improved quality and correctness in the generated conclusions compared to the baseline pointer-generator model. Further inspection points out the limitations of this current approach and future directions to explore.
                                </p>
                            </div>
                        </li>
                    </ul>
                </div>
            </div>
            <hr>

            <div class="row" id="honors">
                <div class="col">
                    <h2>Honors</h2>
                    <ul>
                        <li>
                            <b>Dean’s list (4 times)</b>, Electrical Engineering Dept. at NTU, Spring ’18, Spring ’19, Fall ’19, Spring ’20
                        </li>
                        <li>
                            <b>Irving T. Ho Memorial Scholarship (2 times)</b>, EECS at NTU, Fall ’18, Fall ’19
                        </li>
                        <li>
                            <b>Travel Grant</b>, INTERSPEECH 2020 conference, Sep. 2020
                        </li>
                        <li>
                            <b>Appier Best Application Award</b>, 2020 NTU CSIE Undergrad Special Research Exhibition, Jun. 2020
                        </li>
                        <li>
                            <b>2nd Place & Appier 1st Award</b>, 2019 NTU CSIE Undergrad Special Research Exhibition, Jun. 2019
                        </li>
                        <li>
                            <b>2nd Place</b>, 2019 NTUEE Undergraduate Innovation Award, Jun. 2019
                        </li>
                        <li>
                            <b>1st Place</b>, 2018 H. Spectrum Demo Day (out of 21 teams), Jul. 2018
                        </li>
                        <li>
                            <b>1st Place</b>, NCTS Health Hackathon 2018 (out of 18 teams), Jun. 2018
                        </li>
                        <li>
                            <b>Top 8 Finalist</b>, Microsoft Imagine Cup Taiwan National Final 2018, Apr. 2018
                        </li>
                        <li>
                            <b>Best Tech Award & Microsoft Enterprise Award</b>, MakeNTU 2018 (out of 50 teams), Mar. 2018
                        </li>
                        <li>
                            <b>1st place of Dept. of Transportation</b>, HackNTU 2017 (out of 100+ teams), Jul. 2017
                        </li>
                    </ul>
                </div>
            </div>
            <hr>
            <div class="row" id="services">
                <div class="col">
                    <h2>Services</h2>
                    <ul>
                        <li>
                            <b>Reviewer:</b> <br/>
                              NeurIPS 2021, 2022, 2023, 2024<br/>
                              ICLR 2022, 2023, 2024, 2025<br/>
                              ICML 2022, 2023, 2024, 2025<br/>
                              ACL ARR 2023, 2024, 2025<br/>
                              EMNLP 2022, 2023<br/>
                              ACL 2023<br/>
                              AAAI 2023<br/>
                              ICASSP 2022, 2023<br/>
                              TASL 2023, 2024, 2025<br/>
                        </li>
                    </ul>
                </div>
            </div>
            <hr>
            <div class="row">
                <div class="col">
                    <ul>
                        <li>
                            This website is built from the <a href="https://github.com/nelson-liu/website">source code</a> of Nelson F. Liu's awesome website (<a href="https://nelsonliu.me/"
                            target="_blank">https://nelsonliu.me</a>).
                        </li>
                    </ul>
                </div>
            </div>
            <footer class="pt-2 my-md-2 pt-md-2 border-top">
                <div class="row justify-content-center">
                    <div class="col-6 col-md text-left align-self-center">
                        <p class="h5 text-muted">
                            © Yung-Sung Chuang, 2022
                        </p>
                    </div>
                    <div class="col-6 col-md text-right">
                        <a href="https://www.csail.mit.edu/" class="image-link">
                            <img class="mr-4" src="img/mit_csail_logo.svg" alt="MIT CSAIL logo." height="75">
                        </a>
                        <a href="https://www.mit.edu/" class="image-link">
                            <img src="img/mit_logo.svg" alt="MIT logo." height="50">
                        </a>
                    </div>
                </div>
            </footer>
        </div>
    </body>
</html>
