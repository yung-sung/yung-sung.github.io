<!DOCTYPE html>
<html lang="en">
    <head>
        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-PK2FJE42TV"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-PK2FJE42TV');
        </script>
        <title>Yung-Sung Chuang</title>
        <meta http-equiv="content-type" content="text/html; charset=UTF-8">
        <meta charset="utf-8">
        <meta property="og:url" content="https://people.csail.mit.edu/yungsung/" />
	    <meta property="og:title" content="Yung-Sung Chuang" />
	    <meta property="og:image" content="img/Yung-Sung.jpg" />
	    <meta http-equiv="X-UA-Compatible" content="IE=edge">
	    <meta name="author" content="Yung-Sung Chuang">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
        <link rel="shortcut icon" type="image/png" href="favicon.ico"/>

        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha512-iecdLmaskl7CVkqkXNQ/ZH/XLlvWZOJyj7Yy7tcenmpD1ypASozpmT/E0iPtmFIB46ZmdtAc9eNBvH0H/ZpiBw==" crossorigin="anonymous">
        <link rel="stylesheet" href="css/style-08102025.css">
        <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
        <script src="citations/scholar.js"></script>
        <link href='https://fonts.googleapis.com/css?family=EB+Garamond' rel='stylesheet' type='text/css'>
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
        <style>
            /* Add some padding on document's body to prevent the content
            to go underneath the header and footer */
            body{        
                padding-top: 50px;
                padding-bottom: 0px;
            }
            html {
                scroll-padding-top: 70px; /* height of sticky header */
            }
            .container{
                width: 80%;
                margin: 0 auto; /* Center the DIV horizontally */
            }
            .fixed-header, .fixed-footer{
                width: 100%;
                position: fixed;        
                /* background: rgb(69, 142, 226); */
                background: rgb(126, 103, 77);
                padding: 7px 0;
                color: #fff;
                font-size: calc(6px + 0.78125vw);
            }
            .fixed-header{
                top: 0;
            }
            .fixed-footer{
                bottom: 0;
            }    
            /* Some more styles to beutify this example (85, 129, 212) */
            a {
                color: #517cc8;
            }
            a:hover {
                color: #204487;
            }
            .emph {
                color: rgb(23, 111, 23) !important;
            }
            nav a{
                color: #fff;
                text-decoration: none;
                padding: 7px 1.7vw;
                display: inline-block;
            }
            h1 span {
                font-size: 20pt;
            }
            h1 span a{
                font-size: 12pt;
            }
            /* set the font color of <a href...> to be brown */
        </style>
    </head>
    <body>
        <div class="fixed-header" style="z-index:1000;">
            <nav>
                <div class="nav-brand">
                    <!-- <i class="fas fa-user-graduate"></i> -->
                    <a href="#" onclick="window.scrollTo({top: 0, behavior: 'smooth'}); return false;" title="Back to top">
                        Yung-Sung Chuang
                    </a>
                </div>
                <div class="nav-links">
                    <a href="https://scholar.google.com/citations?hl=en&user=3ar1DOwAAAAJ" target="_blank" title="Google Scholar">
                        <i class="ai ai-google-scholar"></i>Google Scholar
                        <span class="citation-badge" id="citationBadge">...</span>
                    </a>
                    <a href="files/CV.pdf" target="_blank" title="Download CV">
                        <i class="fas fa-file-pdf"></i>CV
                    </a>
                    <a href="https://twitter.com/YungSungChuang" target="_blank" title="Twitter">
                        <i class="fab fa-twitter"></i>Twitter
                    </a>
                    <a href="https://github.com/voidism" target="_blank" title="Github">
                        <i class="fab fa-github"></i>Github
                    </a>
                    <a href="https://www.linkedin.com/in/yschuang" target="_blank" title="LinkedIn">
                        <i class="fab fa-linkedin"></i>LinkedIn
                    </a>
                    <a href="https://voidism.github.io/" target="_blank" title="Visit my personal blog">
                        <i class="fas fa-blog"></i>Blog
                    </a>
                </div>
            </nav>
        </div>
        <div class="container mt-5" style="z-index:0;">
            <div class="bio-section">
                <div class="bio-item">
                    <div class="bio-photo">
                        <img src="img/Yung-Sung.jpg" alt="Yung-Sung Chuang">
                        <div class="info-card">
                            <!-- <h5>Yung-Sung Chuang</h5> -->
                            <p class="title">
                                MIT EECS PhD Student @ CSAIL<br>
                                Office: 32-G436<br>
                                <br>
                                Email: yungsung [AT] mit.edu
                            </p>
                        </div>
                    </div>
                    <div class="bio-content">
                        <h1>Yung-Sung Chuang <span>(莊永松) <a href="#" onclick="$('#my_name').toggle();return false;"><i>How to pronounce?</i></a></span></h1>
                        
                        <div id="my_name" class="abstract" style="display:none;">
                            <p style="color:gray">
                                First name: Yung-Sung (永松) → pronounced as "yong song"<br>
                                Last name: Chuang (莊) → pronounced as "jwang"<br>
                                </br>
                                <audio controls>
                                    <source src="files/name-tts-long.mp3" type="audio/mpeg">
                                </audio>
                            </p>
                        </div>

                        <p>
                            I am a fourth-year PhD student working with <a href="https://www.csail.mit.edu/person/jim-glass" target="_blank">Jim Glass</a> at <a href="https://www.csail.mit.edu/" target="_blank">MIT CSAIL</a>. My research focuses on large language models: hallucinations, factuality, and retrieval-augmented generation (RAG) of LLMs. In addition, I worked on pre-training <a href="https://arxiv.org/abs/2507.22062" target="_blank">MetaCLIP 2</a>, a multilingual vision-language model pre-trained on worldwide web-scale data, during my internship at Meta FAIR.
                        </p>
                        <p>
                            My research has introduced several approaches for improving LLM factuality. <a href="https://arxiv.org/abs/2309.03883" target="_blank">DoLa</a> enhances factuality through layer-wise knowledge contrasting during decoding. <a href="https://arxiv.org/abs/2407.07071" target="_blank">Lookback Lens</a> detects and mitigates hallucinations by analyzing attention patterns under RAG settings. Most recently, <a href="https://arxiv.org/abs/2502.09604" target="_blank">SelfCite</a> enables LLMs to generate citations in RAG without external supervision. I also used to work on retrieval methods, developing <a href="https://arxiv.org/abs/2204.10298" target="_blank">DiffCSE</a> for better sentence embeddings and <a href="https://arxiv.org/abs/2305.17080" target="_blank">Query Reranking</a> for more accurate passage retrieval.
                        </p>
                        <p>
                            Before MIT, I conducted research in speech processing and NLP with <a href="https://speech.ee.ntu.edu.tw/~hylee/index.html">Hung-Yi Lee</a>, <a href="https://www.csie.ntu.edu.tw/~yvchen/">Yun-Nung Chen</a>, and <a href="https://speech.ee.ntu.edu.tw/previous_version/lslNew.htm">Lin-shan Lee</a> at National Taiwan University, where I obtained my B.S. degree in Electrical Engineering in 2020.
                        </p>
                    </div>
                </div>
            </div>
            <div class="news-section">
                <h2>Recent News</h2>
                <ul class="news-list">
                    <li class="news-item">
                        <div class="news-date">05/2025</div>
                        <div class="news-content">
                            Start my summer internship at FAIR, Meta with <a href="https://scholar.google.com/citations?user=UjpbO6IAAAAJ&hl=en" target="_blank">Luke Zettlemoyer</a>.
                        </div>
                    </li>
                    <li class="news-item">
                        <div class="news-date">04/2025</div>
                        <div class="news-content">
                            My recent talk at MIT EI Seminar <a href="https://youtu.be/oEFRH80LndM?si=uM_lb2sUm1NVmezT" target="_blank">"Reducing Hallucinations in LLMs via Decoding, Detection, and Citation"</a> is on Youtube!
                        </div>
                    </li>
                    <li class="news-item">
                        <div class="news-date">05/2024</div>
                        <div class="news-content">
                            Start my summer internship at FAIR, Meta with <a href="https://howardhsu.github.io/" target="_blank">Hu Xu</a>, <a href="https://swdanielli.github.io/" target="_blank">Daniel Li</a>, <a href="https://scottyih.org/" target="_blank">Scott Yih</a>. See our <a href="https://arxiv.org/abs/2507.22062" target="_blank">MetaCLIP 2 paper</a>.
                        </div>
                    </li>
                    <li class="news-item">
                        <div class="news-date">06/2023</div>
                        <div class="news-content">
                            Start my summer internship at Microsoft with <a href="https://scholar.google.com/citations?user=TS1RoxAAAAAJ&hl=en" target="_blank">Pengcheng He</a>, <a href="https://sites.google.com/view/yujia" target="_blank">Yujia Xie</a>. See our <a href="https://arxiv.org/abs/2309.03883" target="_blank">DoLa paper</a>.
                        </div>
                    </li>
                    <li class="news-item">
                        <div class="news-date">06/2022</div>
                        <div class="news-content">
                            Start my summer internship at MIT-IBM Watson AI Lab with <a href="https://mitibmwatsonailab.mit.edu/people/yang-zhang/" target="_blank">Yang Zhang</a>, <a href="https://mitibmwatsonailab.mit.edu/people/shiyu-chang/" target="_blank">Shiyu Chang</a>, <a href="https://people.csail.mit.edu/yoonkim/" target="_blank">Yoon Kim</a>, <a href="https://scholar.google.com/citations?user=uEpr4C4AAAAJ&hl=en" target="_blank">Kaizhi Qian</a>.
                        </div>
                    </li>
                    <li class="news-item">
                        <div class="news-date">09/2021</div>
                        <div class="news-content">
                            Start my PhD journey at MIT, Cambridge, MA.
                        </div>
                    </li>
                </ul>
            </div>
            <div class="row" id="talks">
                <div class="col">
                    <h2>Talks</h2>
                    <ul class="talks-list">
                        <li>
                            <div class="talk-item">
                                <div class="talk-video">
                                    <iframe src="https://www.youtube.com/embed/oEFRH80LndM?si=nuyL3AsfZxjw4Tf3" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
                                </div>
                                <div class="talk-content">
                                    <h4>
                                        <b>Reducing Hallucinations in LLMs via Decoding, Detection, and Citation</b>
                                    </h4>
                                    <div class="venue">
                                        MIT CSAIL: EI Seminar, April 24, 2025
                                    </div>
                                    <div class="description">
                                        A comprehensive overview of my research on reducing hallucinations in large language models through three key approaches: improved decoding strategies (<a href="https://arxiv.org/abs/2309.03883" target="_blank">DoLa</a>), detection methods (<a href="https://arxiv.org/abs/2407.07071" target="_blank">Lookback Lens</a>), and citation generation (<a href="https://arxiv.org/abs/2502.09604" target="_blank">SelfCite</a>).
                                    </div>
                                </div>
                            </div>
                        </li>
                    </ul>
                    <p><i>For my other talks at conferences, please see <a href="https://www.youtube.com/@yung-sung" target="_blank">https://www.youtube.com/@yung-sung</a></i></p>
                </div>
            </div>
            
            <br/>
            <div class="row" id="publications">
                <div class="col">
                    <h2>Selected Publications</h2>
                    <p><i>For a full list of papers, see my <a href="https://scholar.google.com/citations?hl=en&user=3ar1DOwAAAAJ" target="_blank">Google Scholar</a>.</i></p>
                    
                    <h3> Multi-modal Large-scale Pre-training </h3>

                    <ul class="pl">
                        <li>
                            <div class="pub-item">
                                <div class="pub-teaser">
                                    <img src="teaser/metaclip2.jpg" alt="MetaCLIP 2 teaser">
                                </div>
                                <div class="pub-content">
                                    <h4>
                                        <a href="https://arxiv.org/abs/2507.22062" target="_blank">
                                            <b>MetaCLIP 2: A Worldwide Scaling Recipe</b>
                                        </a>
                                    </h4>
                                    <div class="authors">
                                        <b>Yung-Sung Chuang</b>,
                                        Yang Li,
                                        Dong Wang,
                                        <a href="https://scholar.google.com/citations?user=P7ma7pAAAAAJ&hl=en" target="_blank">Ching-Feng Yeh</a>,
                                        Kehan Lyu,
                                        Ramya Raghavendra,
                                        <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>,
                                        Lifei Huang,
                                        <a href="https://scholar.google.com/citations?user=lMkTx0EAAAAJ&hl=en" target="_blank">Jason Weston</a>,
                                        <a href="https://www.cs.washington.edu/people/faculty/lsz" target="_blank">Luke Zettlemoyer</a>,
                                        <a href="https://xinleic.xyz/" target="_blank">Xinlei Chen</a>,
                                        <a href="https://liuzhuang13.github.io/" target="_blank">Zhuang Liu</a>,
                                        <a href="https://www.sainingxie.com/" target="_blank">Saining Xie</a>,
                                        <a href="https://scottyih.org/" target="_blank">Wen-tau Yih</a>,
                                        <a href="https://swdanielli.github.io/" target="_blank">Shang-Wen Li</a>,
                                        <a href="https://howardhsu.github.io/" target="_blank">Hu Xu</a>
                                    </div>
                                    <div class="venue">
                                        arXiv preprint arXiv:2507.22062, 2025.
                                    </div>
                                    <div class="links">
                                        <a href="bibs/chuang2025metaclip2.bib" target="_blank" type="text/plain">
                                            <i class="fas fa-quote-left"></i>bib
                                        </a>
                                        <a href="#" onclick="$('#metaclip2_abstract').toggle();return false;">
                                            <i class="fas fa-align-left"></i>abstract
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.22062.pdf" target="_blank">
                                            <i class="fas fa-file-pdf"></i>pdf
                                        </a>
                                        <a href="https://github.com/facebookresearch/MetaCLIP" target="_blank">
                                            <i class="fab fa-github"></i>code
                                        </a>
                                    </div>
                                    <div id="metaclip2_abstract" class="abstract" style="display:none;">
                                        <p>
                                            Contrastive Language-Image Pretraining (CLIP) is a popular foundation model, supporting from zero-shot classification, retrieval to encoders for multimodal large language models (MLLMs). Although CLIP is successfully trained on billion-scale image-text pairs from the English world, scaling CLIP's training further to learning from the worldwide web data is still challenging: (1) no curation method is available to handle data points from non-English world; (2) the English performance from existing multilingual CLIP is worse than its English-only counterpart, i.e., "curse of multilinguality" that is common in LLMs. Here, we present Meta CLIP 2, the first recipe training CLIP from scratch on worldwide web-scale image-text pairs. To generalize our findings, we conduct rigorous ablations with minimal changes that are necessary to address the above challenges and present a recipe enabling mutual benefits from English and non-English world data. In zero-shot ImageNet classification, Meta CLIP 2 ViT-H/14 surpasses its English-only counterpart by 0.8% and mSigLIP by 0.7%, and surprisingly sets new state-of-the-art without system-level confounding factors (e.g., translation, bespoke architecture changes) on multilingual benchmarks, such as CVQA with 57.4%, Babel-ImageNet with 50.2% and XM3600 with 64.3% on image-to-text retrieval.
                                        </p>
                                    </div>
                                </div>
                            </div>
                        </li>
                    </ul>
                    
                    <h3> Factuality & Transparency</h3>
                    <ul class="pl">
                        <li>
                            <div class="pub-item">
                                <div class="pub-teaser">
                                    <img src="teaser/selfcite.jpg" alt="SelfCite teaser">
                                </div>
                                <div class="pub-content">
                                    <h4>
                                        <a href="https://arxiv.org/abs/2502.09604" target="_blank">
                                            <b>SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models</b>
                                        </a>
                                    </h4>
                                    <div class="authors">
                                        <b>Yung-Sung Chuang</b>,
                                        <a href="https://bencw99.github.io/" target="_blank">Benjamin Cohen-Wang</a>,
                                        <a href="https://www.szj.io/" target="_blank">Shannon Zejiang Shen</a>,
                                        <a href="https://zhaofengwu.github.io/" target="_blank">Zhaofeng Wu</a>,
                                        <a href="https://howardhsu.github.io/" target="_blank">Hu Xu</a>,
                                        <a href="https://victorialin.org/" target="_blank">Xi Victoria Lin</a>,
                                        <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>,
                                        <a href="https://swdanielli.github.io/" target="_blank">Shang-Wen Li</a>,
                                        <a href="https://scottyih.org/" target="_blank">Wen-tau Yih</a>
                                    </div>
                                    <div class="venue">
                                        In <a href="https://icml.cc/Conferences/2025" target="_blank">
                                            <b>The Forty-Second International Conference on Machine Learning (ICML)</b></a>, 2025.
                                    </div>
                                    <div class="links">
                                        <a href="bibs/chuang2025selfcite.bib" target="_blank" type="text/plain">
                                            <i class="fas fa-quote-left"></i>bib
                                        </a>
                                        <a href="#" onclick="$('#selfcite_abstract').toggle();return false;">
                                            <i class="fas fa-align-left"></i>abstract
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09604.pdf" target="_blank">
                                            <i class="fas fa-file-pdf"></i>pdf
                                        </a>
                                        <a href="https://github.com/facebookresearch/SelfCite" target="_blank">
                                            <i class="fab fa-github"></i>code
                                        </a>
                                        <a href="https://selfcite.github.io" target="_blank">
                                            <i class="fas fa-globe"></i>web
                                        </a>
                                    </div>
                                    <div id="selfcite_abstract" class="abstract" style="display:none;">
                                        <p>
                                            We introduce <i>SelfCite</i>, a novel self-supervised approach that aligns LLMs to generate high-quality, fine-grained, sentence-level citations for the statements in their generated responses. Instead of only relying on costly and labor-intensive annotations, SelfCite leverages a reward signal provided by the LLM itself through context ablation: If a citation is necessary, removing the cited text from the context should prevent the same response; if sufficient, retaining the cited text alone should preserve the same response. This reward can guide the inference-time best-of-N sampling strategy to improve citation quality significantly, as well as be used in preference optimization to directly fine-tune the models for generating better citations. The effectiveness of SelfCite is demonstrated by increasing citation F1 up to 5.3 points on the LongBench-Cite benchmark across five long-form question answering tasks.
                                        </p>
                                    </div>
                                </div>
                            </div>
                        </li>
                    </ul>

                    <ul class="pl">
                        <li>
                            <div class="pub-item">
                                <div class="pub-teaser">
                                    <img src="teaser/lookbacklens.jpg" alt="Lookback Lens teaser">
                                </div>
                                <div class="pub-content">
                                    <h4>
                                        <a href="https://arxiv.org/abs/2407.07071" target="_blank">
                                            <b>Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps</b>
                                        </a>
                                    </h4>
                                    <div class="authors">
                                        <b>Yung-Sung Chuang</b>,
                                        <a href="https://linlu-qiu.github.io/" target="_blank">Linlu Qiu</a>,
                                        <a href="https://chengyuhsieh.github.io/" target="_blank">Cheng-Yu Hsieh</a>,
                                        <a href="https://www.ranjaykrishna.com/index.html" target="_blank">Ranjay Krishna</a>,
                                        <a href="https://people.csail.mit.edu/yoonkim/" target="_blank">Yoon Kim</a>,
                                        <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
                                    </div>
                                    <div class="venue">
                                        In <a href="https://2024.emnlp.org" target="_blank"><b>The Conference on Empirical Methods in Natural Language Processing</b></a>, 2024.
                                    </div>
                                    <div class="links">
                                        <a href="bibs/chuang2024lookback.bib" target="_blank" type="text/plain">
                                            <i class="fas fa-quote-left"></i>bib
                                        </a>
                                        <a href="#" onclick="$('#lookback_abstract').toggle();return false;">
                                            <i class="fas fa-align-left"></i>abstract
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07071.pdf" target="_blank">
                                            <i class="fas fa-file-pdf"></i>pdf
                                        </a>
                                        <a href="https://github.com/voidism/Lookback-Lens" target="_blank">
                                            <i class="fab fa-github"></i>code
                                        </a>
                                    </div>
                                    <div id="lookback_abstract" class="abstract" style="display:none;">
                                        <p>
                                            When asked to summarize articles or answer questions given a passage, large language models (LLMs) can hallucinate details and respond with unsubstantiated answers that are inaccurate with respect to the input context. This paper describes a simple approach for detecting such contextual hallucinations. We hypothesize that contextual hallucinations are related to the extent to which an LLM attends to information in the provided context versus its own generations. Based on this intuition, we propose a simple hallucination detection model whose input features are given by the ratio of attention weights on the context versus newly generated tokens (for each attention head). We find that a linear classifier based on these lookback ratio features is as effective as a richer detector that utilizes the entire hidden states of an LLM or a text-based entailment model. The lookback ratio-based detector -- Lookback Lens -- is found to transfer across tasks and even models, allowing a detector that is trained on a 7B model to be applied (without retraining) to a larger 13B model. We further apply this detector to mitigate contextual hallucinations, and find that a simple classifier-guided decoding approach is able to reduce the amount of hallucination, for example by 9.6% in the XSum summarization task.
                                        </p>
                                    </div>
                                </div>
                            </div>
                        </li>
                    </ul>

                    <ul class="pl">
                        <li>
                            <div class="pub-item">
                                <div class="pub-teaser">
                                    <img src="teaser/dola.jpg" alt="DoLa teaser">
                                </div>
                                <div class="pub-content">
                                    <h4>
                                        <a href="https://arxiv.org/abs/2309.03883" target="_blank">
                                            <b>DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models</b>
                                        </a>
                                    </h4>
                                    <div class="authors">
                                        <b>Yung-Sung Chuang</b>,
                                        <a href="https://sites.google.com/view/yujia" target="_blank">Yujia Xie</a>,
                                        <a href="https://luohongyin.github.io/" target="_blank">Hongyin Luo</a>,
                                        <a href="https://people.csail.mit.edu/yoonkim/" target="_blank">Yoon Kim</a>,
                                        <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>,
                                        <a href="https://scholar.google.com/citations?user=TS1RoxAAAAAJ&hl=en" target="_blank">Pengcheng He</a>
                                    </div>
                                    <div class="venue">
                                        In <a href="https://iclr.cc/" target="_blank">
                                            <b>The Twelfth International Conference on Learning Representations (ICLR)</b></a>, 2024.
                                    </div>
                                    <div class="links">
                                        <a href="bibs/chuang2023dola.bib" target="_blank" type="text/plain">
                                            <i class="fas fa-quote-left"></i>bib
                                        </a>
                                        <a href="#" onclick="$('#chuang2023dola_abstract').toggle();return false;">
                                            <i class="fas fa-align-left"></i>abstract
                                        </a>
                                        <a href="https://browse.arxiv.org/pdf/2309.03883.pdf" target="_blank">
                                            <i class="fas fa-file-pdf"></i>pdf
                                        </a>
                                        <a href="https://github.com/voidism/DoLa" target="_blank">
                                            <i class="fab fa-github"></i>code
                                        </a>
                                    </div>
                                    <div id="chuang2023dola_abstract" class="abstract" style="display:none;">
                                        <p>
                                            Despite their impressive capabilities, large language models (LLMs) are prone to hallucinations, i.e., generating content that deviates from facts seen during pretraining. We propose a simple decoding strategy for reducing hallucinations with pretrained LLMs that does not require conditioning on retrieved external knowledge nor additional fine-tuning. Our approach obtains the next-token distribution by contrasting the differences in logits obtained from projecting the later layers versus earlier layers to the vocabulary space, exploiting the fact that factual knowledge in an LLMs has generally been shown to be localized to particular transformer layers. We find that this Decoding by Contrasting Layers (DoLa) approach is able to better surface factual knowledge and reduce the generation of incorrect facts. DoLa consistently improves the truthfulness across multiple choices tasks and open-ended generation tasks, for example improving the performance of LLaMA family models on TruthfulQA by 12-17% absolute points, demonstrating its potential in making LLMs reliably generate truthful facts.
                                        </p>
                                    </div>
                                </div>
                            </div>
                        </li>
                    </ul>


                    <ul class="pl">
                        <li>
                            <div class="pub-item">
                                <div class="pub-teaser">
                                    <img src="teaser/at2.jpg" alt="AT2 teaser">
                                </div>
                                <div class="pub-content">
                                    <h4>
                                        <a href="https://arxiv.org/abs/2504.13752" target="_blank">
                                            <b>Learning to Attribute with Attention</b>
                                        </a>
                                    </h4>
                                    <div class="authors">
                                        <a href="https://bencw99.github.io/" target="_blank">Benjamin Cohen-Wang</a>,
                                        <b>Yung-Sung Chuang</b>,
                                        <a href="https://madry.mit.edu/" target="_blank">Aleksander Madry</a>
                                    </div>
                                    <div class="venue">
                                        arXiv preprint arXiv:2504.13752, 2025.
                                    </div>
                                    <div class="links">
                                        <a href="bibs/cohenwang2025learning.bib" target="_blank" type="text/plain">
                                            <i class="fas fa-quote-left"></i>bib
                                        </a>
                                        <a href="#" onclick="$('#at2_abstract').toggle();return false;">
                                            <i class="fas fa-align-left"></i>abstract
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.13752.pdf" target="_blank">
                                            <i class="fas fa-file-pdf"></i>pdf
                                        </a>
                                        <a href="https://github.com/MadryLab/AT2" target="_blank">
                                            <i class="fab fa-github"></i>code
                                        </a>
                                    </div>
                                    <div id="at2_abstract" class="abstract" style="display:none;">
                                        <p>
                                            Given a sequence of tokens generated by a language model, we may want to identify the preceding tokens that influence the model to generate this sequence. Performing such token attribution is expensive; a common approach is to ablate preceding tokens and directly measure their effects. To reduce the cost of token attribution, we revisit attention weights as a heuristic for how a language model uses previous tokens. Naive approaches to attribute model behavior with attention (e.g., averaging attention weights across attention heads to estimate a token's influence) have been found to be unreliable. To attain faithful attributions, we propose treating the attention weights of different attention heads as features. This way, we can learn how to effectively leverage attention weights for attribution (using signal from ablations). Our resulting method, Attribution with Attention (AT2), reliably performs on par with approaches that involve many ablations, while being significantly more efficient. To showcase the utility of AT2, we use it to prune less important parts of a provided context in a question answering setting, improving answer quality.
                                        </p>
                                    </div>
                                </div>
                            </div>
                        </li>
                    </ul>
                    
                    <h3> Retrieval-based Methods </h3>
                    

                    <ul class="pl">
                        <li>
                            <div class="pub-item">
                                <div class="pub-teaser">
                                    <img src="teaser/jpr.jpg" alt="JPR teaser">
                                </div>
                                <div class="pub-content">
                                    <h4>
                                        <a href="https://aclanthology.org/2024.findings-eacl.151/" target="_blank">
                                            <b>Joint Inference of Retrieval and Generation for Passage Re-ranking</b>
                                        </a>
                                    </h4>
                                    <div class="authors">
                                        <a href="https://people.csail.mit.edu/weifang/" target="_blank">Wei Fang</a>,
                                        <b>Yung-Sung Chuang</b>,
                                        <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
                                    </div>
                                    <div class="venue">
                                        In <a href="https://2024.eacl.org/" target="_blank">
                                            <b>The 18th Conference of the European Chapter of the Association for Computational Linguistics: Findings</b></a>, 2024.
                                    </div>
                                    <div class="links">
                                        <a href="bibs/fang2024joint.bib" target="_blank" type="text/plain">
                                            <i class="fas fa-quote-left"></i>bib
                                        </a>
                                        <a href="#" onclick="$('#fang2024joint_abstract').toggle();return false;">
                                            <i class="fas fa-align-left"></i>abstract
                                        </a>
                                        <a href="https://aclanthology.org/2024.findings-eacl.151/" target="_blank">
                                            <i class="fas fa-file-pdf"></i>pdf
                                        </a>
                                        <a href="https://github.com/wfangtw/jpr" target="_blank">
                                            <i class="fab fa-github"></i>code
                                        </a>
                                    </div>
                                    <div id="fang2024joint_abstract" class="abstract" style="display:none;">
                                        <p>
                                            Passage retrieval is a crucial component of modern open-domain question answering (QA) systems, providing information for downstream QA components to generate accurate and transparent answers. This study focuses on passage re-ranking, proposing a simple yet effective method, Joint Passage Re-ranking (JPR), that optimizes the mutual information between query and passage distributions, integrating both cross-encoders and generative models in the re-ranking process. Experimental results demonstrate that JPR outperforms conventional re-rankers and language model scorers in both open-domain QA retrieval settings and diverse retrieval benchmarks under zero-shot settings.
                                        </p>
                                    </div>
                                </div>
                            </div>
                        </li>
                    </ul>

                    <ul class="pl">
                        <li>
                            <div class="pub-item">
                                <div class="pub-teaser">
                                    <img src="teaser/ear.jpg" alt="EAR teaser">
                                </div>
                                <div class="pub-content">
                                    <h4>
                                        <a href="https://arxiv.org/abs/2305.17080" target="_blank">
                                            <b>Expand, Rerank, and Retrieve: Query Reranking for Open-Domain Question Answering</b>
                                        </a>
                                    </h4>
                                    <div class="authors">
                                        <b>Yung-Sung Chuang</b>,
                                        <a href="https://people.csail.mit.edu/weifang/" target="_blank">Wei Fang</a>,
                                        <a href="https://swdanielli.github.io/" target="_blank">Shang-Wen Li</a>,
                                        <a href="http://scottyih.org/" target="_blank">Wen-tau Yih</a>,
                                        <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
                                    </div>
                                    <div class="venue">
                                        In <a href="https://2023.aclweb.org/" target="_blank">
                                            <b>The 61st Annual Meeting of the Association for Computational Linguistics: Findings</b></a>, 2023.
                                    </div>
                                    <div class="links">
                                        <a href="bibs/chuang2023expand.bib" target="_blank" type="text/plain">
                                            <i class="fas fa-quote-left"></i>bib
                                        </a>
                                        <a href="#" onclick="$('#chuang2023expand_abstract').toggle();return false;">
                                            <i class="fas fa-align-left"></i>abstract
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.17080.pdf" target="_blank">
                                            <i class="fas fa-file-pdf"></i>pdf
                                        </a>
                                        <a href="https://github.com/voidism/EAR" target="_blank">
                                            <i class="fab fa-github"></i>code
                                        </a>
                                    </div>
                                    <div id="chuang2023expand_abstract" class="abstract" style="display:none;">
                                        <p>
                                            We propose EAR, a query Expansion And Reranking approach for improving passage retrieval, with the application to open-domain question answering. EAR first applies a query expansion model to generate a diverse set of queries, and then uses a query reranker to select the ones that could lead to better retrieval results. Motivated by the observation that the best query expansion often is not picked by greedy decoding, EAR trains its reranker to predict the rank orders of the gold passages when issuing the expanded queries to a given retriever. By connecting better the query expansion model and retriever, EAR significantly enhances a traditional sparse retrieval method, BM25. Empirically, EAR improves top-5/20 accuracy by 3-8 and 5-10 points in in-domain and out-of-domain settings, respectively, when compared to a vanilla query expansion model, GAR, and a dense retrieval model, DPR.
                                        </p>
                                    </div>
                                </div>
                            </div>
                        </li>
                    </ul>

                    <ul class="pl">
                        <li>
                            <div class="pub-item">
                                <div class="pub-teaser">
                                    <img src="teaser/sail.jpg" alt="SAIL teaser">
                                </div>
                                <div class="pub-content">
                                    <h4>
                                        <a href="https://arxiv.org/abs/2305.15225" target="_blank">
                                            <b>SAIL: Search-Augmented Instruction Learning</b>
                                        </a>
                                    </h4>
                                    <div class="authors">
                                        <a href="https://luohongyin.github.io/" target="_blank">Hongyin Luo</a>,
                                        <b>Yung-Sung Chuang</b>,
                                        <a href="https://yuangongnd.github.io/" target="_blank">Yuan Gong</a>, Tianhua Zhang, <a href="https://people.csail.mit.edu/yoonkim/" target="_blank">Yoon Kim</a>, Xixin Wu, Danny Fox, Helen Meng, <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
                                    </div>
                                    <div class="venue">
                                        In <a href="https://aclanthology.org/volumes/2023.findings-emnlp/" target="_blank">
                                            <b>Findings of the Association for Computational Linguistics: EMNLP</b></a>, 2023.
                                    </div>
                                    <div class="links">
                                        <a href="bibs/luo2023sail.bib" target="_blank" type="text/plain">
                                            <i class="fas fa-quote-left"></i>bib
                                        </a>
                                        <a href="#" onclick="$('#luo2023sail_abstract').toggle();return false;">
                                            <i class="fas fa-align-left"></i>abstract
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.15225.pdf" target="_blank">
                                            <i class="fas fa-file-pdf"></i>pdf
                                        </a>
                                        <a href="https://github.com/luohongyin/SAIL" target="_blank">
                                            <i class="fab fa-github"></i>code
                                        </a>
                                        <a href="https://huggingface.co/spaces/luohy/SAIL-7B" target="_blank">
                                            <i class="fas fa-play"></i>demo
                                        </a>
                                        <a href="https://openlsr.org/sail-7b" target="_blank">
                                            <i class="fas fa-globe"></i>web
                                        </a>
                                    </div>
                                    <div id="luo2023sail_abstract" class="abstract" style="display:none;">
                                        <p>
                                            Large language models (LLMs) have been significantly improved by instruction fine-tuning, but still lack transparency and the ability to utilize up-to-date knowledge and information. In this work, we propose search-augmented instruction learning (SAIL), which grounds the language generation and instruction following abilities on complex search results generated by in-house and external search engines. With an instruction tuning corpus, we collect search results for each training case from different search APIs and domains, and construct a new search-grounded training set containing \textit{(instruction, grounding information, response)} triplets. We then fine-tune the LLaMA-7B model on the constructed training set. Since the collected results contain unrelated and disputing languages, the model needs to learn to ground on trustworthy search results, filter out distracting passages, and generate the target response. The search result-denoising process entails explicit trustworthy information selection and multi-hop reasoning, since the retrieved passages might be informative but not contain the instruction-following answer. Experiments show that the fine-tuned SAIL-7B model has a strong instruction-following ability, and it performs significantly better on transparency-sensitive tasks, including open-ended question answering and fact checking.
                                        </p>
                                    </div>
                                </div>
                            </div>
                        </li>
                    </ul>
                    
                    <ul class="pl">
                        <li>
                            <div class="pub-item">
                                <div class="pub-teaser">
                                    <img src="teaser/diffcse.jpg" alt="DiffCSE teaser">
                                </div>
                                <div class="pub-content">
                                    <h4>
                                        <a href="https://arxiv.org/abs/2204.10298" target="_blank">
                                            <b>DiffCSE: Difference-based Contrastive Learning for Sentence Embeddings</b>
                                        </a>
                                    </h4>
                                    <div class="authors">
                                        <b>Yung-Sung Chuang</b>, 
                                        <a href="http://super-ms.mit.edu/rumen.html" target="_blank">Rumen Dangovski</a>, 
                                        <a href="http://people.csail.mit.edu/hyluo/" target="_blank">Hongyin Luo</a>, 
                                        <a href="https://mitibmwatsonailab.mit.edu/people/yang-zhang/" target="_blank">Yang Zhang</a>, 
                                        <a href="https://code-terminator.github.io/" target="_blank">Shiyu Chang</a>, 
                                        <a href="http://www.mit.edu/~soljacic/marin.html" target="_blank">Marin Soljačić</a>, 
                                        <a href="https://swdanielli.github.io/" target="_blank">Shang-Wen Li</a>, 
                                        <a href="https://scottyih.org/" target="_blank">Scott Wen-tau Yih</a>, 
                                        <a href="https://people.csail.mit.edu/yoonkim/" target="_blank">Yoon Kim</a>, 
                                        <a href="https://www.csail.mit.edu/person/jim-glass" target="_blank">James Glass</a>.
                                    </div>
                                    <div class="venue">
                                        In <a href="https://2022.naacl.org/" target="_blank">
                                            <b>Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</b></a>, 2022. <a class="emph"><b> (Oral Paper) </b></a>
                                    </div>
                                    <div class="links">
                                        <a href="bibs/chuang2022diffcse.bib" target="_blank" type="text/plain">
                                            <i class="fas fa-quote-left"></i>bib
                                        </a>
                                        <a href="#" onclick="$('#chuang2022diffcse_abstract').toggle();return false;">
                                            <i class="fas fa-align-left"></i>abstract
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.10298.pdf" target="_blank">
                                            <i class="fas fa-file-pdf"></i>pdf
                                        </a>
                                        <a href="https://github.com/voidism/DiffCSE" target="_blank">
                                            <i class="fab fa-github"></i>code
                                        </a>
                                        <a href="https://www.youtube.com/watch?v=9vx-HyzcXtU" target="_blank">
                                            <i class="fab fa-youtube"></i>video
                                        </a>
                                    </div>
                                    <div id="chuang2022diffcse_abstract" class="abstract" style="display:none;">
                                        <p>
                                            We propose DiffCSE, an unsupervised contrastive learning framework for learning sentence embeddings. DiffCSE learns sentence embeddings that are sensitive to the difference between the original sentence and an edited sentence, where the edited sentence is obtained by stochastically masking out the original sentence and then sampling from a masked language model. We show that DiffSCE is an instance of equivariant contrastive learning (Dangovski et al., 2021), which generalizes contrastive learning and learns representations that are insensitive to certain types of augmentations and sensitive to other "harmful" types of augmentations. Our experiments show that DiffCSE achieves state-of-the-art results among unsupervised sentence representation learning methods, outperforming unsupervised SimCSE by 2.3 absolute points on semantic textual similarity tasks.
                                        </p>
                                    </div>
                                </div>
                            </div>
                        </li>
                    </ul>
                    
                    <h3> Speech Processing </h3>
                    <ul class="pl">
                        <li>
                            <div class="pub-item">
                                <div class="pub-teaser">
                                    <img src="teaser/dual.jpg" alt="DUAL teaser">
                                </div>
                                <div class="pub-content">
                                    <h4>
                                        <a href="https://arxiv.org/abs/2203.04911" target="_blank">
                                            <b>DUAL: Discrete Spoken Unit Adaptive Learning for Textless Spoken Question Answering</b>
                                        </a>
                                    </h4>
                                    <div class="authors">
                                        <a href="https://daniellin94144.github.io/" target="_blank">Guan-Ting Lin</a>, 
                                        <b>Yung-Sung Chuang</b>, 
                                        <a href="https://github.com/voidful" target="_blank">Ho-Lam Chung</a>, 
                                        <a href="https://scholar.google.com.tw/citations?user=R1mNI8QAAAAJ" target="_blank">Shu-wen Yang</a>, 
                                        <a href="#">Hsuan-Jui Chen</a>, 
                                        <a href="https://www.amazon.science/author/shuyan-dong" target="_blank">Shuyan Dong</a>, 
                                        <a href="https://swdanielli.github.io/" target="_blank">Shang-Wen Li</a>, 
                                        <a href="http://www.cs.toronto.edu/~asamir/" target="_blank">Abdelrahman Mohamed</a>, 
                                        <a href="https://speech.ee.ntu.edu.tw/~hylee/index.html" target="_blank">Hung-yi Lee</a>, 
                                        <a href="https://speech.ee.ntu.edu.tw/previous_version/lslNew.htm" target="_blank">Lin-shan Lee</a>
                                    </div>
                                    <div class="venue">
                                        In <a href="https://www.interspeech2022.org/" target="_blank">
                                            <b>Interspeech</b></a>, 2022.
                                    </div>
                                    <div class="links">
                                        <a href="bibs/lin2022dual.bib" target="_blank" type="text/plain">
                                            <i class="fas fa-quote-left"></i>bib
                                        </a>
                                        <a href="#" onclick="$('#dual_abstract').toggle();return false;">
                                            <i class="fas fa-align-left"></i>abstract
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.04911.pdf" target="_blank">
                                            <i class="fas fa-file-pdf"></i>pdf
                                        </a>
                                    </div>
                                    <div id="dual_abstract" class="abstract" style="display:none;">
                                        <p>
                                            Spoken Question Answering (SQA) is to find the answer from a spoken document given a question, which is crucial for personal assistants when replying to the queries from the users. Existing SQA methods all rely on Automatic Speech Recognition (ASR) transcripts. Not only does ASR need to be trained with massive annotated data that are time and cost-prohibitive to collect for low-resourced languages, but more importantly, very often the answers to the questions include name entities or out-of-vocabulary words that cannot be recognized correctly. Also, ASR aims to minimize recognition errors equally over all words, including many function words irrelevant to the SQA task. Therefore, SQA without ASR transcripts (textless) is always highly desired, although known to be very difficult. This work proposes Discrete Spoken Unit Adaptive Learning (DUAL), leveraging unlabeled data for pre-training and fine-tuned by the SQA downstream task. The time intervals of spoken answers can be directly predicted from spoken documents. We also release a new SQA benchmark corpus, NMSQA, for data with more realistic scenarios. We empirically showed that DUAL yields results comparable to those obtained by cascading ASR and text QA model and robust to real-world data. Our code and model will be open-sourced.
                                        </p>
                                    </div>
                                </div>
                            </div>
                        </li>
                        <li>
                            <div class="pub-item">
                                <div class="pub-teaser">
                                    <img src="teaser/semi_supervised.jpg" alt="Semi-Supervised Spoken Language Understanding teaser">
                                </div>
                                <div class="pub-content">
                                    <h4>
                                        <a href="https://arxiv.org/abs/2010.13826" target="_blank">
                                            <b>Semi-Supervised Spoken Language Understanding via Self-Supervised Speech and Language Model Pretraining</b>
                                        </a>
                                    </h4>
                                    <div class="authors">
                                        <a href="http://people.csail.mit.edu/clai24/" target="_blank">Cheng-I Lai</a>,
                                        <b>Yung-Sung Chuang</b>,
                                        <a href="https://speech.ee.ntu.edu.tw/~hylee/index.html" target="_blank">Hung-Yi Lee</a>,
                                        <a href="https://swdanielli.github.io/" target="_blank">Shang-Wen Li</a>,
                                        <a href="https://www.csail.mit.edu/person/jim-glass" target="_blank">James Glass</a>
                                    </div>
                                    <div class="venue">
                                        In <a href="https://2021.ieeeicassp.org/" target="_blank">
                                            <b>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</b></a>, 2021.
                                    </div>
                                    <div class="links">
                                        <a href="bibs/lai2020semi.bib" target="_blank" type="text/plain">
                                            <i class="fas fa-quote-left"></i>bib
                                        </a>
                                        <a href="#" onclick="$('#semi_supervised_abstract').toggle();return false;">
                                            <i class="fas fa-align-left"></i>abstract
                                        </a>
                                        <a href="https://arxiv.org/pdf/2010.13826.pdf" target="_blank">
                                            <i class="fas fa-file-pdf"></i>pdf
                                        </a>
                                        <a href="https://github.com/jefflai108/Semi-Supervsied-Spoken-Language-Understanding-PyTorch" target="_blank">
                                            <i class="fab fa-github"></i>code
                                        </a>
                                        <a href="https://www.youtube.com/watch?v=5GOIPp9hWkY" target="_blank">
                                            <i class="fab fa-youtube"></i>video
                                        </a>
                                    </div>
                                    <div id="semi_supervised_abstract" class="abstract" style="display:none;">
                                        <p>
                                            Much recent work on Spoken Language Understanding (SLU) is limited in at least one of three ways: models were trained on oracle text input and neglected ASR errors, models were trained to predict only intents without the slot values, or models were trained on a large amount of in-house data. In this paper, we propose a clean and general framework to learn semantics directly from speech with semi-supervision from transcribed or untranscribed speech to address these issues. Our framework is built upon pretrained end-to-end (E2E) ASR and self-supervised language models, such as BERT, and fine-tuned on a limited amount of target SLU data. We study two semi-supervised settings for the ASR component: supervised pretraining on transcribed speech, and unsupervised pretraining by replacing the ASR encoder with self-supervised speech representations, such as wav2vec. In parallel, we identify two essential criteria for evaluating SLU models: environmental noise-robustness and E2E semantics evaluation. Experiments on ATIS show that our SLU framework with speech as input can perform on par with those using oracle text as input in semantics understanding, even though environmental noise is present and a limited amount of labeled semantics data is available for training.
                                        </p>
                                    </div>
                                </div>
                            </div>
                        </li>
                        <li>
                            <div class="pub-item">
                                <div class="pub-teaser">
                                    <img src="teaser/superb.jpg" alt="SUPERB teaser">
                                </div>
                                <div class="pub-content">
                                    <h4>
                                        <a href="https://arxiv.org/abs/2105.01051" target="_blank">
                                            <b>SUPERB: Speech processing Universal PERformance Benchmark</b>
                                        </a>
                                    </h4>
                                    <div class="authors">
                                        <a href="https://scholar.google.com.tw/citations?user=R1mNI8QAAAAJ" target="_blank">Shu-wen Yang</a>, 
                                        <a href="https://scholar.google.com/citations?user=SiyicoEAAAAJ&hl=zh-TW" target="_blank">Po-Han Chi*</a>, 
                                        <b>Yung-Sung Chuang*</b>, 
                                        <a href="http://people.csail.mit.edu/clai24/" target="_blank">Cheng-I Jeff Lai*</a>, 
                                        <a href="https://ai.facebook.com/people/kushal-lakhotia/" target="_blank">Kushal Lakhotia*</a>, 
                                        <a href="https://scholar.google.com/citations?user=0lrZq9MAAAAJ&hl=en" target="_blank">Yist Y. Lin*</a>, 
                                        <a href="https://andi611.github.io/" target="_blank">Andy T. Liu*</a>, 
                                        <a href="http://shijt.site/" target="_blank">Jiatong Shi*</a>, 
                                        <a href="https://www.lti.cs.cmu.edu/people/222227473/xuankai-chang" target="_blank">Xuankai Chang</a>, 
                                        <a href="https://github.com/DanielLin94144" target="_blank">Guan-Ting Lin</a>, 
                                        <a href="https://tw.linkedin.com/in/tzu-hsien-huang-1686651b6" target="_blank">Tzu-Hsien Huang</a>, 
                                        Wei-Cheng Tseng, 
                                        <a href="https://tw.linkedin.com/in/ko-tik-lee-4747291a2/en?trk=people-guest_people_search-card" target="_blank">Ko-tik Lee</a>, 
                                        <a href="https://scholar.google.com.tw/citations?user=qJ5zXNIAAAAJ" target="_blank">Da-Rong Liu</a>, 
                                        <a href="https://dblp.org/pid/210/0905.html" target="_blank">Zili Huang</a>, 
                                        <a href="https://www.amazon.science/author/shuyan-dong" target="_blank">Shuyan Dong</a>, 
                                        <a href="https://swdanielli.github.io/" target="_blank">Shang-Wen Li</a>, 
                                        <a href="https://sites.google.com/view/shinjiwatanabe" target="_blank">Shinji Watanabe</a>, 
                                        <a href="http://www.cs.toronto.edu/~asamir/" target="_blank">Abdelrahman Mohamed</a>, 
                                        <a href="https://speech.ee.ntu.edu.tw/~hylee/index.html" target="_blank">Hung-Yi Lee</a>
                                    </div>
                                    <div class="venue">
                                        In <a href="https://www.interspeech2021.org/" target="_blank">
                                            <b>Interspeech</b></a>, 2021.
                                    </div>
                                    <div class="links">
                                        <a href="bibs/yang2021superb.bib" target="_blank" type="text/plain">
                                            <i class="fas fa-quote-left"></i>bib
                                        </a>
                                        <a href="#" onclick="$('#superb_abstract').toggle();return false;">
                                            <i class="fas fa-align-left"></i>abstract
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.01051.pdf" target="_blank">
                                            <i class="fas fa-file-pdf"></i>pdf
                                        </a>
                                        <a href="https://github.com/s3prl/s3prl" target="_blank">
                                            <i class="fab fa-github"></i>code
                                        </a>
                                        <a href="https://www.youtube.com/watch?v=zd9fiVvej0k" target="_blank">
                                            <i class="fab fa-youtube"></i>video
                                        </a>
                                    </div>
                                    <div id="superb_abstract" class="abstract" style="display:none;">
                                        <p>
                                            The foundation model paradigm leverages a shared foundation model to achieve state-of-the-art (SOTA) performance for various tasks, requiring minimal downstream-specific data collection and modeling. This approach has proven crucial in the field of Natural Language Processing (NLP). However, the speech processing community lacks a similar setup to explore the paradigm systematically. To bridge this gap, we establish the Speech processing Universal PERformance Benchmark (SUPERB). SUPERB represents an ecosystem designed to evaluate foundation models across a wide range of speech processing tasks, facilitating the sharing of results on an online leaderboard and fostering collaboration through a community-driven benchmark database that aids in new development cycles. We present a unified learning framework for solving the speech processing tasks in SUPERB with the frozen foundation model followed by task-specialized lightweight prediction heads. Combining our results with community submissions, we verify that the framework is simple yet effective, as the best-performing foundation model shows competitive generalizability across most SUPERB tasks. Finally, we conduct a series of analyses to offer an in-depth understanding of SUPERB and speech foundation models, including information flows across tasks inside the models and the statistical significance and robustness of the benchmark.
                                        </p>
                                    </div>
                                </div>
                            </div>
                        </li>
                        <li>
                            <div class="pub-item">
                                <div class="pub-teaser">
                                    <img src="teaser/speechbert.jpg" alt="SpeechBERT teaser">
                                </div>
                                <div class="pub-content">
                                    <h4>
                                        <a href="https://arxiv.org/abs/1910.11559" target="_blank">
                                            <b>SpeechBERT: An Audio-and-text Jointly Learned Language Model for End-to-end Spoken Question Answering</b>
                                        </a>
                                    </h4>
                                    <div class="authors">
                                        <b>Yung-Sung Chuang</b>, 
                                        <a href="https://liangtaiwan.github.io/" target="_blank">Chi-Liang Liu</a>,
                                        <a href="https://speech.ee.ntu.edu.tw/~hylee/index.html" target="_blank">Hung-Yi Lee</a>, 
                                        <a href="https://speech.ee.ntu.edu.tw/previous_version/lslNew.htm" target="_blank">Lin-shan Lee</a>
                                    </div>
                                    <div class="venue">
                                        In <a href="http://www.interspeech2020.org/" target="_blank">
                                            <b>Interspeech</b></a>, 2020. 
                                        <a class="emph"><b>(Interspeech 2020 Travel Grant)</b></a>
                                    </div>
                                    <div class="links">
                                        <a href="bibs/chuang2020speechbert.bib" target="_blank" type="text/plain">
                                            <i class="fas fa-quote-left"></i>bib
                                        </a>
                                        <a href="#" onclick="$('#speechbert_abstract').toggle();return false;">
                                            <i class="fas fa-align-left"></i>abstract
                                        </a>
                                        <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2020/chuang20b_interspeech.pdf" target="_blank">
                                            <i class="fas fa-file-pdf"></i>pdf
                                        </a>
                                        <a href="https://www.youtube.com/watch?v=7mf7nSh8dGE" target="_blank">
                                            <i class="fab fa-youtube"></i>video
                                        </a>
                                    </div>
                                    <div id="speechbert_abstract" class="abstract" style="display:none;">
                                        <p>
                                            While various end-to-end models for spoken language understanding tasks have been explored recently, this paper is probably the first known attempt to challenge the very difficult task of end-to-end spoken question answering (SQA). Learning from the very successful BERT model for various text processing tasks, here we proposed an audio-and-text jointly learned SpeechBERT model. This model outperformed the conventional approach of cascading ASR with the following text question answering (TQA) model on datasets including ASR errors in answer spans, because the end-to-end model was shown to be able to extract information out of audio data before ASR produced errors. When ensembling the proposed end-to-end model with the cascade architecture, even better performance was achieved. In addition to the potential of end-to-end SQA, the SpeechBERT can also be considered for many other spoken language understanding tasks just as BERT for many text processing tasks.
                                        </p>
                                    </div>
                                </div>
                            </div>
                        </li>
                    </ul>
                </div>
            </div>

            <div class="honors-section" id="honors">
                <h2>Honors</h2>
                <ul class="honors-list">
                    <li class="honor-item">
                        <div class="honor-icon">🎓</div>
                        <div class="honor-content">
                            <div class="award-name">Dean's list (4 times)</div>
                            <div class="award-details">Electrical Engineering Dept. at NTU, Spring '18, Spring '19, Fall '19, Spring '20</div>
                        </div>
                    </li>
                    <li class="honor-item">
                        <div class="honor-icon">💰</div>
                        <div class="honor-content">
                            <div class="award-name">Irving T. Ho Memorial Scholarship (2 times)</div>
                            <div class="award-details">EECS at NTU, Fall '18, Fall '19</div>
                        </div>
                    </li>
                    <!-- <li class="honor-item">
                        <div class="honor-icon">✈️</div>
                        <div class="honor-content">
                            <div class="award-name">Travel Grant</div>
                            <div class="award-details">INTERSPEECH 2020 conference, Sep. 2020</div>
                        </div>
                    </li>
                    <li class="honor-item">
                        <div class="honor-icon">🏆</div>
                        <div class="honor-content">
                            <div class="award-name">Appier Best Application Award</div>
                            <div class="award-details">2020 NTU CSIE Undergrad Special Research Exhibition, Jun. 2020</div>
                        </div>
                    </li>
                    <li class="honor-item">
                        <div class="honor-icon">🥈</div>
                        <div class="honor-content">
                            <div class="award-name">2nd Place & Appier 1st Award</div>
                            <div class="award-details">2019 NTU CSIE Undergrad Special Research Exhibition, Jun. 2019</div>
                        </div>
                    </li>
                    <li class="honor-item">
                        <div class="honor-icon">🥈</div>
                        <div class="honor-content">
                            <div class="award-name">2nd Place</div>
                            <div class="award-details">2019 NTUEE Undergraduate Innovation Award, Jun. 2019</div>
                        </div>
                    </li>
                    <li class="honor-item">
                        <div class="honor-icon">🥇</div>
                        <div class="honor-content">
                            <div class="award-name">1st Place</div>
                            <div class="award-details">2018 H. Spectrum Demo Day (out of 21 teams), Jul. 2018</div>
                        </div>
                    </li>
                    <li class="honor-item">
                        <div class="honor-icon">🥇</div>
                        <div class="honor-content">
                            <div class="award-name">1st Place</div>
                            <div class="award-details">NCTS Health Hackathon 2018 (out of 18 teams), Jun. 2018</div>
                        </div>
                    </li>
                    <li class="honor-item">
                        <div class="honor-icon">🏅</div>
                        <div class="honor-content">
                            <div class="award-name">Top 8 Finalist</div>
                            <div class="award-details">Microsoft Imagine Cup Taiwan National Final 2018, Apr. 2018</div>
                        </div>
                    </li>
                    <li class="honor-item">
                        <div class="honor-icon">🏆</div>
                        <div class="honor-content">
                            <div class="award-name">Best Tech Award & Microsoft Enterprise Award</div>
                            <div class="award-details">MakeNTU 2018 (out of 50 teams), Mar. 2018</div>
                        </div>
                    </li>
                    <li class="honor-item">
                        <div class="honor-icon">🥇</div>
                        <div class="honor-content">
                            <div class="award-name">1st place of Dept. of Transportation</div>
                            <div class="award-details">HackNTU 2017 (out of 100+ teams), Jul. 2017</div>
                        </div>
                    </li> -->
                </ul>
            </div>
            <div class="services-section" id="services">
                <h2>Services</h2>
                <div class="service-category">
                    <h4>Reviewer</h4>
                    <div class="service-venues">
                        <span class="service-venue">NeurIPS 2021, 2022, 2023, 2024, 2025</span>
                        <span class="service-venue">ICLR 2022, 2023, 2024, 2025</span>
                        <span class="service-venue">ICML 2022, 2023, 2024, 2025</span>
                        <span class="service-venue">ACL ARR 2023, 2024, 2025</span>
                        <span class="service-venue">EMNLP 2022, 2023</span>
                        <span class="service-venue">ACL 2023</span>
                        <span class="service-venue">AAAI 2023</span>
                        <span class="service-venue">ICASSP 2022, 2023</span>
                        <span class="service-venue">TASL 2023, 2024, 2025</span>
                    </div>
                </div>
            </div>
            <div class="attribution-section">
                <div class="attribution-card">
                    <i class="fas fa-code" style="margin-right: 8px; color: #666;"></i>
                    This website is built from the <a href="https://github.com/nelson-liu/website" target="_blank">source code</a> of Nelson F. Liu's awesome website (<a href="https://nelsonliu.me/" target="_blank">nelsonliu.me</a>).
                </div>
            </div>
            <footer class="pt-2 my-md-2 pt-md-2 border-top">
                <div class="row justify-content-center">
                    <div class="col-6 col-md text-left align-self-center">
                        <p class="h5 text-muted">
                            © Yung-Sung Chuang, 2025
                        </p>
                    </div>
                    <div class="col-6 col-md text-right">
                        <a href="https://www.csail.mit.edu/" class="image-link">
                            <img class="mr-4" src="img/mit_csail_logo_new.svg" alt="MIT CSAIL logo." height="75">
                        </a>
                        <a href="https://www.mit.edu/" class="image-link">
                            <img src="img/mit_logo.svg" alt="MIT logo." height="50">
                        </a>
                    </div>
                </div>
            </footer>
        </div>

        <!-- Image Modal -->
        <div id="imageModal" class="image-modal">
            <img id="modalImage" src="" alt="">
        </div>

        <!-- Bib Modal -->
        <div id="bibModal" class="bib-modal">
            <div class="bib-modal-content">
                <div class="bib-modal-header">
                    <h3 class="bib-modal-title" id="bibModalTitle">BibTeX Citation</h3>
                    <button class="bib-modal-close" id="bibModalClose">&times;</button>
                </div>
                <div class="bib-modal-text" id="bibModalText"></div>
            </div>
        </div>

        <script>
            document.addEventListener('DOMContentLoaded', function() {
                // Load citation count from external JS file
                function loadCitationCount() {
                    const badgeElement = document.getElementById('citationBadge');
                    if (!badgeElement) return;
                    
                    // citationData is loaded from citations/scholar.js
                    if (typeof citationData !== 'undefined' && citationData && typeof citationData.citations === 'number') {
                        badgeElement.textContent = citationData.citations.toString();
                        console.log(`Citation count: ${citationData.citations} (last updated: ${citationData.date})`);
                    } else {
                        console.error('Citation data not loaded or invalid format');
                        badgeElement.textContent = '?';
                    }
                }

                // Load citation count on page load
                loadCitationCount();
                // Mobile header responsive text
                function updateHeaderText() {
                    const nameElement = document.querySelector('.nav-brand a');
                    const navLinks = document.querySelectorAll('.nav-links a');
                    
                    // Ensure elements exist before proceeding
                    if (!nameElement || navLinks.length === 0) {
                        console.log('Header elements not found, retrying...');
                        setTimeout(updateHeaderText, 100);
                        return;
                    }
                    
                    if (window.innerWidth <= 768) {
                        // Mobile: two-line name
                        if (nameElement) {
                            nameElement.innerHTML = 'Yung-Sung<br>Chuang';
                        }
                        
                        // Mobile: labels with line breaks for longer ones
                        const mobileLabels = ['Google<br>Scholar', 'CV', 'Twitter', 'Github', 'LinkedIn', 'Blog'];
                        navLinks.forEach((link, index) => {
                            if (link && mobileLabels[index]) {
                                const icon = link.querySelector('i');
                                const badge = link.querySelector('.citation-badge');
                                
                                // Clear and rebuild
                                link.innerHTML = '';
                                if (icon) link.appendChild(icon);
                                if (badge) link.appendChild(badge);
                                
                                // Create text element with HTML for line breaks
                                const textSpan = document.createElement('span');
                                textSpan.innerHTML = mobileLabels[index];
                                textSpan.style.fontSize = '9px';
                                textSpan.style.lineHeight = '1.1';
                                textSpan.style.textAlign = 'center';
                                link.appendChild(textSpan);
                            }
                        });
                    } else {
                        // Desktop: full name
                        if (nameElement) {
                            nameElement.innerHTML = 'Yung-Sung Chuang';
                        }
                        
                        // Desktop: full labels for nav links
                        const desktopLabels = ['Google Scholar', 'CV', 'Twitter', 'Github', 'LinkedIn', 'Blog'];
                        navLinks.forEach((link, index) => {
                            if (link && desktopLabels[index]) {
                                const icon = link.querySelector('i');
                                const badge = link.querySelector('.citation-badge');
                                
                                // Clear and rebuild
                                link.innerHTML = '';
                                if (icon) link.appendChild(icon);
                                if (badge) link.appendChild(badge);
                                
                                const textNode = document.createTextNode(desktopLabels[index]);
                                link.appendChild(textNode);
                            }
                        });
                    }
                }

                // Multiple initialization attempts to ensure it works
                function initializeHeader() {
                    updateHeaderText();
                    // Backup calls in case the first one fails
                    setTimeout(updateHeaderText, 100);
                    setTimeout(updateHeaderText, 500);
                }

                // Initial call and resize listener
                initializeHeader();
                window.addEventListener('resize', updateHeaderText);
                window.addEventListener('load', updateHeaderText);
                // Image popup functionality
                const imageModal = document.getElementById('imageModal');
                const modalImg = document.getElementById('modalImage');
                const teaserImages = document.querySelectorAll('.pub-teaser img');

                // Add click event to all teaser images
                teaserImages.forEach(function(img) {
                    img.addEventListener('click', function() {
                        imageModal.style.display = 'block';
                        modalImg.src = this.src;
                        modalImg.alt = this.alt;
                        document.body.style.overflow = 'hidden';
                    });
                });

                // Close image modal when clicking anywhere on it
                imageModal.addEventListener('click', function() {
                    imageModal.style.display = 'none';
                    document.body.style.overflow = 'auto';
                });

                // Bib modal functionality
                const bibModal = document.getElementById('bibModal');
                const bibModalTitle = document.getElementById('bibModalTitle');
                const bibModalText = document.getElementById('bibModalText');
                const bibModalClose = document.getElementById('bibModalClose');

                // Bib content mapping
                const bibContent = {
                    'chuang2025selfcite': `@article{chuang2025selfcite,
  title={SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models},
  author={Chuang, Yung-Sung and Cohen-Wang, Benjamin and Shen, Shannon Zejiang and Wu, Zhaofeng and Xu, Hu and Lin, Xi Victoria and Glass, James and Li, Shang-Wen and Yih, Wen-tau},
  journal={arXiv preprint arXiv:2502.09604},
  year={2025}
}`,
                    'chuang2024lookback': `@inproceedings{chuang2024lookback,
  title={Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps},
  author={Chuang, Yung-Sung and Qiu, Linlu and Hsieh, Cheng-Yu and Krishna, Ranjay and Kim, Yoon and Glass, James},
  booktitle={The Conference on Empirical Methods in Natural Language Processing},
  year={2024}
}`,
                    'chuang2023dola': `@inproceedings{chuang2023dola,
  title={DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models},
  author={Chuang, Yung-Sung and Xie, Yujia and Luo, Hongyin and Kim, Yoon and Glass, James R and He, Pengcheng},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}`,
                    'cohenwang2025learning': `@article{cohenwang2025learning,
  title={Learning to Attribute with Attention},
  author={Cohen-Wang, Benjamin and Chuang, Yung-Sung and Madry, Aleksander},
  journal={arXiv preprint arXiv:2504.13752},
  year={2025}
}`,
                    'chuang2025metaclip2': `@article{chuang2025metaclip2,
  title={Meta CLIP 2: A Worldwide Scaling Recipe},
  author={Chuang, Yung-Sung and Li, Yang and Wang, Dong and Yeh, Ching-Feng and Lyu, Kehan and Raghavendra, Ramya and Glass, James and Huang, Lifei and Weston, Jason and Zettlemoyer, Luke and Chen, Xinlei and Liu, Zhuang and Xie, Saining and Yih, Wen-tau and Li, Shang-Wen and Xu, Hu},
  journal={arXiv preprint arXiv:2507.22062},
  year={2025}
}`,
                    'fang2024joint': `@inproceedings{fang2024joint,
  title={Joint Inference of Retrieval and Generation for Passage Re-ranking},
  author={Fang, Wei and Chuang, Yung-Sung and Glass, James},
  booktitle={The 18th Conference of the European Chapter of the Association for Computational Linguistics: Findings},
  year={2024}
}`,
                    'chuang2023expand': `@inproceedings{chuang2023expand,
  title={Expand, Rerank, and Retrieve: Query Reranking for Open-Domain Question Answering},
  author={Chuang, Yung-Sung and Fang, Wei and Li, Shang-Wen and Yih, Wen-tau and Glass, James},
  booktitle={The 61st Annual Meeting of the Association for Computational Linguistics: Findings},
  year={2023}
}`,
                    'luo2023sail': `@inproceedings{luo2023sail,
  title={SAIL: Search-Augmented Instruction Learning},
  author={Luo, Hongyin and Chuang, Yung-Sung and Gong, Yuan and Zhang, Tianhua and Kim, Yoon and Wu, Xixin and Fox, Danny and Meng, Helen and Glass, James},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP},
  year={2023}
}`,
                    'chuang2022diffcse': `@inproceedings{chuang2022diffcse,
  title={{DiffCSE}: Difference-based Contrastive Learning for Sentence Embeddings},
  author={Chuang, Yung-Sung and Dangovski, Rumen and Luo, Hongyin and Zhang, Yang and Chang, Shiyu and Soljačić, Marin and Li, Shang-Wen and Yih, Wen-tau and Kim, Yoon and Glass, James},
  booktitle={Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)},
  year={2022}
}`,
                    'yang2021superb': `@article{yang2021superb,
  title={SUPERB: Speech processing Universal PERformance Benchmark},
  author={Yang, Shu-wen and Chi, Po-Han and Chuang, Yung-Sung and Lai, Cheng-I Jeff and Lakhotia, Kushal and Lin, Yist Y. and Liu, Andy T. and Shi, Jiatong and Chang, Xuankai and Lin, Guan-Ting and Huang, Tzu-Hsien and Tseng, Wei-Cheng and Lee, Ko-tik and Liu, Da-Rong and Huang, Zili and Dong, Shuyan and Li, Shang-Wen and Watanabe, Shinji and Mohamed, Abdelrahman and Lee, Hung-yi},
  journal={arXiv preprint arXiv:2105.01051},
  year={2021}
}`,
                    'chuang2020speechbert': `@inproceedings{chuang2020speechbert,
  title={SpeechBERT: An Audio-and-text Jointly Learned Language Model for End-to-end Spoken Question Answering},
  author={Chuang, Yung-Sung and Liu, Chi-Liang and Lee, Hung-yi and Lee, Lin-shan},
  booktitle={Interspeech},
  year={2020}
}`,
                    'lin2022dual': `@inproceedings{lin2022dual,
  title={DUAL: Discrete Spoken Unit Adaptive Learning for Textless Spoken Question Answering},
  author={Lin, Guan-Ting and Chuang, Yung-Sung and Chung, Ho-Lam and Yang, Shu-wen and Chen, Hsuan-Jui and Dong, Shuyan and Li, Shang-Wen and Mohamed, Abdelrahman and Lee, Hung-yi and Lee, Lin-shan},
  booktitle={Interspeech},
  year={2022}
}`,
                    'lai2020semi': `@inproceedings{lai2020semi,
  title={Semi-Supervised Spoken Language Understanding via Self-Supervised Speech and Language Model Pretraining},
  author={Lai, Cheng-I and Chuang, Yung-Sung and Lee, Hung-Yi and Li, Shang-Wen and Glass, James},
  booktitle={IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year={2021}
}`
                };

                // Add click event to all bib links
                document.querySelectorAll('a[href*=".bib"]').forEach(function(link) {
                    link.addEventListener('click', function(e) {
                        e.preventDefault();
                        const href = this.getAttribute('href');
                        const bibKey = href.replace('bibs/', '').replace('.bib', '');
                        
                        if (bibContent[bibKey]) {
                            bibModalTitle.textContent = 'BibTeX Citation';
                            bibModalText.textContent = bibContent[bibKey];
                            bibModal.style.display = 'block';
                            document.body.style.overflow = 'hidden';
                        }
                    });
                });

                // Close bib modal when clicking background
                bibModal.addEventListener('click', function(e) {
                    if (e.target === bibModal) {
                        bibModal.style.display = 'none';
                        document.body.style.overflow = 'auto';
                    }
                });

                // Close bib modal when clicking X button
                bibModalClose.addEventListener('click', function() {
                    bibModal.style.display = 'none';
                    document.body.style.overflow = 'auto';
                });

                // Close modals with Escape key
                document.addEventListener('keydown', function(event) {
                    if (event.key === 'Escape') {
                        if (imageModal.style.display === 'block') {
                            imageModal.style.display = 'none';
                            document.body.style.overflow = 'auto';
                        }
                        if (bibModal.style.display === 'block') {
                            bibModal.style.display = 'none';
                            document.body.style.overflow = 'auto';
                        }
                    }
                });

                // Publication section collapsible functionality
                document.querySelectorAll('#publications h3').forEach(function(header) {
                    header.addEventListener('click', function() {
                        // Toggle collapsed class
                        this.classList.toggle('collapsed');
                        
                        // Find all ul elements between this h3 and the next h3
                        let nextElement = this.nextElementSibling;
                        let targetUls = [];
                        
                        // Collect all ul elements until we hit another h3 or end of section
                        while (nextElement) {
                            if (nextElement.tagName === 'H3') {
                                // Stop when we reach the next section header
                                break;
                            }
                            if (nextElement.tagName === 'UL') {
                                targetUls.push(nextElement);
                            }
                            nextElement = nextElement.nextElementSibling;
                        }
                        
                        // Toggle visibility for all collected ul elements
                        const isCollapsed = this.classList.contains('collapsed');
                        targetUls.forEach(function(ul) {
                            if (isCollapsed) {
                                ul.style.display = 'none';
                            } else {
                                ul.style.display = 'block';
                            }
                        });
                    });
                });
            });
        </script>
    </body>
</html>
