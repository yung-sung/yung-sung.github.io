<!DOCTYPE html>
<html lang="en">
    <head>
        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-PK2FJE42TV"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-PK2FJE42TV');
        </script>
        <title>Yung-Sung Chuang</title>
        <meta http-equiv="content-type" content="text/html; charset=UTF-8">
        <meta charset="utf-8">
        <meta property="og:url" content="https://people.csail.mit.edu/yungsung/" />
	    <meta property="og:title" content="Yung-Sung Chuang" />
	    <meta property="og:image" content="img/Yung-Sung.jpg" />
	    <meta http-equiv="X-UA-Compatible" content="IE=edge">
	    <meta name="author" content="Yung-Sung Chuang">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
        <link rel="shortcut icon" type="image/png" href="favicon.ico"/>

        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha512-iecdLmaskl7CVkqkXNQ/ZH/XLlvWZOJyj7Yy7tcenmpD1ypASozpmT/E0iPtmFIB46ZmdtAc9eNBvH0H/ZpiBw==" crossorigin="anonymous">
        <link rel="stylesheet" href="css/style.css">
        <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
        <link href='https://fonts.googleapis.com/css?family=EB+Garamond' rel='stylesheet' type='text/css'>
        <style>
            /* Add some padding on document's body to prevent the content
            to go underneath the header and footer */
            body{        
                padding-top: 50px;
                padding-bottom: 0px;
            }
            html {
                scroll-padding-top: 70px; /* height of sticky header */
            }
            .container{
                width: 80%;
                margin: 0 auto; /* Center the DIV horizontally */
            }
            .fixed-header, .fixed-footer{
                width: 100%;
                position: fixed;        
                /* background: rgb(69, 142, 226); */
                background: rgb(126, 103, 77);
                padding: 7px 0;
                color: #fff;
                font-size: calc(6px + 0.78125vw);
            }
            .fixed-header{
                top: 0;
            }
            .fixed-footer{
                bottom: 0;
            }    
            /* Some more styles to beutify this example (85, 129, 212) */
            a {
                color: #517cc8;
            }
            a:hover {
                color: #204487;
            }
            .emph {
                color: rgb(23, 111, 23) !important;
            }
            nav a{
                color: #fff;
                text-decoration: none;
                padding: 7px 1.7vw;
                display: inline-block;
            }
            h1 span {
                font-size: 20pt;
            }
            h1 span a{
                font-size: 12pt;
            }
            /* set the font color of <a href...> to be brown */
        </style>
    </head>
    <body>
        <div class="fixed-header" style="z-index:1000;">
            <nav>
                <div class="nav-brand">
                    <!-- <i class="fas fa-user-graduate"></i> -->
                    <a href="#" onclick="window.scrollTo({top: 0, behavior: 'smooth'}); return false;" title="Back to top">
                        Yung-Sung Chuang
                    </a>
                </div>
                <div class="nav-links">
                    <a href="https://scholar.google.com/citations?hl=en&user=3ar1DOwAAAAJ" target="_blank" title="Google Scholar">
                        <i class="fas fa-graduation-cap"></i>Google Scholar
                        <span class="citation-badge">3046</span>
                    </a>
                    <a href="files/CV.pdf" target="_blank" title="Download CV">
                        <i class="fas fa-file-pdf"></i>CV
                    </a>
                    <a href="https://twitter.com/YungSungChuang" target="_blank" title="Twitter">
                        <i class="fab fa-twitter"></i>Twitter
                    </a>
                    <a href="https://github.com/voidism" target="_blank" title="Github">
                        <i class="fab fa-github"></i>Github
                    </a>
                    <a href="https://www.linkedin.com/in/yschuang" target="_blank" title="LinkedIn">
                        <i class="fab fa-linkedin"></i>LinkedIn
                    </a>
                    <a href="https://voidism.github.io/" target="_blank" title="Visit my personal blog">
                        <i class="fas fa-blog"></i>Blog
                    </a>
                </div>
            </nav>
        </div>
        <div class="container mt-5" style="z-index:0;">
            <div class="bio-section">
                <div class="bio-item">
                    <div class="bio-photo">
                        <img src="img/Yung-Sung.jpg" alt="Yung-Sung Chuang">
                        <div class="info-card">
                            <!-- <h5>Yung-Sung Chuang</h5> -->
                            <p class="title">
                                MIT EECS PhD Student @ CSAIL<br>
                                Office: 32-G436<br>
                                <br>
                                Email: yungsung [AT] mit.edu
                            </p>
                        </div>
                    </div>
                    <div class="bio-content">
                        <h1>Yung-Sung Chuang <span>(莊永松) <a href="#" onclick="$('#my_name').toggle();return false;"><i>How to pronounce?</i></a></span></h1>
                        
                        <div id="my_name" class="abstract" style="display:none;">
                            <p style="color:gray">
                                My first name Yung-Sung (永松) should be pronounced as "yong song". My last name Chuang (莊) should be pronounced as "jwang". In my first name, Yung means "forever" and Sung means "pine tree", so it has the meaning of "long-lasting" or "longevity".
                                </br>
                                <audio controls>
                                    <source src="files/name-tts.mp3" type="audio/mpeg">
                                </audio>
                            </p>
                        </div>
                        

                        <p>
                            I am a fourth-year PhD student working with <a href="https://www.csail.mit.edu/person/jim-glass" target="_blank">Jim Glass</a> at <a href="https://www.csail.mit.edu/" target="_blank">MIT CSAIL</a>. My research focuses on large language models: hallucinations, factuality, and transparency in LLMs. During my internship at Meta FAIR, I also worked on pre-training <a href="https://arxiv.org/abs/2507.22062" target="_blank">MetaCLIP 2</a>, a multilingual vision-language model pre-trained on worldwide web-scale data.
                        </p>
                        <p>
                            My research has introduced several approaches for improving LLM factuality. <a href="https://arxiv.org/abs/2309.03883" target="_blank">DoLa</a> enhances factuality through layer-wise knowledge contrasting during decoding. <a href="https://arxiv.org/abs/2407.07071" target="_blank">Lookback Lens</a> detects and mitigates hallucinations by analyzing attention patterns. Most recently, <a href="https://arxiv.org/abs/2502.09604" target="_blank">SelfCite</a> enables LLMs to generate citations without external supervision. I also used to work on retrieval-based methods, developing <a href="https://arxiv.org/abs/2204.10298" target="_blank">DiffCSE</a> for better sentence embeddings and <a href="https://arxiv.org/abs/2305.17080" target="_blank">Query Reranking</a> for more accurate passage retrieval.
                        </p>
                        <p>
                            Before MIT, I conducted research in speech processing and NLP with <a href="https://speech.ee.ntu.edu.tw/~hylee/index.html">Hung-Yi Lee</a> and <a href="https://www.csie.ntu.edu.tw/~yvchen/">Yun-Nung Chen</a> at National Taiwan University, where I obtained my B.S. degree in Electrical Engineering in 2020.
                        </p>
                    </div>
                </div>
            </div>
            <div class="news-section">
                <h2>Recent News</h2>
                <ul class="news-list">
                    <li class="news-item">
                        <div class="news-date">05/2025</div>
                        <div class="news-content">
                            Start my summer internship at FAIR, Meta with <a href="https://scholar.google.com/citations?user=UjpbO6IAAAAJ&hl=en" target="_blank">Luke Zettlemoyer</a>.
                        </div>
                    </li>
                    <li class="news-item">
                        <div class="news-date">04/2025</div>
                        <div class="news-content">
                            My recent talk at MIT EI Seminar <a href="https://youtu.be/oEFRH80LndM?si=uM_lb2sUm1NVmezT" target="_blank">"Reducing Hallucinations in LLMs via Decoding, Detection, and Citation"</a> is on Youtube!
                        </div>
                    </li>
                    <li class="news-item">
                        <div class="news-date">05/2024</div>
                        <div class="news-content">
                            Start my summer internship at FAIR, Meta with <a href="https://howardhsu.github.io/" target="_blank">Hu Xu</a>, <a href="https://swdanielli.github.io/" target="_blank">Daniel Li</a>, <a href="https://scottyih.org/" target="_blank">Scott Yih</a>. See our <a href="https://arxiv.org/abs/2507.22062" target="_blank">MetaCLIP 2 paper</a>.
                        </div>
                    </li>
                    <li class="news-item">
                        <div class="news-date">06/2023</div>
                        <div class="news-content">
                            Start my summer internship at Microsoft with <a href="https://scholar.google.com/citations?user=TS1RoxAAAAAJ&hl=en" target="_blank">Pengcheng He</a>, <a href="https://sites.google.com/view/yujia" target="_blank">Yujia Xie</a>. See our <a href="https://arxiv.org/abs/2309.03883" target="_blank">DoLa paper</a>.
                        </div>
                    </li>
                    <li class="news-item">
                        <div class="news-date">06/2022</div>
                        <div class="news-content">
                            Start my summer internship at MIT-IBM Watson AI Lab with <a href="https://mitibmwatsonailab.mit.edu/people/yang-zhang/" target="_blank">Yang Zhang</a>, <a href="https://mitibmwatsonailab.mit.edu/people/shiyu-chang/" target="_blank">Shiyu Chang</a>, <a href="https://people.csail.mit.edu/yoonkim/" target="_blank">Yoon Kim</a>, <a href="https://scholar.google.com/citations?user=uEpr4C4AAAAJ&hl=en" target="_blank">Kaizhi Qian</a>.
                        </div>
                    </li>
                    <li class="news-item">
                        <div class="news-date">09/2021</div>
                        <div class="news-content">
                            Start my PhD journey at MIT, Cambridge, MA.
                        </div>
                    </li>
                </ul>
            </div>
            <hr>
            <div class="row" id="talks">
                <div class="col">
                    <h2>Talks</h2>
                    <ul class="talks-list">
                        <li>
                            <div class="talk-item">
                                <div class="talk-video">
                                    <iframe src="https://www.youtube.com/embed/oEFRH80LndM?si=nuyL3AsfZxjw4Tf3" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
                                </div>
                                <div class="talk-content">
                                    <h4>
                                        <b>Reducing Hallucinations in LLMs via Decoding, Detection, and Citation</b>
                                    </h4>
                                    <div class="venue">
                                        MIT CSAIL: EI Seminar, April 24, 2025
                                    </div>
                                    <div class="description">
                                        A comprehensive overview of my research on reducing hallucinations in large language models through three key approaches: improved decoding strategies (DoLa), detection methods (Lookback Lens), and citation generation (SelfCite).
                                    </div>
                                </div>
                            </div>
                        </li>
                    </ul>
                    <p><i>For my other talks at conferences, please see <a href="https://www.youtube.com/@yung-sung" target="_blank">https://www.youtube.com/@yung-sung</a></i></p>
                </div>
            </div>
                    
            <br/>
            <div class="row" id="publications">
                <div class="col">
                    <h2>Selected Publications</h2>
                    <p><i>For a full list of papers, see my <a href="https://scholar.google.com/citations?hl=en&user=3ar1DOwAAAAJ" target="_blank">Google Scholar</a>.</i></p>
                    <h3> Factuality & Transparency</h3>
                    <ul class="pl">
                        <li>
                            <div class="pub-item">
                                <div class="pub-teaser">
                                    <img src="teaser/selfcite.png" alt="SelfCite teaser">
                                </div>
                                <div class="pub-content">
                                    <h4>
                                        <a href="https://arxiv.org/abs/2502.09604" target="_blank">
                                            <b>SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models</b>
                                        </a>
                                    </h4>
                                    <div class="authors">
                                        <b>Yung-Sung Chuang</b>,
                                        <a href="https://bencw99.github.io/" target="_blank">Benjamin Cohen-Wang</a>,
                                        <a href="https://www.szj.io/" target="_blank">Shannon Zejiang Shen</a>,
                                        <a href="https://zhaofengwu.github.io/" target="_blank">Zhaofeng Wu</a>,
                                        <a href="https://howardhsu.github.io/" target="_blank">Hu Xu</a>,
                                        <a href="https://victorialin.org/" target="_blank">Xi Victoria Lin</a>,
                                        <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>,
                                        <a href="https://swdanielli.github.io/" target="_blank">Shang-Wen Li</a>,
                                        <a href="https://scottyih.org/" target="_blank">Wen-tau Yih</a>
                                    </div>
                                    <div class="venue">
                                        <b>ICML</b>, 2025.
                                    </div>
                                    <div class="links">
                                        <a href="bibs/chuang2025selfcite.bib" target="_blank" type="text/plain">
                                            <i class="fas fa-quote-left"></i>bib
                                        </a>
                                        <a href="#" onclick="$('#selfcite_abstract').toggle();return false;">
                                            <i class="fas fa-align-left"></i>abstract
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09604.pdf" target="_blank">
                                            <i class="fas fa-file-pdf"></i>pdf
                                        </a>
                                        <a href="https://github.com/facebookresearch/SelfCite" target="_blank">
                                            <i class="fab fa-github"></i>code
                                        </a>
                                        <a href="https://selfcite.github.io" target="_blank">
                                            <i class="fas fa-globe"></i>web
                                        </a>
                                    </div>
                                    <div id="selfcite_abstract" class="abstract" style="display:none;">
                                        <p>
                                            We introduce <i>SelfCite</i>, a novel self-supervised approach that aligns LLMs to generate high-quality, fine-grained, sentence-level citations for the statements in their generated responses. Instead of only relying on costly and labor-intensive annotations, SelfCite leverages a reward signal provided by the LLM itself through context ablation: If a citation is necessary, removing the cited text from the context should prevent the same response; if sufficient, retaining the cited text alone should preserve the same response. This reward can guide the inference-time best-of-N sampling strategy to improve citation quality significantly, as well as be used in preference optimization to directly fine-tune the models for generating better citations. The effectiveness of SelfCite is demonstrated by increasing citation F1 up to 5.3 points on the LongBench-Cite benchmark across five long-form question answering tasks.
                                        </p>
                                    </div>
                                </div>
                            </div>
                        </li>
                    </ul>

                    <ul class="pl">
                        <li>
                            <div class="pub-item">
                                <div class="pub-teaser">
                                    <img src="teaser/lookbacklens.png" alt="Lookback Lens teaser">
                                </div>
                                <div class="pub-content">
                                    <h4>
                                        <a href="https://arxiv.org/abs/2407.07071" target="_blank">
                                            <b>Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps</b>
                                        </a>
                                    </h4>
                                    <div class="authors">
                                        <b>Yung-Sung Chuang</b>,
                                        <a href="https://linlu-qiu.github.io/" target="_blank">Linlu Qiu</a>,
                                        <a href="https://chengyuhsieh.github.io/" target="_blank">Cheng-Yu Hsieh</a>,
                                        <a href="https://www.ranjaykrishna.com/index.html" target="_blank">Ranjay Krishna</a>,
                                        <a href="https://people.csail.mit.edu/yoonkim/" target="_blank">Yoon Kim</a>,
                                        <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
                                    </div>
                                    <div class="venue">
                                        In <a href="https://2024.emnlp.org" target="_blank"><b>The Conference on Empirical Methods in Natural Language Processing</b></a>, 2024.
                                    </div>
                                    <div class="links">
                                        <a href="bibs/chuang2024lookback.bib" target="_blank" type="text/plain">
                                            <i class="fas fa-quote-left"></i>bib
                                        </a>
                                        <a href="#" onclick="$('#lookback_abstract').toggle();return false;">
                                            <i class="fas fa-align-left"></i>abstract
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07071.pdf" target="_blank">
                                            <i class="fas fa-file-pdf"></i>pdf
                                        </a>
                                        <a href="https://github.com/voidism/Lookback-Lens" target="_blank">
                                            <i class="fab fa-github"></i>code
                                        </a>
                                    </div>
                                    <div id="lookback_abstract" class="abstract" style="display:none;">
                                        <p>
                                            When asked to summarize articles or answer questions given a passage, large language models (LLMs) can hallucinate details and respond with unsubstantiated answers that are inaccurate with respect to the input context. This paper describes a simple approach for detecting such contextual hallucinations. We hypothesize that contextual hallucinations are related to the extent to which an LLM attends to information in the provided context versus its own generations. Based on this intuition, we propose a simple hallucination detection model whose input features are given by the ratio of attention weights on the context versus newly generated tokens (for each attention head). We find that a linear classifier based on these lookback ratio features is as effective as a richer detector that utilizes the entire hidden states of an LLM or a text-based entailment model. The lookback ratio-based detector -- Lookback Lens -- is found to transfer across tasks and even models, allowing a detector that is trained on a 7B model to be applied (without retraining) to a larger 13B model. We further apply this detector to mitigate contextual hallucinations, and find that a simple classifier-guided decoding approach is able to reduce the amount of hallucination, for example by 9.6% in the XSum summarization task.
                                        </p>
                                    </div>
                                </div>
                            </div>
                        </li>
                    </ul>

                    <ul class="pl">
                        <li>
                            <div class="pub-item">
                                <div class="pub-teaser">
                                    <img src="teaser/dola.png" alt="DoLa teaser">
                                </div>
                                <div class="pub-content">
                                    <h4>
                                        <a href="https://arxiv.org/abs/2309.03883" target="_blank">
                                            <b>DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models</b>
                                        </a>
                                    </h4>
                                    <div class="authors">
                                        <b>Yung-Sung Chuang</b>,
                                        <a href="https://sites.google.com/view/yujia" target="_blank">Yujia Xie</a>,
                                        <a href="https://luohongyin.github.io/" target="_blank">Hongyin Luo</a>,
                                        <a href="https://people.csail.mit.edu/yoonkim/" target="_blank">Yoon Kim</a>,
                                        <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>,
                                        <a href="https://scholar.google.com/citations?user=TS1RoxAAAAAJ&hl=en" target="_blank">Pengcheng He</a>
                                    </div>
                                    <div class="venue">
                                        In <a href="https://iclr.cc/" target="_blank">
                                            <b>The Twelfth International Conference on Learning Representations (ICLR)</b></a>, 2024.
                                    </div>
                                    <div class="links">
                                        <a href="bibs/chuang2023dola.bib" target="_blank" type="text/plain">
                                            <i class="fas fa-quote-left"></i>bib
                                        </a>
                                        <a href="#" onclick="$('#chuang2023dola_abstract').toggle();return false;">
                                            <i class="fas fa-align-left"></i>abstract
                                        </a>
                                        <a href="https://browse.arxiv.org/pdf/2309.03883.pdf" target="_blank">
                                            <i class="fas fa-file-pdf"></i>pdf
                                        </a>
                                        <a href="https://github.com/voidism/DoLa" target="_blank">
                                            <i class="fab fa-github"></i>code
                                        </a>
                                    </div>
                                    <div id="chuang2023dola_abstract" class="abstract" style="display:none;">
                                        <p>
                                            Despite their impressive capabilities, large language models (LLMs) are prone to hallucinations, i.e., generating content that deviates from facts seen during pretraining. We propose a simple decoding strategy for reducing hallucinations with pretrained LLMs that does not require conditioning on retrieved external knowledge nor additional fine-tuning. Our approach obtains the next-token distribution by contrasting the differences in logits obtained from projecting the later layers versus earlier layers to the vocabulary space, exploiting the fact that factual knowledge in an LLMs has generally been shown to be localized to particular transformer layers. We find that this Decoding by Contrasting Layers (DoLa) approach is able to better surface factual knowledge and reduce the generation of incorrect facts. DoLa consistently improves the truthfulness across multiple choices tasks and open-ended generation tasks, for example improving the performance of LLaMA family models on TruthfulQA by 12-17% absolute points, demonstrating its potential in making LLMs reliably generate truthful facts.
                                        </p>
                                    </div>
                                </div>
                            </div>
                        </li>
                    </ul>


                    <ul class="pl">
                        <li>
                            <div class="pub-item">
                                <div class="pub-teaser">
                                    <img src="teaser/at2.png" alt="AT2 teaser">
                                </div>
                                <div class="pub-content">
                                    <h4>
                                        <a href="https://arxiv.org/abs/2504.13752" target="_blank">
                                            <b>Learning to Attribute with Attention</b>
                                        </a>
                                    </h4>
                                    <div class="authors">
                                        <a href="https://bencw99.github.io/" target="_blank">Benjamin Cohen-Wang</a>,
                                        <b>Yung-Sung Chuang</b>,
                                        <a href="https://madry.mit.edu/" target="_blank">Aleksander Madry</a>
                                    </div>
                                    <div class="venue">
                                        arXiv preprint arXiv:2504.13752, 2025.
                                    </div>
                                    <div class="links">
                                        <a href="bibs/cohenwang2025learning.bib" target="_blank" type="text/plain">
                                            <i class="fas fa-quote-left"></i>bib
                                        </a>
                                        <a href="#" onclick="$('#at2_abstract').toggle();return false;">
                                            <i class="fas fa-align-left"></i>abstract
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.13752.pdf" target="_blank">
                                            <i class="fas fa-file-pdf"></i>pdf
                                        </a>
                                        <a href="https://github.com/MadryLab/AT2" target="_blank">
                                            <i class="fab fa-github"></i>code
                                        </a>
                                    </div>
                                    <div id="at2_abstract" class="abstract" style="display:none;">
                                        <p>
                                            Given a sequence of tokens generated by a language model, we may want to identify the preceding tokens that influence the model to generate this sequence. Performing such token attribution is expensive; a common approach is to ablate preceding tokens and directly measure their effects. To reduce the cost of token attribution, we revisit attention weights as a heuristic for how a language model uses previous tokens. Naive approaches to attribute model behavior with attention (e.g., averaging attention weights across attention heads to estimate a token's influence) have been found to be unreliable. To attain faithful attributions, we propose treating the attention weights of different attention heads as features. This way, we can learn how to effectively leverage attention weights for attribution (using signal from ablations). Our resulting method, Attribution with Attention (AT2), reliably performs on par with approaches that involve many ablations, while being significantly more efficient. To showcase the utility of AT2, we use it to prune less important parts of a provided context in a question answering setting, improving answer quality.
                                        </p>
                                    </div>
                                </div>
                            </div>
                        </li>
                    </ul>

                    <h3> Multi-modal Large-scale Pre-training </h3>

                    <ul class="pl">
                        <li>
                            <div class="pub-item">
                                <div class="pub-teaser">
                                    <img src="teaser/metaclip2.png" alt="MetaCLIP 2 teaser">
                                </div>
                                <div class="pub-content">
                                    <h4>
                                        <a href="https://arxiv.org/abs/2507.22062" target="_blank">
                                            <b>MetaCLIP 2: A Worldwide Scaling Recipe</b>
                                        </a>
                                    </h4>
                                    <div class="authors">
                                        <b>Yung-Sung Chuang</b>,
                                        Yang Li,
                                        Dong Wang,
                                        <a href="https://scholar.google.com/citations?user=P7ma7pAAAAAJ&hl=en" target="_blank">Ching-Feng Yeh</a>,
                                        Kehan Lyu,
                                        Ramya Raghavendra,
                                        <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>,
                                        Lifei Huang,
                                        <a href="https://scholar.google.com/citations?user=lMkTx0EAAAAJ&hl=en" target="_blank">Jason Weston</a>,
                                        <a href="https://www.cs.washington.edu/people/faculty/lsz" target="_blank">Luke Zettlemoyer</a>,
                                        <a href="https://xinleic.xyz/" target="_blank">Xinlei Chen</a>,
                                        <a href="https://liuzhuang13.github.io/" target="_blank">Zhuang Liu</a>,
                                        <a href="https://www.sainingxie.com/" target="_blank">Saining Xie</a>,
                                        <a href="https://scottyih.org/" target="_blank">Wen-tau Yih</a>,
                                        <a href="https://swdanielli.github.io/" target="_blank">Shang-Wen Li</a>,
                                        <a href="https://howardhsu.github.io/" target="_blank">Hu Xu</a>
                                    </div>
                                    <div class="venue">
                                        arXiv preprint arXiv:2507.22062, 2025.
                                    </div>
                                    <div class="links">
                                        <a href="bibs/chuang2025metaclip2.bib" target="_blank" type="text/plain">
                                            <i class="fas fa-quote-left"></i>bib
                                        </a>
                                        <a href="#" onclick="$('#metaclip2_abstract').toggle();return false;">
                                            <i class="fas fa-align-left"></i>abstract
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.22062.pdf" target="_blank">
                                            <i class="fas fa-file-pdf"></i>pdf
                                        </a>
                                        <a href="https://github.com/facebookresearch/MetaCLIP" target="_blank">
                                            <i class="fab fa-github"></i>code
                                        </a>
                                    </div>
                                    <div id="metaclip2_abstract" class="abstract" style="display:none;">
                                        <p>
                                            Contrastive Language-Image Pretraining (CLIP) is a popular foundation model, supporting from zero-shot classification, retrieval to encoders for multimodal large language models (MLLMs). Although CLIP is successfully trained on billion-scale image-text pairs from the English world, scaling CLIP's training further to learning from the worldwide web data is still challenging: (1) no curation method is available to handle data points from non-English world; (2) the English performance from existing multilingual CLIP is worse than its English-only counterpart, i.e., "curse of multilinguality" that is common in LLMs. Here, we present Meta CLIP 2, the first recipe training CLIP from scratch on worldwide web-scale image-text pairs. To generalize our findings, we conduct rigorous ablations with minimal changes that are necessary to address the above challenges and present a recipe enabling mutual benefits from English and non-English world data. In zero-shot ImageNet classification, Meta CLIP 2 ViT-H/14 surpasses its English-only counterpart by 0.8% and mSigLIP by 0.7%, and surprisingly sets new state-of-the-art without system-level confounding factors (e.g., translation, bespoke architecture changes) on multilingual benchmarks, such as CVQA with 57.4%, Babel-ImageNet with 50.2% and XM3600 with 64.3% on image-to-text retrieval.
                                        </p>
                                    </div>
                                </div>
                            </div>
                        </li>
                    </ul>
                    
                    <h3> Retrieval-based Methods </h3>
                    

                    <ul class="pl">
                        <li>
                            <div class="pub-item">
                                <div class="pub-teaser">
                                    <img src="teaser/jpr.png" alt="JPR teaser">
                                </div>
                                <div class="pub-content">
                                    <h4>
                                        <a href="https://aclanthology.org/2024.findings-eacl.151/" target="_blank">
                                            <b>Joint Inference of Retrieval and Generation for Passage Re-ranking</b>
                                        </a>
                                    </h4>
                                    <div class="authors">
                                        <a href="https://people.csail.mit.edu/weifang/" target="_blank">Wei Fang</a>,
                                        <b>Yung-Sung Chuang</b>,
                                        <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
                                    </div>
                                    <div class="venue">
                                        In <a href="https://2024.eacl.org/" target="_blank">
                                            <b>The 18th Conference of the European Chapter of the Association for Computational Linguistics: Findings</b></a>, 2024.
                                    </div>
                                    <div class="links">
                                        <a href="bibs/fang2024joint.bib" target="_blank" type="text/plain">
                                            <i class="fas fa-quote-left"></i>bib
                                        </a>
                                        <a href="#" onclick="$('#fang2024joint_abstract').toggle();return false;">
                                            <i class="fas fa-align-left"></i>abstract
                                        </a>
                                        <a href="https://aclanthology.org/2024.findings-eacl.151/" target="_blank">
                                            <i class="fas fa-file-pdf"></i>pdf
                                        </a>
                                    </div>
                                    <div id="fang2024joint_abstract" class="abstract" style="display:none;">
                                        <p>
                                            Passage retrieval is a crucial component of modern open-domain question answering (QA) systems, providing information for downstream QA components to generate accurate and transparent answers. This study focuses on passage re-ranking, proposing a simple yet effective method, Joint Passage Re-ranking (JPR), that optimizes the mutual information between query and passage distributions, integrating both cross-encoders and generative models in the re-ranking process. Experimental results demonstrate that JPR outperforms conventional re-rankers and language model scorers in both open-domain QA retrieval settings and diverse retrieval benchmarks under zero-shot settings.
                                        </p>
                                    </div>
                                </div>
                            </div>
                        </li>
                    </ul>

                    <ul class="pl">
                        <li>
                            <div class="pub-item">
                                <div class="pub-teaser">
                                    <img src="teaser/ear.png" alt="EAR teaser">
                                </div>
                                <div class="pub-content">
                                    <h4>
                                        <a href="https://arxiv.org/abs/2305.17080" target="_blank">
                                            <b>Expand, Rerank, and Retrieve: Query Reranking for Open-Domain Question Answering</b>
                                        </a>
                                    </h4>
                                    <div class="authors">
                                        <b>Yung-Sung Chuang</b>,
                                        <a href="https://people.csail.mit.edu/weifang/" target="_blank">Wei Fang</a>,
                                        <a href="https://swdanielli.github.io/" target="_blank">Shang-Wen Li</a>,
                                        <a href="http://scottyih.org/" target="_blank">Wen-tau Yih</a>,
                                        <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
                                    </div>
                                    <div class="venue">
                                        In <a href="https://2023.aclweb.org/" target="_blank">
                                            <b>The 61st Annual Meeting of the Association for Computational Linguistics: Findings</b></a>, 2023.
                                    </div>
                                    <div class="links">
                                        <a href="bibs/chuang2023expand.bib" target="_blank" type="text/plain">
                                            <i class="fas fa-quote-left"></i>bib
                                        </a>
                                        <a href="#" onclick="$('#chuang2023expand_abstract').toggle();return false;">
                                            <i class="fas fa-align-left"></i>abstract
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.17080.pdf" target="_blank">
                                            <i class="fas fa-file-pdf"></i>pdf
                                        </a>
                                        <a href="https://github.com/voidism/EAR" target="_blank">
                                            <i class="fab fa-github"></i>code
                                        </a>
                                    </div>
                                    <div id="chuang2023expand_abstract" class="abstract" style="display:none;">
                                        <p>
                                            We propose EAR, a query Expansion And Reranking approach for improving passage retrieval, with the application to open-domain question answering. EAR first applies a query expansion model to generate a diverse set of queries, and then uses a query reranker to select the ones that could lead to better retrieval results. Motivated by the observation that the best query expansion often is not picked by greedy decoding, EAR trains its reranker to predict the rank orders of the gold passages when issuing the expanded queries to a given retriever. By connecting better the query expansion model and retriever, EAR significantly enhances a traditional sparse retrieval method, BM25. Empirically, EAR improves top-5/20 accuracy by 3-8 and 5-10 points in in-domain and out-of-domain settings, respectively, when compared to a vanilla query expansion model, GAR, and a dense retrieval model, DPR.
                                        </p>
                                    </div>
                                </div>
                            </div>
                        </li>
                    </ul>

                    <ul class="pl">
                        <li>
                            <div class="pub-item">
                                <div class="pub-teaser">
                                    <img src="teaser/sail.png" alt="SAIL teaser">
                                </div>
                                <div class="pub-content">
                                    <h4>
                                        <a href="https://arxiv.org/abs/2305.15225" target="_blank">
                                            <b>SAIL: Search-Augmented Instruction Learning</b>
                                        </a>
                                    </h4>
                                    <div class="authors">
                                        <a href="https://luohongyin.github.io/" target="_blank">Hongyin Luo</a>,
                                        <b>Yung-Sung Chuang</b>,
                                        <a href="https://yuangongnd.github.io/" target="_blank">Yuan Gong</a>, Tianhua Zhang, <a href="https://people.csail.mit.edu/yoonkim/" target="_blank">Yoon Kim</a>, Xixin Wu, Danny Fox, Helen Meng, <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
                                    </div>
                                    <div class="venue">
                                        In <a href="https://aclanthology.org/volumes/2023.findings-emnlp/" target="_blank">
                                            <b>Findings of the Association for Computational Linguistics: EMNLP</b></a>, 2023.
                                    </div>
                                    <div class="links">
                                        <a href="bibs/luo2023sail.bib" target="_blank" type="text/plain">
                                            <i class="fas fa-quote-left"></i>bib
                                        </a>
                                        <a href="#" onclick="$('#luo2023sail_abstract').toggle();return false;">
                                            <i class="fas fa-align-left"></i>abstract
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.15225.pdf" target="_blank">
                                            <i class="fas fa-file-pdf"></i>pdf
                                        </a>
                                        <a href="https://github.com/luohongyin/SAIL" target="_blank">
                                            <i class="fab fa-github"></i>code
                                        </a>
                                        <a href="https://huggingface.co/spaces/luohy/SAIL-7B" target="_blank">
                                            <i class="fas fa-play"></i>demo
                                        </a>
                                        <a href="https://openlsr.org/sail-7b" target="_blank">
                                            <i class="fas fa-globe"></i>web
                                        </a>
                                    </div>
                                    <div id="luo2023sail_abstract" class="abstract" style="display:none;">
                                        <p>
                                            Large language models (LLMs) have been significantly improved by instruction fine-tuning, but still lack transparency and the ability to utilize up-to-date knowledge and information. In this work, we propose search-augmented instruction learning (SAIL), which grounds the language generation and instruction following abilities on complex search results generated by in-house and external search engines. With an instruction tuning corpus, we collect search results for each training case from different search APIs and domains, and construct a new search-grounded training set containing \textit{(instruction, grounding information, response)} triplets. We then fine-tune the LLaMA-7B model on the constructed training set. Since the collected results contain unrelated and disputing languages, the model needs to learn to ground on trustworthy search results, filter out distracting passages, and generate the target response. The search result-denoising process entails explicit trustworthy information selection and multi-hop reasoning, since the retrieved passages might be informative but not contain the instruction-following answer. Experiments show that the fine-tuned SAIL-7B model has a strong instruction-following ability, and it performs significantly better on transparency-sensitive tasks, including open-ended question answering and fact checking.
                                        </p>
                                    </div>
                                </div>
                            </div>
                        </li>
                    </ul>
                    
                    <ul class="pl">
                        <li>
                            <div class="pub-item">
                                <div class="pub-teaser">
                                    <img src="teaser/diffcse.png" alt="DiffCSE teaser">
                                </div>
                                <div class="pub-content">
                                    <h4>
                                        <a href="https://arxiv.org/abs/2204.10298" target="_blank">
                                            <b>DiffCSE: Difference-based Contrastive Learning for Sentence Embeddings</b>
                                        </a>
                                    </h4>
                                    <div class="authors">
                                        <b>Yung-Sung Chuang</b>, 
                                        <a href="http://super-ms.mit.edu/rumen.html" target="_blank">Rumen Dangovski</a>, 
                                        <a href="http://people.csail.mit.edu/hyluo/" target="_blank">Hongyin Luo</a>, 
                                        <a href="https://mitibmwatsonailab.mit.edu/people/yang-zhang/" target="_blank">Yang Zhang</a>, 
                                        <a href="https://code-terminator.github.io/" target="_blank">Shiyu Chang</a>, 
                                        <a href="http://www.mit.edu/~soljacic/marin.html" target="_blank">Marin Soljačić</a>, 
                                        <a href="https://swdanielli.github.io/" target="_blank">Shang-Wen Li</a>, 
                                        <a href="https://scottyih.org/" target="_blank">Scott Wen-tau Yih</a>, 
                                        <a href="https://people.csail.mit.edu/yoonkim/" target="_blank">Yoon Kim</a>, 
                                        <a href="https://www.csail.mit.edu/person/jim-glass" target="_blank">James Glass</a>.
                                    </div>
                                    <div class="venue">
                                        In <a href="https://2022.naacl.org/" target="_blank">
                                            <b>Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</b></a>, 2022. <a class="emph"><b> (Oral Paper) </b></a>
                                    </div>
                                    <div class="links">
                                        <a href="bibs/chuang2022diffcse.bib" target="_blank" type="text/plain">
                                            <i class="fas fa-quote-left"></i>bib
                                        </a>
                                        <a href="#" onclick="$('#chuang2022diffcse_abstract').toggle();return false;">
                                            <i class="fas fa-align-left"></i>abstract
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.10298.pdf" target="_blank">
                                            <i class="fas fa-file-pdf"></i>pdf
                                        </a>
                                        <a href="https://github.com/voidism/DiffCSE" target="_blank">
                                            <i class="fab fa-github"></i>code
                                        </a>
                                        <a href="https://www.youtube.com/watch?v=9vx-HyzcXtU" target="_blank">
                                            <i class="fab fa-youtube"></i>video
                                        </a>
                                    </div>
                                    <!-- [<a href="https://superbbenchmark.org/" target="_blank">leaderboard</a>] -->
                                    <div id="chuang2022diffcse_abstract" class="abstract" style="display:none;">
                                        <p>
                                            We propose DiffCSE, an unsupervised contrastive learning framework for learning sentence embeddings. DiffCSE learns sentence embeddings that are sensitive to the difference between the original sentence and an edited sentence, where the edited sentence is obtained by stochastically masking out the original sentence and then sampling from a masked language model. We show that DiffSCE is an instance of equivariant contrastive learning (Dangovski et al., 2021), which generalizes contrastive learning and learns representations that are insensitive to certain types of augmentations and sensitive to other "harmful" types of augmentations. Our experiments show that DiffCSE achieves state-of-the-art results among unsupervised sentence representation learning methods, outperforming unsupervised SimCSE by 2.3 absolute points on semantic textual similarity tasks.
                                        </p>
                                    </div>
                                </div>
                            </div>
                        </li>
                    </ul>
                </div>
            </div>
            <hr>

            <div class="honors-section" id="honors">
                <h2>Honors</h2>
                <ul class="honors-list">
                    <li class="honor-item">
                        <div class="honor-icon">🎓</div>
                        <div class="honor-content">
                            <div class="award-name">Dean's list (4 times)</div>
                            <div class="award-details">Electrical Engineering Dept. at NTU, Spring '18, Spring '19, Fall '19, Spring '20</div>
                        </div>
                    </li>
                    <li class="honor-item">
                        <div class="honor-icon">💰</div>
                        <div class="honor-content">
                            <div class="award-name">Irving T. Ho Memorial Scholarship (2 times)</div>
                            <div class="award-details">EECS at NTU, Fall '18, Fall '19</div>
                        </div>
                    </li>
                    <!-- <li class="honor-item">
                        <div class="honor-icon">✈️</div>
                        <div class="honor-content">
                            <div class="award-name">Travel Grant</div>
                            <div class="award-details">INTERSPEECH 2020 conference, Sep. 2020</div>
                        </div>
                    </li>
                    <li class="honor-item">
                        <div class="honor-icon">🏆</div>
                        <div class="honor-content">
                            <div class="award-name">Appier Best Application Award</div>
                            <div class="award-details">2020 NTU CSIE Undergrad Special Research Exhibition, Jun. 2020</div>
                        </div>
                    </li>
                    <li class="honor-item">
                        <div class="honor-icon">🥈</div>
                        <div class="honor-content">
                            <div class="award-name">2nd Place & Appier 1st Award</div>
                            <div class="award-details">2019 NTU CSIE Undergrad Special Research Exhibition, Jun. 2019</div>
                        </div>
                    </li>
                    <li class="honor-item">
                        <div class="honor-icon">🥈</div>
                        <div class="honor-content">
                            <div class="award-name">2nd Place</div>
                            <div class="award-details">2019 NTUEE Undergraduate Innovation Award, Jun. 2019</div>
                        </div>
                    </li>
                    <li class="honor-item">
                        <div class="honor-icon">🥇</div>
                        <div class="honor-content">
                            <div class="award-name">1st Place</div>
                            <div class="award-details">2018 H. Spectrum Demo Day (out of 21 teams), Jul. 2018</div>
                        </div>
                    </li>
                    <li class="honor-item">
                        <div class="honor-icon">🥇</div>
                        <div class="honor-content">
                            <div class="award-name">1st Place</div>
                            <div class="award-details">NCTS Health Hackathon 2018 (out of 18 teams), Jun. 2018</div>
                        </div>
                    </li>
                    <li class="honor-item">
                        <div class="honor-icon">🏅</div>
                        <div class="honor-content">
                            <div class="award-name">Top 8 Finalist</div>
                            <div class="award-details">Microsoft Imagine Cup Taiwan National Final 2018, Apr. 2018</div>
                        </div>
                    </li>
                    <li class="honor-item">
                        <div class="honor-icon">🏆</div>
                        <div class="honor-content">
                            <div class="award-name">Best Tech Award & Microsoft Enterprise Award</div>
                            <div class="award-details">MakeNTU 2018 (out of 50 teams), Mar. 2018</div>
                        </div>
                    </li>
                    <li class="honor-item">
                        <div class="honor-icon">🥇</div>
                        <div class="honor-content">
                            <div class="award-name">1st place of Dept. of Transportation</div>
                            <div class="award-details">HackNTU 2017 (out of 100+ teams), Jul. 2017</div>
                        </div>
                    </li> -->
                </ul>
            </div>
            <hr>
            <div class="services-section" id="services">
                <h2>Services</h2>
                <div class="service-category">
                    <h4>Reviewer</h4>
                    <div class="service-venues">
                        <span class="service-venue">NeurIPS 2021, 2022, 2023, 2024, 2025</span>
                        <span class="service-venue">ICLR 2022, 2023, 2024, 2025</span>
                        <span class="service-venue">ICML 2022, 2023, 2024, 2025</span>
                        <span class="service-venue">ACL ARR 2023, 2024, 2025</span>
                        <span class="service-venue">EMNLP 2022, 2023</span>
                        <span class="service-venue">ACL 2023</span>
                        <span class="service-venue">AAAI 2023</span>
                        <span class="service-venue">ICASSP 2022, 2023</span>
                        <span class="service-venue">TASL 2023, 2024, 2025</span>
                    </div>
                </div>
            </div>
            <hr>
            <div class="attribution-section">
                <div class="attribution-card">
                    <i class="fas fa-code" style="margin-right: 8px; color: #666;"></i>
                    This website is built from the <a href="https://github.com/nelson-liu/website" target="_blank">source code</a> of Nelson F. Liu's awesome website (<a href="https://nelsonliu.me/" target="_blank">nelsonliu.me</a>).
                </div>
            </div>
            <footer class="pt-2 my-md-2 pt-md-2 border-top">
                <div class="row justify-content-center">
                    <div class="col-6 col-md text-left align-self-center">
                        <p class="h5 text-muted">
                            © Yung-Sung Chuang, 2025
                        </p>
                    </div>
                    <div class="col-6 col-md text-right">
                        <a href="https://www.csail.mit.edu/" class="image-link">
                            <img class="mr-4" src="img/mit_csail_logo.svg" alt="MIT CSAIL logo." height="75">
                        </a>
                        <a href="https://www.mit.edu/" class="image-link">
                            <img src="img/mit_logo.svg" alt="MIT logo." height="50">
                        </a>
                    </div>
                </div>
            </footer>
        </div>

        <!-- Image Modal -->
        <div id="imageModal" class="image-modal">
            <img id="modalImage" src="" alt="">
        </div>

        <!-- Bib Modal -->
        <div id="bibModal" class="bib-modal">
            <div class="bib-modal-content">
                <div class="bib-modal-header">
                    <h3 class="bib-modal-title" id="bibModalTitle">BibTeX Citation</h3>
                    <button class="bib-modal-close" id="bibModalClose">&times;</button>
                </div>
                <div class="bib-modal-text" id="bibModalText"></div>
            </div>
        </div>

        <script>
            document.addEventListener('DOMContentLoaded', function() {
                // Mobile header responsive text
                function updateHeaderText() {
                    const nameElement = document.querySelector('.nav-brand a');
                    const navLinks = document.querySelectorAll('.nav-links a');
                    
                    if (window.innerWidth <= 768) {
                        // Mobile: two-line name
                        nameElement.innerHTML = 'Yung-Sung<br>Chuang';
                        
                        // Mobile: labels with line breaks for longer ones
                        const mobileLabels = ['Google<br>Scholar', 'CV', 'Twitter', 'Github', 'LinkedIn', 'Blog'];
                        navLinks.forEach((link, index) => {
                            const icon = link.querySelector('i');
                            const badge = link.querySelector('.citation-badge');
                            link.innerHTML = '';
                            link.appendChild(icon);
                            if (badge) link.appendChild(badge);
                            
                            // Create text element with HTML for line breaks
                            const textSpan = document.createElement('span');
                            textSpan.innerHTML = mobileLabels[index];
                            link.appendChild(textSpan);
                        });
                    } else {
                        // Desktop: full name
                        nameElement.textContent = 'Yung-Sung Chuang';
                        
                        // Desktop: full labels for nav links
                        const desktopLabels = ['Google Scholar', 'CV', 'Twitter', 'Github', 'LinkedIn', 'Blog'];
                        navLinks.forEach((link, index) => {
                            const icon = link.querySelector('i');
                            const badge = link.querySelector('.citation-badge');
                            link.innerHTML = '';
                            link.appendChild(icon);
                            if (badge) link.appendChild(badge);
                            link.appendChild(document.createTextNode(desktopLabels[index]));
                        });
                    }
                }

                // Initial call and resize listener
                updateHeaderText();
                window.addEventListener('resize', updateHeaderText);
                // Image popup functionality
                const imageModal = document.getElementById('imageModal');
                const modalImg = document.getElementById('modalImage');
                const teaserImages = document.querySelectorAll('.pub-teaser img');

                // Add click event to all teaser images
                teaserImages.forEach(function(img) {
                    img.addEventListener('click', function() {
                        imageModal.style.display = 'block';
                        modalImg.src = this.src;
                        modalImg.alt = this.alt;
                        document.body.style.overflow = 'hidden';
                    });
                });

                // Close image modal when clicking anywhere on it
                imageModal.addEventListener('click', function() {
                    imageModal.style.display = 'none';
                    document.body.style.overflow = 'auto';
                });

                // Bib modal functionality
                const bibModal = document.getElementById('bibModal');
                const bibModalTitle = document.getElementById('bibModalTitle');
                const bibModalText = document.getElementById('bibModalText');
                const bibModalClose = document.getElementById('bibModalClose');

                // Bib content mapping
                const bibContent = {
                    'chuang2025selfcite': `@article{chuang2025selfcite,
  title={SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models},
  author={Chuang, Yung-Sung and Cohen-Wang, Benjamin and Shen, Shannon Zejiang and Wu, Zhaofeng and Xu, Hu and Lin, Xi Victoria and Glass, James and Li, Shang-Wen and Yih, Wen-tau},
  journal={arXiv preprint arXiv:2502.09604},
  year={2025}
}`,
                    'chuang2024lookback': `@inproceedings{chuang2024lookback,
  title={Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps},
  author={Chuang, Yung-Sung and Qiu, Linlu and Hsieh, Cheng-Yu and Krishna, Ranjay and Kim, Yoon and Glass, James},
  booktitle={The Conference on Empirical Methods in Natural Language Processing},
  year={2024}
}`,
                    'chuang2023dola': `@inproceedings{chuang2023dola,
  title={DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models},
  author={Chuang, Yung-Sung and Xie, Yujia and Luo, Hongyin and Kim, Yoon and Glass, James R and He, Pengcheng},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}`,
                    'cohenwang2025learning': `@article{cohenwang2025learning,
  title={Learning to Attribute with Attention},
  author={Cohen-Wang, Benjamin and Chuang, Yung-Sung and Madry, Aleksander},
  journal={arXiv preprint arXiv:2504.13752},
  year={2025}
}`,
                    'chuang2025metaclip2': `@article{chuang2025metaclip2,
  title={Meta CLIP 2: A Worldwide Scaling Recipe},
  author={Chuang, Yung-Sung and Li, Yang and Wang, Dong and Yeh, Ching-Feng and Lyu, Kehan and Raghavendra, Ramya and Glass, James and Huang, Lifei and Weston, Jason and Zettlemoyer, Luke and Chen, Xinlei and Liu, Zhuang and Xie, Saining and Yih, Wen-tau and Li, Shang-Wen and Xu, Hu},
  journal={arXiv preprint arXiv:2507.22062},
  year={2025}
}`,
                    'fang2024joint': `@inproceedings{fang2024joint,
  title={Joint Inference of Retrieval and Generation for Passage Re-ranking},
  author={Fang, Wei and Chuang, Yung-Sung and Glass, James},
  booktitle={The 18th Conference of the European Chapter of the Association for Computational Linguistics: Findings},
  year={2024}
}`,
                    'chuang2023expand': `@inproceedings{chuang2023expand,
  title={Expand, Rerank, and Retrieve: Query Reranking for Open-Domain Question Answering},
  author={Chuang, Yung-Sung and Fang, Wei and Li, Shang-Wen and Yih, Wen-tau and Glass, James},
  booktitle={The 61st Annual Meeting of the Association for Computational Linguistics: Findings},
  year={2023}
}`,
                    'luo2023sail': `@inproceedings{luo2023sail,
  title={SAIL: Search-Augmented Instruction Learning},
  author={Luo, Hongyin and Chuang, Yung-Sung and Gong, Yuan and Zhang, Tianhua and Kim, Yoon and Wu, Xixin and Fox, Danny and Meng, Helen and Glass, James},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP},
  year={2023}
}`,
                    'chuang2022diffcse': `@inproceedings{chuang2022diffcse,
  title={{DiffCSE}: Difference-based Contrastive Learning for Sentence Embeddings},
  author={Chuang, Yung-Sung and Dangovski, Rumen and Luo, Hongyin and Zhang, Yang and Chang, Shiyu and Soljačić, Marin and Li, Shang-Wen and Yih, Wen-tau and Kim, Yoon and Glass, James},
  booktitle={Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)},
  year={2022}
}`
                };

                // Add click event to all bib links
                document.querySelectorAll('a[href*=".bib"]').forEach(function(link) {
                    link.addEventListener('click', function(e) {
                        e.preventDefault();
                        const href = this.getAttribute('href');
                        const bibKey = href.replace('bibs/', '').replace('.bib', '');
                        
                        if (bibContent[bibKey]) {
                            bibModalTitle.textContent = 'BibTeX Citation';
                            bibModalText.textContent = bibContent[bibKey];
                            bibModal.style.display = 'block';
                            document.body.style.overflow = 'hidden';
                        }
                    });
                });

                // Close bib modal when clicking background
                bibModal.addEventListener('click', function(e) {
                    if (e.target === bibModal) {
                        bibModal.style.display = 'none';
                        document.body.style.overflow = 'auto';
                    }
                });

                // Close bib modal when clicking X button
                bibModalClose.addEventListener('click', function() {
                    bibModal.style.display = 'none';
                    document.body.style.overflow = 'auto';
                });

                // Close modals with Escape key
                document.addEventListener('keydown', function(event) {
                    if (event.key === 'Escape') {
                        if (imageModal.style.display === 'block') {
                            imageModal.style.display = 'none';
                            document.body.style.overflow = 'auto';
                        }
                        if (bibModal.style.display === 'block') {
                            bibModal.style.display = 'none';
                            document.body.style.overflow = 'auto';
                        }
                    }
                });
            });
        </script>
    </body>
</html>
