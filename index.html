<!DOCTYPE html>
<html lang="en">
    <head>
        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-116076474-3"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-116076474-3');
        </script>
        <title>Yung-Sung Chuang</title>
        <meta http-equiv="content-type" content="text/html; charset=UTF-8">
        <meta charset="utf-8">
        <meta property="og:url" content="https://people.csail.mit.edu/yungsung/" />
	    <meta property="og:title" content="Yung-Sung Chuang" />
	    <meta property="og:image" content="img/Yung-Sung.jpg" />
	    <meta http-equiv="X-UA-Compatible" content="IE=edge">
	    <meta name="author" content="Yung-Sung Chuang">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
        <!-- <link rel="shortcut icon" type="image/png" href="favicon.ico"/> -->

        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
        <link rel="stylesheet" href="css/style.css">
        <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
        <link href='https://fonts.googleapis.com/css?family=EB+Garamond' rel='stylesheet' type='text/css'>
    </head>
    <body>
        <div class="container mt-5">
            <div class="row mb-3">
                <div class="col">
                    <h1>Yung-Sung Chuang</h1>
                </div>
            </div>
            <div class="row">
                <div class="col-lg-4 col-md-6 order-0 order-xs-0 order-sm-0 order-md-1 order-lg-1">
                    <div class="card mb-3">
                        <img class="card-img-top" src="img/Yung-Sung.jpg" alt="Yung-Sung Chuang">
                        <div class="card-body">
                            <h5 class="card-title">
                                <b>Yung-Sung Chuang</b>
                            </h5>
                            <p class="card-text">
                                MIT EECS PhD Student
                                </br>
                                Office: 32-G436
                            </p>
                        </div>
                    </div>
                </div>
                <div class="col-lg-8 col-md-6 order-1 order-xs-1 order-sm-1 order-md-0 order-lg-0">
                    <p>
                        Hi! I'm a first-year PhD student in <a href="https://www.eecs.mit.edu/" target="_blank">Electrical Engineering and Computer Science</a> at
                        <a href="https://www.mit.edu" target="_blank">Massachusetts Institute of Technology</a>, where I work with <a href="https://www.csail.mit.edu/person/jim-glass"
                        target="_blank">Jim Glass</a>. My research interest broadly covers 
                        the deep learning technique for natural language processing and 
                        speech processing. In particular, I aim to utilize the ability of 
                        machines to help people grasp large information in text/audio form 
                        in efficient ways.
                    </p>
                    <p>
                        Previously, I was an undergraduate student in Electrical Engineering
                        at National Taiwan University. I joined Speech Processing Lab 
                        supervised by <a href="https://speech.ee.ntu.edu.tw/~hylee/index.html">Hung-Yi Lee</a> and Machine Intelligence Understanding Lab 
                        supervised by <a href="https://www.csie.ntu.edu.tw/~yvchen/">Yun-Nung (Vivian) Chen</a>. I received the NTU Presidential 
                        Award for top 5% students four times in 2018-2020, <a href="https://irvingthofoundation.github.io/ho-fellows.htm">Irving T. Ho Memorial 
                        Scholarship</a> in 2018 and 2019. Here is my <a href="https://people.csail.mit.edu/yungsung/CV.pdf">Curriculum Vitae</a>.
                    </p>
                </div>
            </div>
            <div class="row">
                <div class="col">
                    <p>
                        Email: yungsung [AT] mit.edu
                    </p>
                    <p>
                        Links:
                        [<a href="files/CV.pdf" target="_blank">Full CV</a>] [<a href="https://twitter.com/YungSungChuang" target="_blank">Twitter</a>] [<a href="https://github.com/voidism" target="_blank">Github</a>] [<a href="https://scholar.google.com/citations?hl=en&user=3ar1DOwAAAAJ" target="_blank">Google Scholar</a>] [<a href="https://dblp.org/pers/hd/c/Chuang:Yung=Sung" target="_blank">DBLP</a>] [<a href="https://voidism.github.io/" target="_blank">Blog</a>] [<a href="https://www.linkedin.com/in/yschuang">Linkedin</a>]
                    </p>
                </div>
            </div>
            <div class="row">
                <div class="col">
                    <h2>Recent News</h2>
                    <ul>
                        <li>
                            (9/2021) I started my PhD life in Cambridge!
                        </li>
                    </ul>
                </div>
            </div>
            <hr>
            <div class="row" id="publications">
                <div class="col">
                    <h2>Publications</h2>
                    <h3>2021</h3>
                    <ul class="pl">
                        <li>
                            <a href="https://arxiv.org/abs/2106.05933" target="_blank">
                                <b>PARP: Prune, Adjust and Re-Prune for Self-Supervised Speech Recognition</b>
                            </a>
                            <br/>
                            <a href="http://people.csail.mit.edu/clai24/" target="_blank">Cheng-I Jeff Lai*</a>, 
                            Yang Zhang, Alexander H. Liu, Shiyu Chang, Yi-Lun Liao, 
                            <b>Yung-Sung Chuang*</b>, 
                            Kaizhi Qian, Sameer Khurana, David Cox, James Glass
                            <br/>
                            In <a href="https://www.interspeech2021.org/" target="_blank">
                                <b>
                                    Thirty-fifth Conference on Neural Information Processing Systems (NeurIPS)</b></a>, 2021.
                            <br/>
                            [<a href="bibs/yang2021superb.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#yang2020superb_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://arxiv.org/pdf/2105.01051.pdf" target="_blank">pdf</a>]
                            [<a href="https://github.com/s3prl/s3prl" target="_blank">code</a>]
                            [<a href="https://www.youtube.com/watch?v=zd9fiVvej0k" target="_blank">video</a>]
                            [<a href="https://superbbenchmark.org/" target="_blank">leaderboard</a>]
                            <div id="yang2020superb_abstract" class="abstract" style="display:none;">
                                <p>
                                    Self-supervised learning (SSL) has proven vital for advancing research in natural language processing (NLP) and computer vision (CV). The paradigm pretrains a shared model on large volumes of unlabeled data and achieves state-of-the-art (SOTA) for various tasks with minimal adaptation. However, the speech processing community lacks a similar setup to systematically explore the paradigm. To bridge this gap, we introduce Speech processing Universal PERformance Benchmark (SUPERB). SUPERB is a leaderboard to benchmark the performance of a shared model across a wide range of speech processing tasks with minimal architecture changes and labeled data. Among multiple usages of the shared model, we especially focus on extracting the representation learned from SSL due to its preferable re-usability. We present a simple framework to solve SUPERB tasks by learning task-specialized lightweight prediction heads on top of the frozen shared model. Our results demonstrate that the framework is promising as SSL representations show competitive generalizability and accessibility across SUPERB tasks. We release SUPERB as a challenge with a leaderboard and a benchmark toolkit to fuel the research in representation learning and general speech processing.
                                </p>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <a href="https://arxiv.org/abs/2105.01051" target="_blank">
                                <b>SUPERB: Speech processing Universal PERformance Benchmark</b>
                            </a>
                            <br/>
                            <a href="https://scholar.google.com.tw/citations?user=R1mNI8QAAAAJ" target="_blank">Shu-wen Yang</a>, <a href="https://scholar.google.com/citations?user=SiyicoEAAAAJ&hl=zh-TW" target="_blank">Po-Han Chi*</a>, <b>Yung-Sung Chuang*</b>, 
                            <a href="http://people.csail.mit.edu/clai24/" target="_blank">Cheng-I Jeff Lai*</a>, 
                            <a href="https://ai.facebook.com/people/kushal-lakhotia/" target="_blank">Kushal Lakhotia*</a>, <a href="https://scholar.google.com/citations?user=0lrZq9MAAAAJ&hl=en" target="_blank">Yist Y. Lin*</a>, 
                            <a href="https://andi611.github.io/" target="_blank">Andy T. Liu*</a>, Jiatong Shi*, Xuankai Chang, Guan-Ting Lin, 
                            Tzu-Hsien Huang, Wei-Cheng Tseng, Ko-tik Lee, Da-Rong Liu, 
                            Zili Huang, <a href="https://www.amazon.science/author/shuyan-dong" target="_blank">Shuyan Dong</a>, <a href="https://swdanielli.github.io/" target="_blank">Shang-Wen Li</a>, 
                            <a href="https://sites.google.com/view/shinjiwatanabe" target="_blank">Shinji Watanabe</a>, 
                            <a href="http://www.cs.toronto.edu/~asamir/" target="_blank">Abdelrahman Mohamed</a>, <a href="https://speech.ee.ntu.edu.tw/~hylee/index.html" target="_blank">Hung-Yi Lee</a>.
                            <br/>
                            In <a href="https://www.interspeech2021.org/" target="_blank">
                                <b>
                                    Interspeech</b></a>, 2021.
                            <br/>
                            [<a href="bibs/yang2021superb.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#yang2020superb_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://arxiv.org/pdf/2105.01051.pdf" target="_blank">pdf</a>]
                            [<a href="https://github.com/s3prl/s3prl" target="_blank">code</a>]
                            [<a href="https://www.youtube.com/watch?v=zd9fiVvej0k" target="_blank">video</a>]
                            [<a href="https://superbbenchmark.org/" target="_blank">leaderboard</a>]
                            <div id="yang2020superb_abstract" class="abstract" style="display:none;">
                                <p>
                                    Self-supervised learning (SSL) has proven vital for advancing research in natural language processing (NLP) and computer vision (CV). The paradigm pretrains a shared model on large volumes of unlabeled data and achieves state-of-the-art (SOTA) for various tasks with minimal adaptation. However, the speech processing community lacks a similar setup to systematically explore the paradigm. To bridge this gap, we introduce Speech processing Universal PERformance Benchmark (SUPERB). SUPERB is a leaderboard to benchmark the performance of a shared model across a wide range of speech processing tasks with minimal architecture changes and labeled data. Among multiple usages of the shared model, we especially focus on extracting the representation learned from SSL due to its preferable re-usability. We present a simple framework to solve SUPERB tasks by learning task-specialized lightweight prediction heads on top of the frozen shared model. Our results demonstrate that the framework is promising as SSL representations show competitive generalizability and accessibility across SUPERB tasks. We release SUPERB as a challenge with a leaderboard and a benchmark toolkit to fuel the research in representation learning and general speech processing.
                                </p>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <a href="https://arxiv.org/abs/2010.13826" target="_blank">
                                <b>Semi-Supervised Spoken Language Understanding via Self-Supervised Speech and Language Model Pretraining</b>
                            </a>
                            <br/>
                            <a href="http://people.csail.mit.edu/clai24/" target="_blank">Cheng-I Lai</a>,
                            <b>Yung-Sung Chuang</b>,
                            <a href="https://speech.ee.ntu.edu.tw/~hylee/index.html" target="_blank">Hung-Yi Lee</a>,
                            <a href="https://swdanielli.github.io/" target="_blank">Shang-Wen Li</a>,
                            and <a href="https://www.csail.mit.edu/person/jim-glass" target="_blank">James Glass</a>.
                            <br/>
                            In <a href="https://2021.ieeeicassp.org/" target="_blank">
                                <b>
                                    IEEE International Conference on Acoustics, Speech and Signal Processing
                                    (ICASSP)</b></a>, 2021.
                            <br/>
                            [<a href="bibs/lai2020semi.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#lai2020semi_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://arxiv.org/pdf/2010.13826.pdf" target="_blank">pdf</a>]
                            [<a href="https://github.com/jefflai108/Semi-Supervsied-Spoken-Language-Understanding-PyTorch" target="_blank">code</a>]
                            [<a href="https://www.youtube.com/watch?v=5GOIPp9hWkY" target="_blank">video</a>]
                            <!-- [<a href="https://leaderboard.allenai.org/quoref" target="_blank">leaderboard</a>] -->
                            <div id="lai2020semi_abstract" class="abstract" style="display:none;">
                                <p>
                                    Much recent work on Spoken Language Understanding (SLU) is limited in at least one of three ways: models were trained on oracle text input and neglected ASR errors, models were trained to predict only intents without the slot values, or models were trained on a large amount of in-house data. In this paper, we propose a clean and general framework to learn semantics directly from speech with semi-supervision from transcribed or untranscribed speech to address these issues. Our framework is built upon pretrained end-to-end (E2E) ASR and self-supervised language models, such as BERT, and fine-tuned on a limited amount of target SLU data. We study two semi-supervised settings for the ASR component: supervised pretraining on transcribed speech, and unsupervised pretraining by replacing the ASR encoder with self-supervised speech representations, such as wav2vec. In parallel, we identify two essential criteria for evaluating SLU models: environmental noise-robustness and E2E semantics evaluation. Experiments on ATIS show that our SLU framework with speech as input can perform on par with those using oracle text as input in semantics understanding, even though environmental noise is present and a limited amount of labeled semantics data is available for training.
                                </p>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <a href="papers/logan+liu+peters+gardner+singh.acl2019.pdf" target="_blank">
                                <b>Barack's Wife Hillary: Using Knowledge Graphs for Fact-Aware Language Modeling</b>
                            </a>
                            <br/>
                            <a href="https://rloganiv.github.io/" target="_blank">Robert L. Logan IV</a>,
                            <b>Nelson F. Liu</b>,
                            <a href="http://matt-peters.github.io/" target="_blank">Matthew E. Peters</a>,
                            <a href="https://matt-gardner.github.io/" target="_blank">Matt Gardner</a>,
                            and <a href="http://sameersingh.org/" target="_blank">Sameer Singh</a>.
                            <br/>
                            In <a href="http://www.acl2019.org/" target="_blank">
                                <b>Annual Meeting of the Association for Computational Linguistics (ACL)</b></a>, 2019.
                            <br/>
                            [<a href="papers/logan+liu+peters+gardner+singh.acl2019.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#logan_liu_peters_gardner_singh_acl2019_abstract').toggle();return false;">abstract</a>]
                            [<a href="papers/logan+liu+peters+gardner+singh.acl2019.poster.pdf" target="_blank">poster</a>]
                            [<a href="https://github.com/rloganiv/kglm-model" target="_blank">code</a>]
                            [<a href="https://rloganiv.github.io/linked-wikitext-2/" target="_blank">dataset</a>]
                            <div id="logan_liu_peters_gardner_singh_acl2019_abstract" class="abstract" style="display:none;">
                                <p>
                                    Modeling human language requires the ability
                                    to not only generate fluent text but also
                                    encode factual knowledge. However,
                                    traditional language models are only capable
                                    of remembering facts seen at training time,
                                    and often have difficulty recalling them. To
                                    address this, we introduce the knowledge
                                    graph language model (KGLM), a neural
                                    language model with mechanisms for selecting
                                    and copying facts from a knowledge graph
                                    that are relevant to the context. These
                                    mechanisms enable the model to render
                                    information it has never seen before, as
                                    well as generate out-of-vocabulary tokens.
                                    We also introduce the Linked WikiText-2
                                    dataset, a corpus of annotated text aligned
                                    to the Wikidata knowledge graph whose
                                    contents (roughly) match the popular
                                    WikiText-2 benchmark (Merity et al., 2017).
                                    In experiments, we demonstrate that the KGLM
                                    achieves significantly better performance
                                    than a strong baseline language model. We
                                    additionally compare different language
                                    models’ ability to complete sentences
                                    requiring factual knowledge, and show that
                                    the KGLM outperforms even very large
                                    language models in generating facts.
                                </p>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <a href="papers/liu+gardner+belinkov+peters+smith.naacl2019.pdf" target="_blank">
                                <b>Linguistic Knowledge and Transferability of Contextual Representations</b>
                            </a>
                            <br/>
                            <b>Nelson F. Liu</b>,
                            <a href="https://matt-gardner.github.io/" target="_blank">Matt Gardner</a>,
                            <a href="http://people.csail.mit.edu/belinkov/" target="_blank">Yonatan Belinkov</a>,
                            <a href="http://matt-peters.github.io/" target="_blank">Matthew E. Peters</a>,
                            and <a href="https://homes.cs.washington.edu/~nasmith/" target="_blank">Noah A. Smith</a>.
                            <br/>
                            In <a href="https://naacl2019.org/" target="_blank">
                                <b>North American Chapter of the Association for Computational Linguistics (NAACL)</b></a>, 2019.
                            <br/>
                            [<a href="papers/liu+gardner+belinkov+peters+smith.naacl2019.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#liu_gardner_belinkov_peters_smith_naacl2019_abstract').toggle();return false;">abstract</a>]
                            [slides:
                            <a href="papers/liu+gardner+belinkov+peters+smith.naacl2019.slides.pdf" target="_blank">pdf</a>,
                            <a href="papers/liu+gardner+belinkov+peters+smith.naacl2019.slides.with_speaker_notes.pdf" target="_blank">pdf with notes</a>,
                            <a href="papers/liu+gardner+belinkov+peters+smith.naacl2019.slides.key" target="_blank">key</a>]
                            [<a href="papers/contextual-repr-analysis" target="_blank">code</a>]
                            <div id="liu_gardner_belinkov_peters_smith_naacl2019_abstract" class="abstract" style="display:none;">
                                <p>
                                    Contextual word representations derived from
                                    large-scale neural language models are
                                    successful across a diverse set of NLP
                                    tasks, suggesting that they encode useful
                                    and transferable features of language. To
                                    shed light on the linguistic knowledge they
                                    capture, we study the representations
                                    produced by several recent pretrained
                                    contextualizers (variants of ELMo, the
                                    OpenAI transformer LM, and BERT) with a
                                    suite of seventeen diverse probing tasks. We
                                    find that linear models trained on top of
                                    frozen contextual representations are
                                    competitive with state-of-the-art
                                    task-specific models in many cases, but fail
                                    on tasks requiring fine-grained linguistic
                                    knowledge (e.g., conjunct identification).
                                    To investigate the transferability of
                                    contextual word representations, we quantify
                                    differences in the transferability of
                                    individual layers within contextualizers,
                                    especially between RNNs and transformers.
                                    For instance, higher layers of RNNs are more
                                    task-specific, while transformer layers do
                                    not exhibit the same monotonic trend. In
                                    addition, to better understand what makes
                                    contextual word representations
                                    transferable, we compare language model
                                    pretraining with eleven supervised
                                    pretraining tasks. For any given task,
                                    pretraining on a closely related task yields
                                    better performance than language model
                                    pretraining (which is better on average)
                                    when the pretraining dataset is fixed.
                                    However, language model pretraining on more
                                    data gives the best results.
                                </p>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <a href="papers/liu+schwartz+smith.naacl2019.pdf" target="_blank">
                                <b>Inoculation by Fine-Tuning: A Method for Analyzing Challenge Datasets</b>
                            </a>
                            <br/>
                            <b>Nelson F. Liu</b>,
                            <a href="https://homes.cs.washington.edu/~roysch/" target="_blank">Roy Schwartz</a>,
                            and <a href="https://homes.cs.washington.edu/~nasmith/" target="_blank">Noah A. Smith</a>.
                            <br/>
                            In <a href="https://naacl2019.org/" target="_blank">
                                <b>North American Chapter of the Association for Computational Linguistics (NAACL)</b></a>, 2019.
                            <br/>
                            [<a href="papers/liu+schwartz+smith.naacl2019.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#liu_schwartz_smith_naacl2019_abstract').toggle();return false;">abstract</a>]
                            [slides:
                            <a href="papers/liu+schwartz+smith.naacl2019.slides.pdf" target="_blank">pdf</a>,
                            <a href="papers/liu+schwartz+smith.naacl2019.slides.with_speaker_notes.pdf" target="_blank">pdf with notes</a>,
                            <a href="papers/liu+schwartz+smith.naacl2019.slides.key" target="_blank">key</a>]
                            [<a href="papers/inoculation-by-finetuning" target="_blank">code</a>]
                            <div id="liu_schwartz_smith_naacl2019_abstract" class="abstract" style="display:none;">
                                <p>
                                    Several datasets have recently been
                                    constructed to expose brittleness in models
                                    trained on existing benchmarks. While model
                                    performance on these challenge datasets is
                                    significantly lower compared to the original
                                    benchmark, it is unclear what particular
                                    weaknesses they reveal. For example, a
                                    challenge dataset may be difficult because
                                    it targets phenomena that current models
                                    cannot capture, or because it simply
                                    exploits blind spots in a model's specific
                                    training set. We introduce inoculation by
                                    fine-tuning, a new analysis method for
                                    studying challenge datasets by exposing
                                    models (the metaphorical patient) to a small
                                    amount of data from the challenge dataset (a
                                    metaphorical pathogen) and assessing how
                                    well they can adapt. We apply our method to
                                    analyze the NLI "stress tests" (Naik et al.,
                                    2018) and the Adversarial SQuAD dataset (Jia
                                    and Liang, 2017). We show that after slight
                                    exposure, some of these datasets are no
                                    longer challenging, while others remain
                                    difficult. Our results indicate that
                                    failures on challenge datasets may lead to
                                    very different conclusions about models,
                                    training datasets, and the challenge
                                    datasets themselves.
                                </p>
                            </div>
                        </li>
                    </ul>
                    <h3>2018</h3>
                    <ul class="pl">
                        <li>
                            <a href="papers/liu+levy+schwartz+tan+smith.repl4nlp2018.pdf" target="_blank">
                                <b>LSTMs Exploit Linguistic Attributes of Data</b>
                            </a>
                            <br/>
                            <b>Nelson F. Liu</b>,
                            <a href="https://levyomer.wordpress.com/" target="_blank">Omer Levy</a>,
                            <a href="https://homes.cs.washington.edu/~roysch/" target="_blank">Roy Schwartz</a>,
                            <a href="https://chenhaot.com/" target="_blank">Chenhao Tan</a>,
                            and <a href="https://homes.cs.washington.edu/~nasmith/" target="_blank">Noah A. Smith</a>.
                            <br/>
                            In <a href="https://sites.google.com/site/repl4nlp2018" target="_blank">
                                <b>ACL Workshop on Representation Learning for NLP (RepL4NLP)</b></a>, 2018.
                            <b><a href="https://sites.google.com/site/repl4nlp2018/accepted-papers" target="_blank" style="color:#e22222">(Best Paper Award)</a></b>.
                            <br/>
                            [<a href="papers/liu+levy+schwartz+tan+smith.repl4nlp2018.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#liu_levy_schwartz_tan_smith_repl4nlp2018_abstract').toggle();return false;">abstract</a>]
                            [<a href="papers/liu+levy+schwartz+tan+smith.repl4nlp2018.slides.pdf" target="_blank">(short) slides</a>]
                            [<a href="papers/liu+levy+schwartz+tan+smith.repl4nlp2018.poster.pdf" target="_blank">poster</a>]
                            [<a href="papers/lstms-exploit-linguistic-attributes" target="_blank">code</a>]
                            <div id="liu_levy_schwartz_tan_smith_repl4nlp2018_abstract" class="abstract" style="display:none;">
                                <p>
                                    While recurrent neural networks have found
                                    success in a variety of natural language
                                    processing applications, they are general
                                    models of sequential data. We investigate
                                    how the properties of natural language data
                                    affect an LSTM's ability to learn a
                                    nonlinguistic task: recalling elements from
                                    its input. We find that models trained on
                                    natural language data are able to recall
                                    tokens from much longer sequences than
                                    models trained on non-language sequential
                                    data. Furthermore, we show that the LSTM
                                    learns to solve the memorization task by
                                    explicitly using a subset of its neurons to
                                    count timesteps in the input. We hypothesize
                                    that the patterns and structure in natural
                                    language data enable LSTMs to learn by
                                    providing approximate ways of reducing loss,
                                    but understanding the effect of different
                                    training data on the learnability of LSTMs
                                    remains an open question.
                                </p>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <a href="papers/allennlp.nlposs2018.pdf" target="_blank">
                                <b>AllenNLP: A Deep Semantic Natural Language Processing Platform</b>
                            </a>
                            <br/>
                            <a href="https://matt-gardner.github.io/" target="_blank">Matt Gardner</a>,
                            <a href="http://joelgrus.com/" target="_blank">Joel Grus</a>,
                            <a href="http://markneumann.xyz/" target="_blank">Mark Neumann</a>,
                            <a href="https://allenai.org/team/oyvindt/" target="_blank">Oyvind Tafjord</a>,
                            <a href="http://www.cs.cmu.edu/~pdasigi/" target="_blank">Pradeep Dasigi</a>,
                            <b>Nelson F. Liu</b>,
                            <a href="http://matt-peters.github.io/" target="_blank">Matthew Peters</a>,
                            <a href="https://schmitztech.com/" target="_blank">Michael Schmitz</a>,
                            and <a href="https://www.cs.washington.edu/people/faculty/lsz/" target="_blank">Luke Zettlemoyer</a>.
                            <br/>
                            In <a href="https://nlposs.github.io/" target="_blank">
                                <b>ACL Workshop for Natural Language Processing Open Source Software (NLP-OSS)</b></a>, 2018.
                            <br/>
                            [<a href="papers/allennlp.nlposs2018.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#allennlp_nlposs2018_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://github.com/allenai/allennlp" target="_blank">code</a>]
                            <div id="allennlp_nlposs2018_abstract" class="abstract" style="display:none;">
                                <p>
                                    This paper describes AllenNLP, a platform
                                    for research on deep learning methods in
                                    natural language understanding. AllenNLP is
                                    designed to support researchers who want to
                                    build novel language understanding models
                                    quickly and easily. It is built on top of
                                    PyTorch, allowing for dynamic computation
                                    graphs, and provides (1) a flexible data API
                                    that handles intelligent batching and
                                    padding, (2) high-level abstractions for
                                    common operations in working with text, and
                                    (3) a modular and extensible experiment
                                    framework that makes doing good science
                                    easy. It also includes reference
                                    implementations of high quality approaches
                                    for both core semantic problems (e.g.
                                    semantic role labeling (Palmer et al.,
                                    2005)) and language understanding
                                    applications (e.g. machine comprehension
                                    (Rajpurkar et al., 2016)). AllenNLP is an
                                    ongoing open-source effort maintained by
                                    engineers and researchers at the Allen
                                    Institute for Artificial Intelligence.
                                </p>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <a href="papers/liu+levow+smith.sclem2018.pdf" target="_blank">
                                <b>Discovering Phonesthemes with Sparse Regularization</b>
                            </a>
                            <br/>
                            <b>Nelson F. Liu</b>,
                            <a href="https://faculty.washington.edu/levow/" target="_blank">Gina-Anne Levow</a>,
                            and <a href="https://homes.cs.washington.edu/~nasmith/" target="_blank">Noah A. Smith</a>.
                            <br/>
                            In <a href="https://sites.google.com/view/sclem2018/" target="_blank">
                                <b>NAACL Workshop on Subword and Character Level Models in NLP (SCLeM)</b></a>, 2018.
                            <br/>
                            [<a href="papers/liu+levow+smith.sclem2018.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#liu_levow_smith_sclem2018_abstract').toggle();return false;">abstract</a>]
                            [<a href="papers/liu+levow+smith.sclem2018.poster.pdf" target="_blank">poster</a>]
                            [<a href="papers/phonesthemes" target="_blank">code</a>]
                            <div id="liu_levow_smith_sclem2018_abstract" class="abstract" style="display:none;">
                                <p>
                                    We introduce a simple method for extracting
                                    non-arbitrary form-meaning representations
                                    from a collection of semantic vectors. We
                                    treat the problem as one of feature
                                    selection for a model trained to predict
                                    word vectors from subword features. We apply
                                    this model to the problem of automatically
                                    discovering phonesthemes, which are
                                    submorphemic sound clusters that appear in
                                    words with similar meaning. Many of our
                                    model-predicted phonesthemes overlap with
                                    those proposed in the linguistics
                                    literature, and we validate our approach
                                    with human judgments.
                                </p>
                            </div>
                        </li>
                    </ul>
                    <h3>2017</h3>
                    <ul class="pl">
                        <li>
                            <a href="papers/welbl+liu+gardner.wnut2017.pdf" target="_blank">
                                <b>Crowdsourcing Multiple Choice Science Questions</b>
                            </a>
                            <br/>
                            <a href="https://jowel.gitlab.io/welbl/" target="_blank">Johannes Welbl</a>,
                            <b>Nelson F. Liu</b>,
                            and <a href="https://matt-gardner.github.io/" target="_blank">Matt Gardner</a>.
                            <br/>
                            In <a href="http://noisy-text.github.io/2017" target="_blank">
                                <b>EMNLP Workshop on Noisy User-generated Text</b></a>, 2017.
                            <br/>
                            [<a href="papers/welbl+liu+gardner.wnut2017.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#welbl_liu_gardner_wnut2017_abstract').toggle();return false;">abstract</a>]
                            [<a href="http://data.allenai.org/sciq/" target="_blank">data</a>]
                            [<a href="papers/sciq/welbl+liu+gardner.wnut2017.poster.pdf" target="_blank">poster</a>]
                            <div id="welbl_liu_gardner_wnut2017_abstract" class="abstract" style="display:none;">
                                <p>
                                    We present a novel method for obtaining
                                    high-quality, domain-targeted multiple
                                    choice questions from crowd workers.
                                    Generating these questions can be difficult
                                    without trading away originality, relevance
                                    or diversity in the answer options. Our
                                    method addresses these problems by
                                    leveraging a large corpus of domain-specific
                                    text and a small set of existing questions.
                                    It produces model suggestions for document
                                    selection and answer distractor choice which
                                    aid the human question generation process.
                                    With this method we have assembled SciQ, a
                                    dataset of 13.7K multiple choice science
                                    exam questions (Dataset available
                                    at <a href="http://data.allenai.org/sciq/"
                                    target="_blank">data.allenai.org/sciq/</a>).
                                    We demonstrate that the method produces
                                    in-domain questions by providing an analysis
                                    of this new dataset and by showing that
                                    humans cannot distinguish the crowdsourced
                                    questions from original questions. When
                                    using SciQ as additional training data to
                                    existing questions, we observe accuracy
                                    improvements on real science exams.
                                </p>
                            </div>
                        </li>
                    </ul>
                </div>
            </div>
            <hr>
            <div class="row">
                <div class="col">
                    <!-- <h2>Miscellany</h2> -->
                    <ul>
                        <li>
                            This website is built from the <a href="https://github.com/nelson-liu/website">source code</a> of Nelson F. Liu's nice website (<a href="https://nelsonliu.me/"
                            target="_blank">https://nelsonliu.me</a>).
                        </li>
                    </ul>
                </div>
            </div>
            <footer class="pt-2 my-md-2 pt-md-2 border-top">
                <div class="row justify-content-center">
                    <div class="col-6 col-md text-left align-self-center">
                        <p class="h5 text-muted">
                            © Yung-Sung Chuang, 2021
                        </p>
                    </div>
                    <div class="col-6 col-md text-right">
                        <a href="https://www.csail.mit.edu/" class="image-link">
                            <img class="mr-4" src="img/mit_csail_logo.svg" alt="MIT CSAIL logo." height="75">
                        </a>
                        <a href="https://www.mit.edu/" class="image-link">
                            <img src="img/mit_logo.svg" alt="MIT logo." height="50">
                        </a>
                    </div>
                </div>
            </footer>
        </div>
    </body>
</html>
