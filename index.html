<!DOCTYPE html>
<html lang="en">
    <head>
        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-116076474-3"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-116076474-3');
        </script>
        <title>Yung-Sung Chuang</title>
        <meta http-equiv="content-type" content="text/html; charset=UTF-8">
        <meta charset="utf-8">
        <meta property="og:url" content="https://people.csail.mit.edu/yungsung/" />
	    <meta property="og:title" content="Yung-Sung Chuang" />
	    <meta property="og:image" content="img/Yung-Sung.jpg" />
	    <meta http-equiv="X-UA-Compatible" content="IE=edge">
	    <meta name="author" content="Yung-Sung Chuang">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
        <link rel="shortcut icon" type="image/png" href="favicon.ico"/>

        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
        <link rel="stylesheet" href="css/style.css">
        <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
        <link href='https://fonts.googleapis.com/css?family=EB+Garamond' rel='stylesheet' type='text/css'>
        <style>
            /* Add some padding on document's body to prevent the content
            to go underneath the header and footer */
            body{        
                padding-top: 50px;
                padding-bottom: 0px;
            }
            html {
                scroll-padding-top: 70px; /* height of sticky header */
            }
            .container{
                width: 80%;
                margin: 0 auto; /* Center the DIV horizontally */
            }
            .fixed-header, .fixed-footer{
                width: 100%;
                position: fixed;        
                background: rgb(69, 142, 226);
                padding: 7px 0;
                color: #fff;
                font-size: calc(6px + 0.78125vw);
            }
            .fixed-header{
                top: 0;
            }
            .fixed-footer{
                bottom: 0;
            }    
            /* Some more styles to beutify this example (85, 129, 212) */
            nav a{
                color: #fff;
                text-decoration: none;
                padding: 7px 1.7vw;
                display: inline-block;
            }
            h1 span {
                font-size: 20pt;
            }
            h1 span a{
                font-size: 12pt;
            }
        </style>
    </head>
    <body>
        <div class="fixed-header" style="z-index:1;">
            <div class="container">
                <nav>
                    <a href="#">Home</a>
                    <a href=#publications>Publications</a>
                    <a href=#talks>Talks</a>
                    <a href=#projects>Projects</a>
                    <a href=#honors>Honors</a>
                    <!-- <a href=#services>Services</a> -->
                    <a href="https://voidism.github.io/">Blog</a>
                </nav>
            </div>
        </div>
        <div class="container mt-5" style="z-index:0;">
            <div class="row mb-3">
                <div class="col">
                    <h1>Yung-Sung Chuang <span>(莊永松) <a href="#" onclick="$('#my_name').toggle();return false;"><i>How to pronounce?</i></a></span></h1>
                </div>
            </div>
            <div id="my_name" class="abstract" style="display:none;">
                <p style="color:gray">
                    My first name Yung-Sung (永松) should be pronounced as "yǒng sōng". My last name Chuang (莊) should be pronounced as "juāng". In my first name, Yung means "forever" and Sung means "pine tree", so it has the meaning of "long-lasting" or "longevity".
                    </br>
                    <audio controls>
                        <source src="files/name-tts.mp3" type="audio/mpeg">
                    </audio>
                </p>
            </div>
            <div class="row">
                <div class="col-lg-4 col-md-6 order-0 order-xs-0 order-sm-0 order-md-1 order-lg-1">
                    <div class="card mb-3">
                        <img class="card-img-top" src="img/Yung-Sung.jpg" alt="Yung-Sung Chuang">
                        <div class="card-body">
                            <h5 class="card-title">
                                <b>Yung-Sung Chuang</b>
                            </h5>
                            <p class="card-text">
                                MIT EECS PhD Student
                                </br>
                                Office: 32-G436
                            </p>
                        </div>
                    </div>
                </div>
                <div class="col-lg-8 col-md-6 order-1 order-xs-1 order-sm-1 order-md-0 order-lg-0">
                    <!-- <p>▶ [<a href=#publications>Publications</a>] [<a href=#talks>Talks</a>] [<a href=#projects>Projects</a>] [<a href=#honors>Honors</a>] [<a href=#services>Services</a>] </p> -->
                    <p>
                        Hi! I'm a first-year PhD student in <a href="https://www.eecs.mit.edu/" target="_blank">Electrical Engineering and Computer Science</a> at
                        <a href="https://www.mit.edu" target="_blank">Massachusetts Institute of Technology</a>, where I work with <a href="https://www.csail.mit.edu/person/jim-glass"
                        target="_blank">Jim Glass</a>. 
                    </p>
                    <p>
                        My research interest broadly covers 
                        the deep learning technique for natural language processing and 
                        speech processing. In particular, I aim to utilize the ability of 
                        machines to help people grasp large information in text/audio form 
                        in efficient ways.
                    </p>
                    <p>
                        Previously, I was an undergraduate student in Electrical Engineering
                        at National Taiwan University. I joined Speech Processing Lab 
                        supervised by <a href="https://speech.ee.ntu.edu.tw/~hylee/index.html">Hung-Yi Lee</a> and <a href="https://speech.ee.ntu.edu.tw/previous_version/lslNew.htm">Lin-shan Lee</a>, and Machine Intelligence Understanding Lab 
                        supervised by <a href="https://www.csie.ntu.edu.tw/~yvchen/">Yun-Nung (Vivian) Chen</a>. I received the NTU Presidential 
                        Award for top 5% students four times in 2018-2020, <a href="https://irvingthofoundation.github.io/ho-fellows.htm">Irving T. Ho Memorial 
                        Scholarship</a> in 2018 and 2019. Here is my <a href="https://people.csail.mit.edu/yungsung/files/CV.pdf">Curriculum Vitae</a>.
                    </p>
                </div>
            </div>
            <div class="row">
                <div class="col">
                    <p>
                        Email: yungsung [AT] mit.edu
                    </p>
                    <p>
                        Links:
                        [<a href="files/CV.pdf" target="_blank">CV</a>] [<a href="https://twitter.com/YungSungChuang" target="_blank">Twitter</a>] [<a href="https://github.com/voidism" target="_blank">Github</a>] [<a href="https://scholar.google.com/citations?hl=en&user=3ar1DOwAAAAJ" target="_blank">Google Scholar</a>] [<a href="https://dblp.org/pers/hd/c/Chuang:Yung=Sung" target="_blank">DBLP</a>] [<a href="https://voidism.github.io/" target="_blank">Blog</a>] [<a href="https://www.linkedin.com/in/yschuang">Linkedin</a>] [<a href="https://www.instagram.com/yungsung.chuang/">Instagram</a>]
                    </p>
                </div>
            </div>
            <div class="row">
                <div class="col">
                    <h2>Recent News</h2>
                    <ul>
                        <li>
                            (04/2022) Our paper <a href="https://arxiv.org/abs/2204.10298" target="_blank">"DiffCSE: Difference-based Contrastive Learning for Sentence Embeddings"</a> is accepted by NAACL 2022 as a oral paper! <br/>
                        </li>
                        <li>
                            (03/2022) I am serving as a reviewer for <a href="https://nips.cc/Conferences/2022" target="_blank">NeurIPS 2022</a>. <br/>
                        </li>
                        <li>
                            (03/2022) I am serving on the Program Committee for <a href="https://www.workshopononlineabuse.com/" target="_blank">The Sixth Workshop on Online Abuse and Harms in NAACL 2022</a>. <br/>
                        </li>
                        <li>
                            (01/2022) I am serving as a reviewer for <a href="https://icml.cc/Conferences/2022" target="_blank">ICML 2022</a>. <br/>
                        </li>
                        <li>
                            (09/2021) Our paper <a href="https://arxiv.org/abs/2106.05933" target="_blank">"PARP: Prune, Adjust and Re-Prune for Self-Supervised Speech Recognition"</a> is accepted by NeurIPS 2021 as a spotlight paper! <br/>
                        </li>
                        <li>
                            (09/2021) Start my PhD life in Cambridge!
                        </li>
                        <li>
                            (06/2021) I am serving as a reviewer for <a href="https://iclr.cc/Conferences/2022" target="_blank">ICLR 2022</a>. <br/>
                        </li>
                        <li>
                            (04/2021) I am serving as a reviewer for <a href="https://nips.cc/Conferences/2021" target="_blank">NeurIPS 2021</a>. <br/>
                        </li>
                    </ul>
                </div>
            </div>
            <hr>
            <div class="row" id="publications">
                <div class="col">
                    <h2>Publications</h2>
                    <h3>2022</h3>
                    <ul class="pl">
                        <li>
                            <a href="https://arxiv.org/abs/2204.10298" target="_blank">
                                <b>DiffCSE: Difference-based Contrastive Learning for Sentence Embeddings</b>
                            </a>
                            <br/>
                            <b>Yung-Sung Chuang</b>, 
                            <a href="http://super-ms.mit.edu/rumen.html" target="_blank">Rumen Dangovski</a>, 
                            <a href="http://people.csail.mit.edu/hyluo/" target="_blank">Hongyin Luo</a>, 
                            <a href="https://mitibmwatsonailab.mit.edu/people/yang-zhang/" target="_blank">Yang Zhang</a>, 
                            <a href="https://code-terminator.github.io/" target="_blank">Shiyu Chang</a>, 
                            <a href="http://www.mit.edu/~soljacic/marin.html" target="_blank">Marin Soljačić</a>, 
                            <a href="https://swdanielli.github.io/" target="_blank">Shang-Wen Li</a>, 
                            <a href="https://scottyih.org/" target="_blank">Scott Wen-tau Yih</a>, 
                            <a href="https://people.csail.mit.edu/yoonkim/" target="_blank">Yoon Kim</a>, 
                            <a href="https://www.csail.mit.edu/person/jim-glass" target="_blank">James Glass</a>.
                            <br/>
                            In <a href="https://2022.naacl.org/" target="_blank">
                                <b>
                                  Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</b></a>, 2022. <a style="color:#e22222"> (Oral Paper) </a>
                            <br/>
                            [<a href="bibs/chuang2022diffcse.bib" target="_blank" type="text/plain">bib</a>]
                            [<a href="#" onclick="$('#chuang2022diffcse_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://arxiv.org/pdf/2204.10298.pdf" target="_blank">pdf</a>]
                            [<a href="https://github.com/voidism/DiffCSE" target="_blank">code</a>]
                            <!-- [<a href="https://www.youtube.com/watch?v=zd9fiVvej0k" target="_blank">video</a>] -->
                            <!-- [<a href="https://superbbenchmark.org/" target="_blank">leaderboard</a>] -->
                            <div id="chuang2022diffcse_abstract" class="abstract" style="display:none;">
                                <p>
                                    We propose DiffCSE, an unsupervised contrastive learning framework for learning sentence embeddings. DiffCSE learns sentence embeddings that are sensitive to the difference between the original sentence and an edited sentence, where the edited sentence is obtained by stochastically masking out the original sentence and then sampling from a masked language model. We show that DiffSCE is an instance of equivariant contrastive learning (Dangovski et al., 2021), which generalizes contrastive learning and learns representations that are insensitive to certain types of augmentations and sensitive to other "harmful" types of augmentations. Our experiments show that DiffCSE achieves state-of-the-art results among unsupervised sentence representation learning methods, outperforming unsupervised SimCSE by 2.3 absolute points on semantic textual similarity tasks.
                                </p>
                            </div>
                        </li>
                    </ul>
                    <ul class="pl">
                        <li>
                            <a href="https://arxiv.org/abs/2203.04911" target="_blank">
                                <b>DUAL: Discrete Spoken Unit Adaptive Learning for Textless Spoken Question Answering</b>
                            </a>
                            <br/>
                            <a href="https://daniellin94144.github.io/" target="_blank">Guan-Ting Lin</a>, 
                            <b>Yung-Sung Chuang</b>, 
                            <a href="https://github.com/voidful" target="_blank">Ho-Lam Chung</a>, 
                            <a href="https://scholar.google.com.tw/citations?user=R1mNI8QAAAAJ" target="_blank">Shu-wen Yang</a>, 
                            <a href="#">Hsuan-Jui Chen</a>, 
                            <a href="https://www.amazon.science/author/shuyan-dong" target="_blank">Shuyan Dong</a>, 
                            <a href="https://swdanielli.github.io/" target="_blank">Shang-Wen Li</a>, 
                            <a href="http://www.cs.toronto.edu/~asamir/" target="_blank">Abdelrahman Mohamed</a>, 
                            <a href="https://speech.ee.ntu.edu.tw/~hylee/index.html" target="_blank">Hung-yi Lee</a>, 
                            <a href="https://speech.ee.ntu.edu.tw/previous_version/lslNew.htm" target="_blank">Lin-shan Lee</a>.
                            <br/>
                                <b>
                                    Submitted to Interspeech 2022</b>.
                            <br/>
                            [<a href="bibs/lin2022dual.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#lin2022dual_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://arxiv.org/pdf/2203.04911.pdf" target="_blank">pdf</a>]
                            [<a href="https://github.com/daniellin94144/dual-textless-sqa" target="_blank">code</a>]
                            <!-- [<a href="https://www.youtube.com/watch?v=zd9fiVvej0k" target="_blank">video</a>] -->
                            <!-- [<a href="https://superbbenchmark.org/" target="_blank">leaderboard</a>] -->
                            <div id="lin2022dual_abstract" class="abstract" style="display:none;">
                                <p>
                                    Spoken Question Answering (SQA) is to find the answer from a spoken document given a question, which is crucial for personal assistants when replying to the queries from the users. Existing SQA methods all rely on Automatic Speech Recognition (ASR) transcripts. Not only does ASR need to be trained with massive annotated data that are time and cost-prohibitive to collect for low-resourced languages, but more importantly, very often the answers to the questions include name entities or out-of-vocabulary words that cannot be recognized correctly. Also, ASR aims to minimize recognition errors equally over all words, including many function words irrelevant to the SQA task. Therefore, SQA without ASR transcripts (textless) is always highly desired, although known to be very difficult. This work proposes Discrete Spoken Unit Adaptive Learning (DUAL), leveraging unlabeled data for pre-training and fine-tuned by the SQA downstream task. The time intervals of spoken answers can be directly predicted from spoken documents. We also release a new SQA benchmark corpus, NMSQA, for data with more realistic scenarios. We empirically showed that DUAL yields results comparable to those obtained by cascading ASR and text QA model and robust to real-world data. Our code and model will be open-sourced.
                                </p>
                            </div>
                        </li>
                    </ul>
                    <h3>2021</h3>
                    <ul class="pl">
                        <li>
                            <a href="https://arxiv.org/abs/2106.05933" target="_blank">
                                <b>PARP: Prune, Adjust and Re-Prune for Self-Supervised Speech Recognition</b>
                            </a>
                            <br/>
                            <a href="http://people.csail.mit.edu/clai24/" target="_blank">Cheng-I Jeff Lai</a>, 
                            <a href="https://mitibmwatsonailab.mit.edu/people/yang-zhang/" target="_blank">Yang Zhang</a>, <a href="https://alexander-h-liu.github.io/" target="_blank">Alexander H. Liu</a>, <a href="https://code-terminator.github.io/" target="_blank">Shiyu Chang</a>, <a href="https://tw.linkedin.com/in/yilunliao" target="_blank">Yi-Lun Liao</a>, 
                            <b>Yung-Sung Chuang</b>, 
                            <a href="https://scholar.google.com/citations?user=uEpr4C4AAAAJ&hl=en" target="_blank">Kaizhi Qian</a>, <a href="http://people.csail.mit.edu/sameerk/" target="_blank">Sameer Khurana</a>, <a href="https://mitibmwatsonailab.mit.edu/people/david-cox/" target="_blank">David Cox</a>, <a href="https://www.csail.mit.edu/person/jim-glass" target="_blank">James Glass</a>.
                            <br/>
                            In <a href="https://proceedings.neurips.cc/paper/2021/hash/b17c0907e67d868b4e0feb43dbbe6f11-Abstract.html" target="_blank">
                                <b>
                                    Thirty-fifth Conference on Neural Information Processing Systems (NeurIPS)</b></a>, 2021. <a style="color:#e22222">(Spotlight Paper)</a>
                            <br/>
                            [<a href="bibs/lai2021parp.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#lai2021parp_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://proceedings.neurips.cc/paper/2021/file/b17c0907e67d868b4e0feb43dbbe6f11-Paper.pdf" target="_blank">pdf</a>]
                            <!-- [<a href="https://github.com/s3prl/s3prl" target="_blank">code</a>] -->
                            <!-- [<a href="https://www.youtube.com/watch?v=zd9fiVvej0k" target="_blank">video</a>] -->
                            <!-- [<a href="https://superbbenchmark.org/" target="_blank">leaderboard</a>] -->
                            <div id="lai2021parp_abstract" class="abstract" style="display:none;">
                                <p>
                                    Recent work on speech self-supervised learning (speech SSL) demonstrated the benefits of scale in learning rich and transferable representations for Automatic Speech Recognition (ASR) with limited parallel data. It is then natural to investigate the existence of sparse and transferrable subnetworks in pre-trained speech SSL models that can achieve even better low-resource ASR performance. However, directly applying widely adopted pruning methods such as the Lottery Ticket Hypothesis (LTH) is suboptimal in the computational cost needed. Moreover, contrary to what LTH predicts, the discovered subnetworks yield minimal performance gain compared to the original dense network. In this work, we propose Prune-Adjust- Re-Prune (PARP), which discovers and finetunes subnetworks for much better ASR performance, while only requiring a single downstream finetuning run. PARP is inspired by our surprising observation that subnetworks pruned for pre-training tasks only needed to be slightly adjusted to achieve a sizeable performance boost in downstream ASR tasks. Extensive experiments on low-resource English and multi-lingual ASR show (1) sparse subnetworks exist in pre-trained speech SSL, and (2) the computational advantage and performance gain of PARP over baseline pruning methods. On the 10min Librispeech split without LM decoding, PARP discovers subnetworks from wav2vec 2.0 with an absolute 10.9%/12.6% WER decrease compared to the full model. We demonstrate PARP mitigates performance degradation in cross-lingual mask transfer, and investigate the possibility of discovering a single subnetwork for 10 spoken languages in one run.
                                </p>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <a href="https://arxiv.org/abs/2105.01051" target="_blank">
                                <b>SUPERB: Speech processing Universal PERformance Benchmark</b>
                            </a>
                            <br/>
                            <a href="https://scholar.google.com.tw/citations?user=R1mNI8QAAAAJ" target="_blank">Shu-wen Yang</a>, <a href="https://scholar.google.com/citations?user=SiyicoEAAAAJ&hl=zh-TW" target="_blank">Po-Han Chi*</a>, <b>Yung-Sung Chuang*</b>, 
                            <a href="http://people.csail.mit.edu/clai24/" target="_blank">Cheng-I Jeff Lai*</a>, 
                            <a href="https://ai.facebook.com/people/kushal-lakhotia/" target="_blank">Kushal Lakhotia*</a>, <a href="https://scholar.google.com/citations?user=0lrZq9MAAAAJ&hl=en" target="_blank">Yist Y. Lin*</a>, 
                            <a href="https://andi611.github.io/" target="_blank">Andy T. Liu*</a>, <a href="http://shijt.site/" target="_blank">Jiatong Shi*</a>, <a href="https://www.lti.cs.cmu.edu/people/222227473/xuankai-chang" target="_blank">Xuankai Chang</a>, <a href="https://github.com/DanielLin94144" target="_blank">Guan-Ting Lin</a>, 
                            <a href="https://tw.linkedin.com/in/tzu-hsien-huang-1686651b6" target="_blank">Tzu-Hsien Huang</a>, Wei-Cheng Tseng, <a href="https://tw.linkedin.com/in/ko-tik-lee-4747291a2/en?trk=people-guest_people_search-card" target="_blank">Ko-tik Lee</a>, <a href="https://scholar.google.com.tw/citations?user=qJ5zXNIAAAAJ" target="_blank">Da-Rong Liu</a>, 
                            <a href="https://dblp.org/pid/210/0905.html" target="_blank">Zili Huang</a>, <a href="https://www.amazon.science/author/shuyan-dong" target="_blank">Shuyan Dong</a>, <a href="https://swdanielli.github.io/" target="_blank">Shang-Wen Li</a>, 
                            <a href="https://sites.google.com/view/shinjiwatanabe" target="_blank">Shinji Watanabe</a>, 
                            <a href="http://www.cs.toronto.edu/~asamir/" target="_blank">Abdelrahman Mohamed</a>, <a href="https://speech.ee.ntu.edu.tw/~hylee/index.html" target="_blank">Hung-Yi Lee</a>.
                            <br/>
                            In <a href="https://www.interspeech2021.org/" target="_blank">
                                <b>
                                    Interspeech</b></a>, 2021.
                            <br/>
                            [<a href="bibs/yang2021superb.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#yang2020superb_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://arxiv.org/pdf/2105.01051.pdf" target="_blank">pdf</a>]
                            [<a href="https://github.com/s3prl/s3prl" target="_blank">code</a>]
                            [<a href="https://www.youtube.com/watch?v=zd9fiVvej0k" target="_blank">video</a>]
                            [<a href="https://superbbenchmark.org/" target="_blank">leaderboard</a>]
                            <div id="yang2020superb_abstract" class="abstract" style="display:none;">
                                <p>
                                    Self-supervised learning (SSL) has proven vital for advancing research in natural language processing (NLP) and computer vision (CV). The paradigm pretrains a shared model on large volumes of unlabeled data and achieves state-of-the-art (SOTA) for various tasks with minimal adaptation. However, the speech processing community lacks a similar setup to systematically explore the paradigm. To bridge this gap, we introduce Speech processing Universal PERformance Benchmark (SUPERB). SUPERB is a leaderboard to benchmark the performance of a shared model across a wide range of speech processing tasks with minimal architecture changes and labeled data. Among multiple usages of the shared model, we especially focus on extracting the representation learned from SSL due to its preferable re-usability. We present a simple framework to solve SUPERB tasks by learning task-specialized lightweight prediction heads on top of the frozen shared model. Our results demonstrate that the framework is promising as SSL representations show competitive generalizability and accessibility across SUPERB tasks. We release SUPERB as a challenge with a leaderboard and a benchmark toolkit to fuel the research in representation learning and general speech processing.
                                </p>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <a href="https://arxiv.org/abs/2105.04840" target="_blank">
                                <b>Investigating the Reordering Capability in CTC-based Non-Autoregressive End-to-End Speech Translation</b>
                            </a>
                            <br/>
                            <a href="https://scholar.google.com.hk/citations?user=CYDgtRoAAAAJ" target="_blank">Shun-Po Chuang*</a>, <b>Yung-Sung Chuang*</b>, <a href="https://aclanthology.org/people/c/chih-chiang-chang/" target="_blank">Chih-Chiang Chang*</a>,
                            <a href="https://speech.ee.ntu.edu.tw/~hylee/index.html" target="_blank">Hung-Yi Lee</a>.
                            <br/>
                            In <a href="https://aclanthology.org/volumes/2021.findings-acl/" target="_blank">
                                <b>
                                    The Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Findings</b></a>, 2021.
                            <br/>
                            [<a href="bibs/chuang2021investigating.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#chuang2021investigating_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://aclanthology.org/2021.findings-acl.92.pdf" target="_blank">pdf</a>]
                            [<a href="https://github.com/voidism/NAR-ST" target="_blank">code</a>]
                            [<a href="https://www.youtube.com/watch?v=I80xwWfswl8" target="_blank">video</a>]
                            <!-- [<a href="https://leaderboard.allenai.org/quoref" target="_blank">leaderboard</a>] -->
                            <div id="chuang2021investigating_abstract" class="abstract" style="display:none;">
                                <p>
                                    We study the possibilities of building a non-autoregressive speech-to-text translation model using connectionist temporal classification (CTC), and use CTC-based automatic speech recognition as an auxiliary task to improve the performance. CTC's success on translation is counter-intuitive due to its monotonicity assumption, so we analyze its reordering capability. Kendall's tau distance is introduced as the quantitative metric, and gradient-based visualization provides an intuitive way to take a closer look into the model. Our analysis shows that transformer encoders have the ability to change the word order and points out the future research direction that worth being explored more on non-autoregressive speech translation.
                                </p>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <a href="https://arxiv.org/abs/2106.07240" target="_blank">
                                <b>Mitigating Biases in Toxic Language Detection through Invariant Rationalization</b>
                            </a>
                            <br/>
                            <b>Yung-Sung Chuang</b>, <a href="https://www.linkedin.com/in/mingye94" target="_blank">Mingye Gao</a>,
                            <a href="http://people.csail.mit.edu/hyluo/" target="_blank">Hongyin Luo</a>,
                            <a href="https://www.csail.mit.edu/person/jim-glass" target="_blank">James Glass</a>,
                            <a href="https://speech.ee.ntu.edu.tw/~hylee/index.html" target="_blank">Hung-Yi Lee</a>,
                            <a href="https://www.csie.ntu.edu.tw/~yvchen/" target="_blank">Yun-Nung Chen</a>,
                            <a href="https://swdanielli.github.io/" target="_blank">Shang-Wen Li</a>.
                            <br/>
                            In <a href="https://aclanthology.org/events/woah-2021/" target="_blank">
                                <b>
                                    The 5th Workshop on Online Abuse and Harms at The Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing
                                    (WOAH@ACL-IJCNLP)</b></a>, 2021.
                            <br/>
                            [<a href="bibs/chuang2021mitigating.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#chuang2021mitigating_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://aclanthology.org/2021.woah-1.12.pdf" target="_blank">pdf</a>]
                            [<a href="https://github.com/voidism/invrat_debias" target="_blank">code</a>]
                            [<a href="https://www.youtube.com/watch?v=bAX8oL_ywpQ" target="_blank">video</a>]
                            <!-- [<a href="https://leaderboard.allenai.org/quoref" target="_blank">leaderboard</a>] -->
                            <div id="chuang2021mitigating_abstract" class="abstract" style="display:none;">
                                <p>
                                    Automatic detection of toxic language plays an essential role in protecting social media users, especially minority groups, from verbal abuse. However, biases toward some attributes, including gender, race, and dialect, exist in most training datasets for toxicity detection. The biases make the learned models unfair and can even exacerbate the marginalization of people. Considering that current debiasing methods for general natural language understanding tasks cannot effectively mitigate the biases in the toxicity detectors, we propose to use invariant rationalization (InvRat), a game-theoretic framework consisting of a rationale generator and a predictor, to rule out the spurious correlation of certain syntactic patterns (e.g., identity mentions, dialect) to toxicity labels. We empirically show that our method yields lower false positive rate in both lexical and dialectal attributes than previous debiasing methods.
                                </p>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <a href="https://arxiv.org/abs/2010.13826" target="_blank">
                                <b>Semi-Supervised Spoken Language Understanding via Self-Supervised Speech and Language Model Pretraining</b>
                            </a>
                            <br/>
                            <a href="http://people.csail.mit.edu/clai24/" target="_blank">Cheng-I Lai</a>,
                            <b>Yung-Sung Chuang</b>,
                            <a href="https://speech.ee.ntu.edu.tw/~hylee/index.html" target="_blank">Hung-Yi Lee</a>,
                            <a href="https://swdanielli.github.io/" target="_blank">Shang-Wen Li</a>,
                            <a href="https://www.csail.mit.edu/person/jim-glass" target="_blank">James Glass</a>.
                            <br/>
                            In <a href="https://2021.ieeeicassp.org/" target="_blank">
                                <b>
                                    IEEE International Conference on Acoustics, Speech and Signal Processing
                                    (ICASSP)</b></a>, 2021.
                            <br/>
                            [<a href="bibs/lai2020semi.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#lai2020semi_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://arxiv.org/pdf/2010.13826.pdf" target="_blank">pdf</a>]
                            [<a href="https://github.com/jefflai108/Semi-Supervsied-Spoken-Language-Understanding-PyTorch" target="_blank">code</a>]
                            [<a href="https://www.youtube.com/watch?v=5GOIPp9hWkY" target="_blank">video</a>]
                            <!-- [<a href="https://leaderboard.allenai.org/quoref" target="_blank">leaderboard</a>] -->
                            <div id="lai2020semi_abstract" class="abstract" style="display:none;">
                                <p>
                                    Much recent work on Spoken Language Understanding (SLU) is limited in at least one of three ways: models were trained on oracle text input and neglected ASR errors, models were trained to predict only intents without the slot values, or models were trained on a large amount of in-house data. In this paper, we propose a clean and general framework to learn semantics directly from speech with semi-supervision from transcribed or untranscribed speech to address these issues. Our framework is built upon pretrained end-to-end (E2E) ASR and self-supervised language models, such as BERT, and fine-tuned on a limited amount of target SLU data. We study two semi-supervised settings for the ASR component: supervised pretraining on transcribed speech, and unsupervised pretraining by replacing the ASR encoder with self-supervised speech representations, such as wav2vec. In parallel, we identify two essential criteria for evaluating SLU models: environmental noise-robustness and E2E semantics evaluation. Experiments on ATIS show that our SLU framework with speech as input can perform on par with those using oracle text as input in semantics understanding, even though environmental noise is present and a limited amount of labeled semantics data is available for training.
                                </p>
                            </div>
                        </li>
                    </ul>
                    <h3>2020</h3>
                    <ul class="pl">
                        <li>
                            <a href="https://arxiv.org/abs/2010.02123" target="_blank">
                                <b>Lifelong Language Knowledge Distillation</b>
                            </a>
                            <br/>
                            <b>Yung-Sung Chuang</b>, <a href="https://www.shangyusu.com/" target="_blank">Shang-Yu Su</a>,
                            <a href="https://www.csie.ntu.edu.tw/~yvchen/" target="_blank">Yun-Nung Chen</a>.
                            <br/>
                            In <a href="https://2020.emnlp.org/" target="_blank">
                                <b>
                                    The Conference on Empirical Methods in Natural Language Processing (EMNLP)</b></a>, 2020.
                            <br/>
                            [<a href="bibs/chuang2020lifelong.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#chuang2020lifelong_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://aclanthology.org/2020.emnlp-main.233.pdf" target="_blank">pdf</a>]
                            [<a href="https://github.com/voidism/L2KD" target="_blank">code</a>]
                            [<a href="https://slideslive.com/38938863/lifelong-language-knowledge-distillation" target="_blank">video</a>]
                            <!-- [<a href="https://leaderboard.allenai.org/quoref" target="_blank">leaderboard</a>] -->
                            <div id="chuang2020lifelong_abstract" class="abstract" style="display:none;">
                                <p>
                                    It is challenging to perform lifelong language learning (LLL) on a stream of different tasks without any performance degradation comparing to the multi-task counterparts. To address this issue, we present Lifelong Language Knowledge Distillation (L2KD), a simple but efficient method that can be easily applied to existing LLL architectures in order to mitigate the degradation. Specifically, when the LLL model is trained on a new task, we assign a teacher model to first learn the new task, and pass the knowledge to the LLL model via knowledge distillation. Therefore, the LLL model can better adapt to the new task while keeping the previously learned knowledge. Experiments show that the proposed L2KD consistently improves previous state-of-the-art models, and the degradation comparing to multi-task models in LLL tasks is well mitigated for both sequence generation and text classification tasks.
                                </p>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <a href="https://arxiv.org/abs/2010.04246" target="_blank">
                                <b>Dual Inference for Improving Language Understanding and Generation</b>
                            </a>
                            <br/>
                            <b>Yung-Sung Chuang*</b>, <a href="https://www.shangyusu.com/" target="_blank">Shang-Yu Su*</a>,
                            <a href="https://www.csie.ntu.edu.tw/~yvchen/" target="_blank">Yun-Nung Chen</a>.
                            <br/>
                            In <a href="https://2020.emnlp.org/" target="_blank">
                                <b>
                                    The Conference on Empirical Methods in Natural Language Processing: Findings (EMNLP)</b></a>, 2020.
                            <br/>
                            [<a href="bibs/su2020dual.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#su2020dual_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://aclanthology.org/2020.findings-emnlp.443.pdf" target="_blank">pdf</a>]
                            [<a href="https://github.com/MiuLab/DuaLUG" target="_blank">code</a>]
                            <!-- [<a href="https://slideslive.com/38938863/lifelong-language-knowledge-distillation" target="_blank">video</a>] -->
                            <!-- [<a href="https://leaderboard.allenai.org/quoref" target="_blank">leaderboard</a>] -->
                            <div id="su2020dual_abstract" class="abstract" style="display:none;">
                                <p>
                                    Natural language understanding (NLU) and Natural language generation (NLG) tasks hold a strong dual relationship, where NLU aims at predicting semantic labels based on natural language utterances and NLG does the opposite. The prior work mainly focused on exploiting the duality in model training in order to obtain the models with better performance. However, regarding the fast-growing scale of models in the current NLP area, sometimes we may have difficulty retraining whole NLU and NLG models. To better address the issue, this paper proposes to leverage the duality in the inference stage without the need of retraining. The experiments on three benchmark datasets demonstrate the effectiveness of the proposed method in both NLU and NLG, providing the great potential of practical usage.
                                </p>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <a href="https://arxiv.org/abs/1910.11559" target="_blank">
                                <b>SpeechBERT: An Audio-and-text Jointly Learned Language Model for End-to-end Spoken Question Answering</b>
                            </a>
                            <br/>
                            <b>Yung-Sung Chuang</b>, <a href="https://liangtaiwan.github.io/" target="_blank">Chi-Liang Liu</a>,
                            <a href="https://speech.ee.ntu.edu.tw/~hylee/index.html" target="_blank"> Hung-Yi Lee</a>, <a href="https://speech.ee.ntu.edu.tw/previous_version/lslNew.htm" target="_blank">Lin-shan Lee</a>.
                            <br/>
                            In <a href="http://www.interspeech2020.org/" target="_blank">
                                <b>
                                    Interspeech</b></a>, 2020. <a style="color:#e22222">(Interspeech 2020 Travel Grant)</a>
                            <br/>
                            [<a href="bibs/chuang2020speechbert.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#chuang2020speechbert_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://www.isca-speech.org/archive/pdfs/interspeech_2020/chuang20b_interspeech.pdf" target="_blank">pdf</a>]
                            <!-- [<a href="https://github.com/MiuLab/DuaLUG" target="_blank">code</a>] -->
                            [<a href="https://www.youtube.com/watch?v=7mf7nSh8dGE" target="_blank">video</a>]
                            <!-- [<a href="https://leaderboard.allenai.org/quoref" target="_blank">leaderboard</a>] -->
                            <div id="chuang2020speechbert_abstract" class="abstract" style="display:none;">
                                <p>
                                    While various end-to-end models for spoken language understanding tasks have been explored recently, this paper is probably the first known attempt to challenge the very difficult task of end-to-end spoken question answering (SQA). Learning from the very successful BERT model for various text processing tasks, here we proposed an audio-and-text jointly learned SpeechBERT model. This model outperformed the conventional approach of cascading ASR with the following text question answering (TQA) model on datasets including ASR errors in answer spans, because the end-to-end model was shown to be able to extract information out of audio data before ASR produced errors. When ensembling the proposed end-to-end model with the cascade architecture, even better performance was achieved. In addition to the potential of end-to-end SQA, the SpeechBERT can also be considered for many other spoken language understanding tasks just as BERT for many text processing tasks.
                                </p>
                            </div>
                        </li>
                    </ul>
                    <h3>2019</h3>
                    <ul class="pl">
                        <li>
                            <a href="https://arxiv.org/abs/1910.01462" target="_blank">
                                <b>Towards Understanding of Medical Randomized Controlled Trials by Conclusion Generation</b>
                            </a>
                            <br/>
                            <a href="https://lipolysis.github.io/" target="_blank">Alexander Te-Wei Shieh*</a>, <b>Yung-Sung Chuang*</b>, <a href="https://www.shangyusu.com/" target="_blank">Shang-Yu Su</a>,
                            <a href="https://www.csie.ntu.edu.tw/~yvchen/" target="_blank">Yun-Nung Chen</a>.
                            <br/>
                            In <a href="https://2019.emnlp.org/" target="_blank">
                                <b>
                                    Proceedings of the Tenth International Workshop on Health Text Mining and Information Analysis (LOUHI 2019) at The 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing</b></a>, 2019.
                            <br/>
                            [<a href="bibs/shieh2019towards.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#shieh2019towards_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://aclanthology.org/D19-6214.pdf" target="_blank">pdf</a>]
                            [<a href="https://github.com/MiuLab/RCT-Gen" target="_blank">code</a>]
                            <!-- [<a href="https://slideslive.com/38938863/lifelong-language-knowledge-distillation" target="_blank">video</a>] -->
                            <!-- [<a href="https://leaderboard.allenai.org/quoref" target="_blank">leaderboard</a>] -->
                            <div id="shieh2019towards_abstract" class="abstract" style="display:none;">
                                <p>
                                    Randomized controlled trials (RCTs) represent the paramount evidence of clinical medicine. Using machines to interpret the massive amount of RCTs has the potential of aiding clinical decision-making. We propose a RCT conclusion generation task from the PubMed 200k RCT sentence classification dataset to examine the effectiveness of sequence-to-sequence models on understanding RCTs. We first build a pointer-generator baseline model for conclusion generation. Then we fine-tune the state-of-the-art GPT-2 language model, which is pre-trained with general domain data, for this new medical domain task. Both automatic and human evaluation show that our GPT-2 fine-tuned models achieve improved quality and correctness in the generated conclusions compared to the baseline pointer-generator model. Further inspection points out the limitations of this current approach and future directions to explore.
                                </p>
                            </div>
                        </li>
                    </ul>
                </div>
            </div>
            <hr>
            <div class="row" id="talks">
                <div class="col">
                    <h2>Talks</h2>
                    <ul>
                        <li>
                            <b>Lectures in Mandarin</b><br/>
                            <a href="https://www.youtube.com/watch?v=jvyKmU4OM3c" target="_blank">
                                <b>Non-Autoregressive Conditional Sequence Modeling (DLHLP 2020@NTU)</b>
                            </a>
                            <br/>
                            <iframe width="560" height="315" src="https://www.youtube.com/embed/jvyKmU4OM3c" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                            <br/>
                            <a href="https://www.youtube.com/watch?v=YIuBHB9Ejok" target="_blank">
                                <b>Unsupervised Syntactic Parsing (Machine Learning 2019@NTU)</b>
                            </a>
                            <br/>
                            <iframe width="560" height="315" src="https://www.youtube.com/embed/YIuBHB9Ejok" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                        </li>
                        <li>
                            <b>Virtual Conference Video</b><br/>
                            <a href="https://www.youtube.com/watch?v=bAX8oL_ywpQ" target="_blank">
                                <b>[ACL-IJCNLP 2021] Mitigating Biases in Toxic Language Detection through Invariant Rationalization</b>
                            </a>
                            <br/>
                            <iframe width="560" height="315" src="https://www.youtube.com/embed/bAX8oL_ywpQ" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                            <br/>
                            <a href="https://www.youtube.com/watch?v=I80xwWfswl8" target="_blank">
                                <b>[ACL-IJCNLP 2021] Investigating the Reordering Capability in CTC-based Non-Autoregressive End-to-End Speech Translation</b>
                            </a>
                            <br/>
                            <iframe width="560" height="315" src="https://www.youtube.com/embed/I80xwWfswl8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                            <br/>
                            <a href="https://www.youtube.com/watch?v=t3Ee5fA8mCo" target="_blank">
                                <b>[EMNLP 2020] Lifelong Language Knowledge Distillation</b>
                            </a>
                            <br/>
                            <iframe width="560" height="315" src="https://www.youtube.com/embed/t3Ee5fA8mCo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                            <br/>
                            <a href="https://www.youtube.com/watch?v=a6txVSmX7fI" target="_blank">
                                <b>[Interspeech 2020] SpeechBERT: An Audio-and-text Jointly Learned Language Model for End-to-end Spoken Question Answering</b>
                            </a>
                            <br/>
                            <iframe width="560" height="315" src="https://www.youtube.com/embed/a6txVSmX7fI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                        </li>
                    </ul>
                </div>
            </div>
            <hr>

            <div class="row" id="projects">
                <div class="col">
                    <h2>Projects</h2>
                    <ul>
                        <li>
                            <a href="https://drive.google.com/file/d/199N-2Up1SFS_BtM_nRVw8yCiYuepw1bg/view" target="_blank">
                                <b>Speech Recognition for Impaired Speaker</b>
                            </a>
                            <br/>
                            <!-- [<a href="#" onclick="$('#sris_abstract').toggle();return false;">abstract</a>] -->
                            [<a href="https://drive.google.com/file/d/199N-2Up1SFS_BtM_nRVw8yCiYuepw1bg/view" target="_blank">pdf</a>]
                            <!-- [<a href="https://aclanthology.org/2020.findings-emnlp.443.pdf" target="_blank">pdf</a>] -->
                            [<a href="https://github.com/voidism/ImpairedVoiceASR" target="_blank">code</a>]
                            <!-- [<a href="https://slideslive.com/38938863/lifelong-language-knowledge-distillation" target="_blank">video</a>] -->
                            <!-- [<a href="https://leaderboard.allenai.org/quoref" target="_blank">leaderboard</a>] -->
                            <div>
                                <p>
                                    Reducing WER from 80% to 20% for impaired voice speaker via personalized adaptation (in Mandarin). Final Project in <i>Introduction to Biomedical Engineering 2020 Spring</i>@NTU.
                                </p>
                            </div>
                        </li>
                        <li>
                            <a href="https://github.com/bchao1/108-2-NMLAB-Final" target="_blank">
                                <b>DPP: Decentralized Publishing Platform</b>
                            </a>
                            <br/>
                            <!-- [<a href="#" onclick="$('#sris_abstract').toggle();return false;">abstract</a>] -->
                            <!-- [<a href="https://drive.google.com/file/d/199N-2Up1SFS_BtM_nRVw8yCiYuepw1bg/view" target="_blank">pdf</a>] -->
                            <!-- [<a href="https://aclanthology.org/2020.findings-emnlp.443.pdf" target="_blank">pdf</a>] -->
                            [<a href="https://github.com/bchao1/108-2-NMLAB-Final" target="_blank">code</a>]
                            <!-- [<a href="https://slideslive.com/38938863/lifelong-language-knowledge-distillation" target="_blank">video</a>] -->
                            <!-- [<a href="https://leaderboard.allenai.org/quoref" target="_blank">leaderboard</a>] -->
                            <div>
                                <p>
                                    A Decentralized Publishing Platform created with Blockchain and Etheruem smart contract. Final Project in <i>Networking and Multinmedia Lab 2020 Spring</i>@NTU.
                                </p>
                            </div>
                        </li>
                        <li>
                            <a href="https://github.com/w4n9r3ntru3/MIPS-Processor" target="_blank">
                                <b>Single-Cycle MIPS Processor Implementation in Verilog</b>
                            </a>
                            <br/>
                            <!-- [<a href="#" onclick="$('#sris_abstract').toggle();return false;">abstract</a>] -->
                            <!-- [<a href="https://drive.google.com/file/d/199N-2Up1SFS_BtM_nRVw8yCiYuepw1bg/view" target="_blank">pdf</a>] -->
                            <!-- [<a href="https://aclanthology.org/2020.findings-emnlp.443.pdf" target="_blank">pdf</a>] -->
                            [<a href="https://github.com/w4n9r3ntru3/MIPS-Processor" target="_blank">code</a>]
                            <!-- [<a href="https://slideslive.com/38938863/lifelong-language-knowledge-distillation" target="_blank">video</a>] -->
                            <!-- [<a href="https://leaderboard.allenai.org/quoref" target="_blank">leaderboard</a>] -->
                            <div>
                                <p>
                                    Ranking 2nd place out of 44 groups by A*T value (Area * Clock Time). Final Project in <i>Computer Architecture 2019 Fall</i>@NTU.
                                </p>
                            </div>
                        </li>
                        <li>
                            <a href="https://github.com/raywu0123/ICCAD2019-Problem-E" target="_blank">
                                <b>Rectilinear Polygon Operations for Physical Design</b>
                            </a>
                            <br/>
                            <!-- [<a href="#" onclick="$('#sris_abstract').toggle();return false;">abstract</a>] -->
                            <!-- [<a href="https://drive.google.com/file/d/199N-2Up1SFS_BtM_nRVw8yCiYuepw1bg/view" target="_blank">pdf</a>] -->
                            <!-- [<a href="https://aclanthology.org/2020.findings-emnlp.443.pdf" target="_blank">pdf</a>] -->
                            [<a href="https://github.com/raywu0123/ICCAD2019-Problem-E" target="_blank">code</a>]
                            <!-- [<a href="https://slideslive.com/38938863/lifelong-language-knowledge-distillation" target="_blank">video</a>] -->
                            <!-- [<a href="https://leaderboard.allenai.org/quoref" target="_blank">leaderboard</a>] -->
                            <div>
                                <p>
                                    <a href="http://iccad-contest.org/2019/tw/problems.html" target="_blank">ICCAD 2019 CAD Contest - Problem E</a>. Final Project in <i>Algorithms 2019 Spring</i>@NTU.
                                </p>
                            </div>
                        </li>
                        <li>
                            <a href="https://voidism.github.io/pdfs/2019_DLCV_Final_Project_Poster_(final).pdf" target="_blank">
                                <b>Multi-Source Unsupervised Domain Adaptation Challenge</b>
                            </a>
                            <br/>
                            <!-- [<a href="#" onclick="$('#sris_abstract').toggle();return false;">abstract</a>] -->
                            <!-- [<a href="https://drive.google.com/file/d/199N-2Up1SFS_BtM_nRVw8yCiYuepw1bg/view" target="_blank">pdf</a>] -->
                            <!-- [<a href="https://aclanthology.org/2020.findings-emnlp.443.pdf" target="_blank">pdf</a>] -->
                            [<a href="https://voidism.github.io/pdfs/2019_DLCV_Final_Project_Poster_(final).pdf" target="_blank">pdf</a>]
                            <!-- [<a href="https://slideslive.com/38938863/lifelong-language-knowledge-distillation" target="_blank">video</a>] -->
                            <!-- [<a href="https://leaderboard.allenai.org/quoref" target="_blank">leaderboard</a>] -->
                            <div>
                                <p>
                                    Conducted experiments on unsupervised domain adaptation (UDA) for multi-source dataset from ICCV2019 Workshop Challenge. Final Project in <i>Deep Learning for Computer Vision 2019 Spring</i>@NTU.
                                </p>
                            </div>
                        </li>
                        <!-- <li> -->
                            <!-- <a href="https://github.com/voidism/Transformer_CycleGAN_Text_Style_Transfer-pytorch" target="_blank"> -->
                                <!-- <b>Transformer-based CycleGAN Text Style Transfer</b> -->
                            <!-- </a> -->
                            <!-- <br/> -->
                            <!-- [<a href="#" onclick="$('#sris_abstract').toggle();return false;">abstract</a>] -->
                            <!-- [<a href="https://drive.google.com/file/d/199N-2Up1SFS_BtM_nRVw8yCiYuepw1bg/view" target="_blank">pdf</a>] -->
                            <!-- [<a href="https://aclanthology.org/2020.findings-emnlp.443.pdf" target="_blank">pdf</a>] -->
                            <!-- [<a href="https://github.com/voidism/Transformer_CycleGAN_Text_Style_Transfer-pytorch" target="_blank">pdf</a>] -->
                            <!-- [<a href="https://slideslive.com/38938863/lifelong-language-knowledge-distillation" target="_blank">video</a>] -->
                            <!-- [<a href="https://leaderboard.allenai.org/quoref" target="_blank">leaderboard</a>] -->
                            <!-- <div> -->
                                <!-- <p> -->
                                    <!-- Building a transformer-based cycleGAN style transfer framework for text generation. -->
                                <!-- </p> -->
                            <!-- </div> -->
                        <!-- </li> -->
                        <li>
                            <a href="https://arxiv.org/abs/1901.05816" target="_blank">
                                <b>Robust Chinese Word Segmentation with Contextualized Word Representations</b>
                            </a>
                            <br/>
                            <!-- [<a href="#" onclick="$('#sris_abstract').toggle();return false;">abstract</a>] -->
                            <!-- [<a href="https://drive.google.com/file/d/199N-2Up1SFS_BtM_nRVw8yCiYuepw1bg/view" target="_blank">pdf</a>] -->
                            [<a href="https://arxiv.org/pdf/1901.05816.pdf" target="_blank">pdf</a>]
                            [<a href="https://pypi.org/project/pywordseg/" target="_blank">pypi</a>]
                            [<a href="https://github.com/voidism/pywordseg" target="_blank">code</a>]
                            <!-- [<a href="https://leaderboard.allenai.org/quoref" target="_blank">leaderboard</a>] -->
                            <div>
                                <p>
                                    Developed an open source state-of-the-art Chinese word segmentation system with BiLSTM and ELMo, helping the downstream Chinese NLP task. Final project in <i>Digital Speech Processing 2018 Fall</i>@NTU.
                                </p>
                            </div>
                        </li>
                        <li>
                            <a href="https://github.com/voidism/Big-Two" target="_blank">
                                <b>Big Two Game Environment and Agent in C++</b>
                            </a>
                            <br/>
                            <!-- [<a href="#" onclick="$('#sris_abstract').toggle();return false;">abstract</a>] -->
                            <!-- [<a href="https://drive.google.com/file/d/199N-2Up1SFS_BtM_nRVw8yCiYuepw1bg/view" target="_blank">pdf</a>] -->
                            <!-- [<a href="https://arxiv.org/pdf/1901.05816.pdf" target="_blank">pdf</a>] -->
                            <!-- [<a href="https://pypi.org/project/pywordseg/" target="_blank">pypi</a>] -->
                            [<a href="https://github.com/voidism/Big-Two" target="_blank">code</a>]
                            <!-- [<a href="https://leaderboard.allenai.org/quoref" target="_blank">leaderboard</a>] -->
                            <div>
                                <p>
                                    Developed a human-computer game program of the big-two game. Final Project in <i>Computer Programming 2017 Fall</i>@NTU.
                                </p>
                            </div>
                        </li>
                    </ul>
                </div>
            </div>
            <hr>
            <div class="row" id="honors">
                <div class="col">
                    <h2>Honors</h2>
                    <ul>
                        <li>
                            <b>Dean’s list (4 times)</b>, Electrical Engineering Dept. at NTU, Spring ’18, Spring ’19, Fall ’19, Spring ’20
                        </li>
                        <li>
                            <b>Irving T. Ho Memorial Scholarship (2 times)</b>, EECS at NTU, Fall ’18, Fall ’19
                        </li>
                        <li>
                            <b>Travel Grant</b>, INTERSPEECH 2020 conference, Sep. 2020
                        </li>
                        <li>
                            <b>Appier Best Application Award</b>, 2020 NTU CSIE Undergrad Special Research Exhibition, Jun. 2020
                        </li>
                        <li>
                            <b>2nd Place & Appier 1st Award</b>, 2019 NTU CSIE Undergrad Special Research Exhibition, Jun. 2019
                        </li>
                        <li>
                            <b>2nd Place</b>, 2019 NTUEE Undergraduate Innovation Award, Jun. 2019
                        </li>
                        <li>
                            <b>1st Place</b>, 2018 H. Spectrum Demo Day (out of 21 teams), Jul. 2018
                        </li>
                        <li>
                            <b>1st Place</b>, NCTS Health Hackathon 2018 (out of 18 teams), Jun. 2018
                        </li>
                        <li>
                            <b>Top 8 Finalist</b>, Microsoft Imagine Cup Taiwan National Final 2018, Apr. 2018
                        </li>
                        <li>
                            <b>Best Tech Award & Microsoft Enterprise Award</b>, MakeNTU 2018 (out of 50 teams), Mar. 2018
                        </li>
                        <li>
                            <b>1st place of Dept. of Transportation</b>, HackNTU 2017 (out of 100+ teams), Jul. 2017
                        </li>
                        <!-- <li>
                            <b>Teaching Assistant:</b> <br/>
                              Deep Learning for Human Language Processing, 2020 Spring@NTU (Prof. Hung-Yi Lee)
                        </li> -->
                    </ul>
                </div>
            </div>
            <hr>
            <div class="row" id="services">
                <div class="col">
                    <h2>Services</h2>
                    <ul>
                        <li>
                            <b>Reviewer:</b> <br/>
                              NeurIPS, ICLR, ICML, ICASSP
                        </li>
                        <!-- <li>
                            <b>Teaching Assistant:</b> <br/>
                              Deep Learning for Human Language Processing, 2020 Spring@NTU (Prof. Hung-Yi Lee)
                        </li> -->
                    </ul>
                </div>
            </div>
            <hr>
            <div class="row">
                <div class="col">
                    <!-- <h2>Miscellany</h2> -->
                    <ul>
                        <li>
                            This website is built from the <a href="https://github.com/nelson-liu/website">source code</a> of Nelson F. Liu's awesome website (<a href="https://nelsonliu.me/"
                            target="_blank">https://nelsonliu.me</a>).
                        </li>
                    </ul>
                </div>
            </div>
            <footer class="pt-2 my-md-2 pt-md-2 border-top">
                <div class="row justify-content-center">
                    <div class="col-6 col-md text-left align-self-center">
                        <p class="h5 text-muted">
                            © Yung-Sung Chuang, 2021
                        </p>
                    </div>
                    <div class="col-6 col-md text-right">
                        <a href="https://www.csail.mit.edu/" class="image-link">
                            <img class="mr-4" src="img/mit_csail_logo.svg" alt="MIT CSAIL logo." height="75">
                        </a>
                        <a href="https://www.mit.edu/" class="image-link">
                            <img src="img/mit_logo.svg" alt="MIT logo." height="50">
                        </a>
                    </div>
                </div>
            </footer>
        </div>
    </body>
</html>
