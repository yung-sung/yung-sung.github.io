<!DOCTYPE html>
<html lang="en">
    <head>
        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-116076474-3"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-116076474-3');
        </script>
        <title>Yung-Sung Chuang</title>
        <meta http-equiv="content-type" content="text/html; charset=UTF-8">
        <meta charset="utf-8">
        <meta property="og:url" content="https://people.csail.mit.edu/yungsung/" />
	    <meta property="og:title" content="Yung-Sung Chuang" />
	    <meta property="og:image" content="img/Yung-Sung.jpg" />
	    <meta http-equiv="X-UA-Compatible" content="IE=edge">
	    <meta name="author" content="Yung-Sung Chuang">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
        <link rel="shortcut icon" type="image/png" href="favicon.ico"/>

        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
        <link rel="stylesheet" href="css/style.css">
        <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
        <link href='https://fonts.googleapis.com/css?family=EB+Garamond' rel='stylesheet' type='text/css'>
        <style>
            /* Add some padding on document's body to prevent the content
            to go underneath the header and footer */
            body{        
                padding-top: 50px;
                padding-bottom: 0px;
            }
            html {
                scroll-padding-top: 70px; /* height of sticky header */
            }
            .container{
                width: 80%;
                margin: 0 auto; /* Center the DIV horizontally */
            }
            .fixed-header, .fixed-footer{
                width: 100%;
                position: fixed;        
                background: rgb(69, 142, 226);
                padding: 7px 0;
                color: #fff;
                font-size: calc(6px + 0.78125vw);
            }
            .fixed-header{
                top: 0;
            }
            .fixed-footer{
                bottom: 0;
            }    
            /* Some more styles to beutify this example (85, 129, 212) */
            nav a{
                color: #fff;
                text-decoration: none;
                padding: 7px 1.7vw;
                display: inline-block;
            }
            h1 span {
                font-size: 20pt;
            }
            h1 span a{
                font-size: 12pt;
            }
        </style>
    </head>
    <body>
        <div class="fixed-header" style="z-index:1;">
            <div class="container">
                <nav>
                    <a href="#">Home</a>
                    <a href=#publications>Publications</a>
                    <a href=#talks>Talks</a>
                    <a href=#projects>Projects</a>
                    <a href=#honors>Honors</a>
                    <!-- <a href=#services>Services</a> -->
                    <a href="https://voidism.github.io/">Blog</a>
                    <!-- <a href="https://www.csail.mit.edu/person/yung-sung-chuang" style="float:right;"><i>@csail.mit</i></a> -->
                </nav>
            </div>
        </div>
        <div class="container mt-5" style="z-index:0;">
            <div class="row mb-3">
                <div class="col">
                    <h1>Yung-Sung Chuang <span>(莊永松) <a href="#" onclick="$('#my_name').toggle();return false;"><i>How to pronounce?</i></a></span></h1>
                </div>
            </div>
            <div id="my_name" class="abstract" style="display:none;">
                <p style="color:gray">
                    My first name Yung-Sung (永松) should be pronounced as "yǒng sōng". My last name Chuang (莊) should be pronounced as "juāng". In my first name, Yung means "forever" and Sung means "pine tree", so it has the meaning of "long-lasting" or "longevity".
                    </br>
                    <audio controls>
                        <source src="files/name-tts.mp3" type="audio/mpeg">
                    </audio>
                </p>
            </div>
            <div class="row">
                <div class="col-lg-4 col-md-6 order-0 order-xs-0 order-sm-0 order-md-1 order-lg-1">
                    <div class="card mb-3">
                        <img class="card-img-top" src="img/Yung-Sung.jpg" alt="Yung-Sung Chuang">
                        <div class="card-body">
                            <h5 class="card-title">
                                <b>Yung-Sung Chuang</b>
                            </h5>
                            <p class="card-text">
                                MIT EECS PhD Student
                                </br>
                                Office: 32-G436
                            </p>
                        </div>
                    </div>
                </div>
                <div class="col-lg-8 col-md-6 order-1 order-xs-1 order-sm-1 order-md-0 order-lg-0">
                    <!-- <p>▶ [<a href=#publications>Publications</a>] [<a href=#talks>Talks</a>] [<a href=#projects>Projects</a>] [<a href=#honors>Honors</a>] [<a href=#services>Services</a>] </p> -->
                    <p>
                        Hi! I'm a second-year PhD student in <a href="https://www.eecs.mit.edu/" target="_blank">Electrical Engineering and Computer Science</a> at
                        <a href="https://www.mit.edu" target="_blank">Massachusetts Institute of Technology</a>, where I work with <a href="https://www.csail.mit.edu/person/jim-glass"
                        target="_blank">Jim Glass</a>. 
                    </p>
                    <p>
                        My research interest broadly covers 
                        the deep learning technique for natural language processing and 
                        speech processing. In particular, I aim to utilize the ability of 
                        machines to help people grasp large information in text/audio form 
                        in efficient ways.
                    </p>
                    <p>
                        Previously, I was an undergraduate student in Electrical Engineering
                        at National Taiwan University. I joined Speech Processing Lab 
                        supervised by <a href="https://speech.ee.ntu.edu.tw/~hylee/index.html">Hung-Yi Lee</a> and <a href="https://speech.ee.ntu.edu.tw/previous_version/lslNew.htm">Lin-shan Lee</a>, and Machine Intelligence Understanding Lab 
                        supervised by <a href="https://www.csie.ntu.edu.tw/~yvchen/">Yun-Nung (Vivian) Chen</a>. I received the NTU Presidential 
                        Award for top 5% students four times in 2018-2020, <a href="https://irvingthofoundation.github.io/ho-fellows.htm">Irving T. Ho Memorial 
                        Scholarship</a> in 2018 and 2019. Here is my <a href="https://people.csail.mit.edu/yungsung/files/CV.pdf">Curriculum Vitae</a>.
                    </p>
                </div>
            </div>
            <div class="row">
                <div class="col">
                    <p>
                        Email: yungsung [AT] mit.edu
                    </p>
                    <p>
                        Links:
                        [<a href="files/CV.pdf" target="_blank">CV</a>] [<a href="https://twitter.com/YungSungChuang" target="_blank">Twitter</a>] [<a href="https://github.com/voidism" target="_blank">Github</a>] [<a href="https://scholar.google.com/citations?hl=en&user=3ar1DOwAAAAJ" target="_blank">Google Scholar</a>] [<a href="https://dblp.org/pers/hd/c/Chuang:Yung=Sung" target="_blank">DBLP</a>] [<a href="https://voidism.github.io/" target="_blank">Blog</a>] [<a href="https://www.linkedin.com/in/yschuang">Linkedin</a>] [<a href="https://www.instagram.com/yungsung.chuang/">Instagram</a>]
                    </p>
                </div>
            </div>
            <div class="row">
                <div class="col">
                    <h2>Recent News</h2>
                    <ul>
                        <li>
                          (06/2022) Start my summer internship at MIT-IBM Watson AI Lab with <a href="https://mitibmwatsonailab.mit.edu/people/yang-zhang/" target="_blank">Yang Zhang</a>, <a href="https://mitibmwatsonailab.mit.edu/people/shiyu-chang/" target="_blank">Shiyu Chang</a>, <a href="https://people.csail.mit.edu/yoonkim/" target="_blank">Yoon Kim</a>, <a href="https://scholar.google.com/citations?user=uEpr4C4AAAAJ&hl=en" target="_blank">Kaizhi Qian</a> <br/>
                        </li>
                        <li>
                            (04/2022) Our paper <a href="https://arxiv.org/abs/2204.10298" target="_blank">"DiffCSE: Difference-based Contrastive Learning for Sentence Embeddings"</a> is accepted by NAACL 2022 as a oral paper! <br/>
                        </li>
                        <li>
                            (03/2022) I am serving as a reviewer for <a href="https://nips.cc/Conferences/2022" target="_blank">NeurIPS 2022</a>. <br/>
                        </li>
                        <li>
                            (03/2022) I am serving on the Program Committee for <a href="https://www.workshopononlineabuse.com/" target="_blank">The Sixth Workshop on Online Abuse and Harms in NAACL 2022</a>. <br/>
                        </li>
                        <li>
                            (01/2022) I am serving as a reviewer for <a href="https://icml.cc/Conferences/2022" target="_blank">ICML 2022</a>. <br/>
                        </li>
                        <li>
                            (09/2021) Our paper <a href="https://arxiv.org/abs/2106.05933" target="_blank">"PARP: Prune, Adjust and Re-Prune for Self-Supervised Speech Recognition"</a> is accepted by NeurIPS 2021 as a spotlight paper! <br/>
                        </li>
                        <li>
                            (09/2021) Start my PhD life in Cambridge!
                        </li>
                        <li>
                            (06/2021) I am serving as a reviewer for <a href="https://iclr.cc/Conferences/2022" target="_blank">ICLR 2022</a>. <br/>
                        </li>
                        <li>
                            (04/2021) I am serving as a reviewer for <a href="https://nips.cc/Conferences/2021" target="_blank">NeurIPS 2021</a>. <br/>
                        </li>
                    </ul>
                </div>
            </div>
            <hr>
            <div class="row" id="publications">
                <div class="col">
                    <h2>Publications</h2>
                    <h3>2023</h3>
                    
                    <ul class="pl">
                        <li>
                            <a href="https://arxiv.org/abs/2305.17080" target="_blank">
                                <b>Expand, Rerank, and Retrieve: Query Reranking for Open-Domain Question Answering</b>
                            </a>
                            <br/>
                            <b>Yung-Sung Chuang</b>,
                            <a href="https://people.csail.mit.edu/weifang/" target="_blank">Wei Fang</a>,
                            <a href="https://swdanielli.github.io/" target="_blank">Shang-Wen Li</a>,
                            <a href="http://scottyih.org/" target="_blank">Wen-tau Yih</a>,
                            <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
                            <br/>
                            (To appear) In <a href="https://2023.aclweb.org/" target="_blank">
                                <b>
                                Findings of The 61st Annual Meeting of the Association for Computational Linguistics</b></a>, 2023.
                            <br/>
                            [<a href="bibs/chuang2023expand.bib" target="_blank" type="text/plain">bib</a>]
                            [<a href="#" onclick="$('#chuang2023expand_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://arxiv.org/pdf/2305.17080.pdf" target="_blank">pdf</a>]
                            [<a href="https://github.com/voidism/EAR" target="_blank">code</a>]
                            <div id="chuang2023expand_abstract" class="abstract" style="display:none;">
                                <p>
                                    We propose EAR, a query Expansion And Reranking approach for improving passage retrieval, with the application to open-domain question answering. EAR first applies a query expansion model to generate a diverse set of queries, and then uses a query reranker to select the ones that could lead to better retrieval results. Motivated by the observation that the best query expansion often is not picked by greedy decoding, EAR trains its reranker to predict the rank orders of the gold passages when issuing the expanded queries to a given retriever. By connecting better the query expansion model and retriever, EAR significantly enhances a traditional sparse retrieval method, BM25. Empirically, EAR improves top-5/20 accuracy by 3-8 and 5-10 points in in-domain and out-of-domain settings, respectively, when compared to a vanilla query expansion model, GAR, and a dense retrieval model, DPR.
                                </p>
                            </div>
                        </li>
                    </ul>

                    <ul class="pl">
                        <li>
                            <a href="https://arxiv.org/abs/2305.15225" target="_blank">
                                <b>SAIL: Search-Augmented Instruction Learning</b>
                            </a>
                            <br/>
                            <b>Yung-Sung Chuang</b>,
                            <a href="https://luohongyin.github.io/" target="_blank">Hongyin Luo</a>,
                            <a href="https://yuangongnd.github.io/" target="_blank">Yuan Gong</a>, Tianhua Zhang, <a href="https://people.csail.mit.edu/yoonkim/" target="_blank">Yoon Kim</a>, Xixin Wu, Danny Fox, Helen Meng, <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
                            <br/>
                            In <b>arXiv preprint</b>, 2023.
                            <br/>
                            [<a href="bibs/luo2023sail.bib" target="_blank" type="text/plain">bib</a>]
                            [<a href="#" onclick="$('#luo2023sail_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://arxiv.org/pdf/2305.15225.pdf" target="_blank">pdf</a>]
                            <div id="luo2023sail_abstract" class="abstract" style="display:none;">
                                <p>
                                    Large language models (LLMs) have been significantly improved by instruction fine-tuning, but still lack transparency and the ability to utilize up-to-date knowledge and information. In this work, we propose search-augmented instruction learning (SAIL), which grounds the language generation and instruction following abilities on complex search results generated by in-house and external search engines. With an instruction tuning corpus, we collect search results for each training case from different search APIs and domains, and construct a new search-grounded training set containing \textit{(instruction, grounding information, response)} triplets. We then fine-tune the LLaMA-7B model on the constructed training set. Since the collected results contain unrelated and disputing languages, the model needs to learn to ground on trustworthy search results, filter out distracting passages, and generate the target response. The search result-denoising process entails explicit trustworthy information selection and multi-hop reasoning, since the retrieved passages might be informative but not contain the instruction-following answer. Experiments show that the fine-tuned SAIL-7B model has a strong instruction-following ability, and it performs significantly better on transparency-sensitive tasks, including open-ended question answering and fact checking.
                                </p>
                            </div>
                        </li>
                    </ul>
                    
                    
                    <ul class="pl">
                        <li>
                            <a href="https://arxiv.org/abs/2304.03728" target="_blank">
                                <b>Interpretable Unified Language Checking</b>
                            </a>
                            <br/>
                            <b>Yung-Sung Chuang</b>,
                            <a href="https://luohongyin.github.io/" target="_blank">Hongyin Luo</a>,
                            Tianhua Zhang, <a href="https://people.csail.mit.edu/weifang/" target="_blank">Wei Fang</a>, Luc Gaitskell, Thomas Hartvigsen, Xixin Wu, Danny Fox, Helen Meng, <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>
                            <br/>
                            In <b>arXiv preprint</b>, 2023.
                            <br/>
                            [<a href="bibs/zhang2023interpretable.bib" target="_blank" type="text/plain">bib</a>]
                            [<a href="#" onclick="$('#zhang2023interpretable_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://arxiv.org/pdf/2304.03728.pdf" target="_blank">pdf</a>]
                            [<a href="https://github.com/luohongyin/UniLC" target="_blank">code</a>]
                            <div id="zhang2023interpretable_abstract" class="abstract" style="display:none;">
                                <p>
                                    Despite recent concerns about undesirable behaviors generated by large language models (LLMs), including non-factual, biased, and hateful language, we find LLMs are inherent multi-task language checkers based on their latent representations of natural and social knowledge. We present an interpretable, unified, language checking (UniLC) method for both human and machine-generated language that aims to check if language input is factual and fair. While fairness and fact-checking tasks have been handled separately with dedicated models, we find that LLMs can achieve high performance on a combination of fact-checking, stereotype detection, and hate speech detection tasks with a simple, few-shot, unified set of prompts. With the ``1/2-shot'' multi-task language checking method proposed in this work, the GPT3.5-turbo model outperforms fully supervised baselines on several language tasks. The simple approach and results suggest that based on strong latent knowledge representations, an LLM can be an adaptive and explainable tool for detecting misinformation, stereotypes, and hate speech.
                                </p>
                            </div>
                        </li>
                    </ul>
                    
                    <ul class="pl">
                        <li>
                            <a href="https://ieeexplore.ieee.org/abstract/document/10094821" target="_blank">
                                <b>C2KD: Cross-Lingual Cross-Modal Knowledge Distillation for Multilingual Text-Video Retrieval</b>
                            </a>
                            <br/>
                            <a href="http://people.csail.mit.edu/roudi/" target="_blank">Andrew Rouditchenko</a>,
                            <b>Yung-Sung Chuang</b>,
                            Nina Shvetsova,
                            Samuel Thomas,
                            Rogerio Feris,
                            Brian Kingsbury,
                            Leonid Karlinsky,
                            David Harwath,
                            Hilde Kuehne,
                            James Glass
                            <br/>
                            In <a href="https://2023.ieeeicassp.org/" target="_blank">
                                <b>
                                IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</b></a>, 2023.
                            <br/>
                            [<a href="bibs/roudi2023c2kd.bib" target="_blank" type="text/plain">bib</a>]
                            [<a href="#" onclick="$('#roudi2023c2kd_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://arxiv.org/pdf/2210.03625.pdf" target="_blank">pdf</a>]
                            [<a href="https://github.com/roudimit/c2kd" target="_blank">code & dataset</a>]
                            <div id="roudi2023c2kd_abstract" class="abstract" style="display:none;">
                                <p>
                                    Multilingual text-video retrieval methods have improved significantly in recent years, but the performance for languages other than English still lags. We propose a Cross-Lingual Cross-Modal Knowledge Distillation method to improve multilingual text-video retrieval. Inspired by the fact that English text-video retrieval outperforms other languages, we train a student model using input text in different languages to match the cross-modal predictions from teacher models using input text in English. We propose a cross entropy based objective which forces the distribution over the student’s text-video similarity scores to be similar to those of the teacher models. We introduce a new multilingual video dataset, Multi-YouCook2, by translating the English captions in the YouCook2 video dataset to 8 other languages. Our method improves multilingual text-video retrieval performance on Multi-YouCook2 and several other datasets such as Multi-MSRVTT and VATEX. We also conducted an analysis on the effectiveness of different multilingual text models as teachers.
                                </p>
                            </div>
                        </li>
                    </ul>
                    
                    <ul class="pl">
                        <li>
                            <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Lu_Visual_Language_Pretrained_Multiple_Instance_Zero-Shot_Transfer_for_Histopathology_Images_CVPR_2023_paper.html" target="_blank">
                                <b>Visual Language Pretrained Multiple Instance Zero-Shot Transfer for Histopathology Images</b>
                            </a>
                            <br/>
                            <a href="https://mingylu.me/" target="_blank">Ming Y Lu</a>,
                            Bowen Chen, 
                            Andrew Zhang, 
                            Drew FK Williamson, 
                            Richard J Chen, 
                            Tong Ding, 
                            Long Phi Le, 
                            <b>Yung-Sung Chuang</b>, 
                            Faisal Mahmood
                            <br/>
                            In <a href="https://2023.thecvf.com/" target="_blank">
                                <b>
                                Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</b></a>, 2023.
                            <br/>
                            [<a href="bibs/lu2022visual.bib" target="_blank" type="text/plain">bib</a>]
                            [<a href="#" onclick="$('#lu2022visual').toggle();return false;">abstract</a>]
                            [<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Lu_Visual_Language_Pretrained_Multiple_Instance_Zero-Shot_Transfer_for_Histopathology_Images_CVPR_2023_paper.pdf" target="_blank">pdf</a>]
                            [<a href="https://openaccess.thecvf.com/content/CVPR2023/supplemental/Lu_Visual_Language_Pretrained_CVPR_2023_supplemental.pdf" target="_blank">supplementary material</a>]
                            <div id="lu2022visual_abstract" class="abstract" style="display:none;">
                                <p>
                                    Contrastive visual language pretraining has emerged as a powerful method for either training new language-aware image encoders or augmenting existing pretrained models with zero-shot visual recognition capabilities. However, existing works typically train on large datasets of image-text pairs and have been designed to perform downstream tasks involving only small to medium sized-images, neither of which are applicable to the emerging field of computational pathology where there are limited publicly available paired image-text datasets and each image can span up to 100,000 x 100,000 pixels in dimensions. In this paper we present MI-Zero, a simple and intuitive framework for unleashing the zero-shot transfer capabilities of contrastively aligned image and text models to gigapixel histopathology whole slide images, enabling multiple downstream diagnostic tasks to be carried out by pretrained encoders without requiring any additional labels. MI-Zero reformulates zero-shot transfer under the framework of multiple instance learning to overcome the computational challenge of inference on extremely large images. We used over 550k pathology reports and other available in-domain text corpora to pretrain our text encoder. By effectively leveraging strong pretrained encoders, our best model pretrained on over 33k histopathology image-caption pairs achieves an average median zero-shot accuracy of 70.2% across three different real-world cancer subtyping tasks. Our code is available at: https://github. com/mahmoodlab/MI-Zero.
                                </p>
                            </div>
                        </li>
                    </ul>
                    
                    <h3>2022</h3>
                    <ul class="pl">
                        <li>
                            <a href="https://aclanthology.org/2022.aacl-tutorials.2/" target="_blank">
                                <b>Recent Advances in Pre-trained Language Models: Why Do They Work and How Do They Work</b>
                            </a>
                            <br/>
                            <a href="https://d223302.github.io/" target="_blank">Cheng-Han Chiang</a>,
                            <b>Yung-Sung Chuang</b>, 
                            <a href="https://speech.ee.ntu.edu.tw/~hylee/" target="_blank">Hung-Yi Lee</a>
                            <br/>
                            In <a href="https://aclanthology.org/2022.aacl-tutorials.0/" target="_blank">
                                <b>
                                Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing: Tutorial Abstracts</b></a>, 2022.
                            <br/>
                            [<a href="bibs/chiang2022recent.bib" target="_blank" type="text/plain">bib</a>]
                            [<a href="#" onclick="$('#chiang2022recent_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://aclanthology.org/2022.aacl-tutorials.2.pdf" target="_blank">pdf</a>]
                            [<a href="https://d223302.github.io/AACL2022-Pretrain-Language-Model-Tutorial/" target="_blank">tutorial page</a>]
                            [<a href="https://www.youtube.com/watch?v=thr4-hgLhi8" target="_blank">video</a>]
                            <div id="chiang2022recent_abstract" class="abstract" style="display:none;">
                                <p>
                                    Pre-trained language models (PLMs) are language models that are pre-trained on large-scaled corpora in a self-supervised fashion. These PLMs have fundamentally changed the natural language processing community in the past few years. In this tutorial, we aim to provide a broad and comprehensive introduction from two perspectives: why those PLMs work, and how to use them in NLP tasks. The first part of the tutorial shows some insightful analysis on PLMs that partially explain their exceptional downstream performance. The second part first focuses on emerging pre-training methods that enable PLMs to perform diverse downstream tasks and then illustrates how one can apply those PLMs to downstream tasks under different circumstances. These circumstances include fine-tuning PLMs when under data scarcity, and using PLMs with parameter efficiency. We believe that attendees of different backgrounds would find this tutorial informative and useful.
                                </p>
                            </div>
                        </li>
                    </ul>
                    
                    <ul class="pl">
                        <li>
                            <a href="https://arxiv.org/abs/2110.01147" target="_blank">
                                <b>On the interplay between sparsity, naturalness, intelligibility, and prosody in speech synthesis</b>
                            </a>
                            <br/>
                            <a href="http://people.csail.mit.edu/clai24/" target="_blank">Cheng-I Jeff Lai</a>, 
                            <a href="http://www.cs.columbia.edu/~ecooper/" target="_blank">Erica Cooper</a>, 
                            <a href="https://mitibmwatsonailab.mit.edu/people/yang-zhang/" target="_blank">Yang Zhang</a>, 
                            <a href="https://mitibmwatsonailab.mit.edu/people/shiyu-chang/" target="_blank">Shiyu Chang</a>, 
                            <a href="https://scholar.google.com/citations?user=uEpr4C4AAAAJ&hl=en" target="_blank">Kaizhi Qian</a>, 
                            <a href="https://github.com/yilunliao" target="_blank">Yi-Lun Liao</a>, 
                            <b>Yung-Sung Chuang</b>, 
                            <a href="https://scholar.google.com/citations?user=LIiCDa0AAAAJ&hl=en" target="_blank">Alexander H. Liu</a>, 
                            <a href="https://scholar.google.com/citations?user=nRrdjtwAAAAJ&hl=en" target="_blank">Junichi Yamagishi</a>, 
                            <a href="https://mitibmwatsonailab.mit.edu/people/david-cox/" target="_blank">David Cox</a>, 
                            <a href="http://people.csail.mit.edu/jrg/" target="_blank">James Glass</a>.
                            <br/>
                            In <a href="https://2022.ieeeicassp.org/" target="_blank">
                                <b>ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</b></a>, 2022. <a style="color:#e22222"></a>
                            <br/>
                            [<a href="bibs/lai2022on.bib" target="_blank" type="text/plain">bib</a>]
                            [<a href="#" onclick="$('#lai2022on_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://arxiv.org/pdf/2110.01147.pdf" target="_blank">pdf</a>]
                            [<a href="https://people.csail.mit.edu/clai24/prune-tts/" target="_blank">code</a>]
                            [<a href="https://www.youtube.com/watch?v=od7cK1Yz6oQ" target="_blank">video</a>]
                            <div id="lai2022on_abstract" class="abstract" style="display:none;">
                                <p>
                                    Are end-to-end text-to-speech (TTS) models over-parametrized? To what extent can these models be pruned, and what happens to their synthesis capabilities? This work serves as a starting point to explore pruning both spectrogram prediction networks and vocoders. We thoroughly investigate the tradeoffs between sparsity and its subsequent effects on synthetic speech. Additionally, we explore several aspects of TTS pruning: amount of finetuning data versus sparsity, TTS-Augmentation to utilize unspoken text, and combining knowledge distillation and pruning. Our findings suggest that not only are end-to-end TTS models highly prunable, but also, perhaps surprisingly, pruned TTS models can produce synthetic speech with equal or higher naturalness and intelligibility, with similar prosody. All of our experiments are conducted on publicly available models, and findings in this work are backed by large-scale subjective tests and objective measures. Code and pruned models are made available to facilitate future research on efficiency in TTS.
                                </p>
                            </div>
                        </li>
                    </ul>                             
                    <ul class="pl">
                        <li>
                            <a href="https://arxiv.org/abs/2204.10298" target="_blank">
                                <b>DiffCSE: Difference-based Contrastive Learning for Sentence Embeddings</b>
                            </a>
                            <br/>
                            <b>Yung-Sung Chuang</b>, 
                            <a href="http://super-ms.mit.edu/rumen.html" target="_blank">Rumen Dangovski</a>, 
                            <a href="http://people.csail.mit.edu/hyluo/" target="_blank">Hongyin Luo</a>, 
                            <a href="https://mitibmwatsonailab.mit.edu/people/yang-zhang/" target="_blank">Yang Zhang</a>, 
                            <a href="https://code-terminator.github.io/" target="_blank">Shiyu Chang</a>, 
                            <a href="http://www.mit.edu/~soljacic/marin.html" target="_blank">Marin Soljačić</a>, 
                            <a href="https://swdanielli.github.io/" target="_blank">Shang-Wen Li</a>, 
                            <a href="https://scottyih.org/" target="_blank">Scott Wen-tau Yih</a>, 
                            <a href="https://people.csail.mit.edu/yoonkim/" target="_blank">Yoon Kim</a>, 
                            <a href="https://www.csail.mit.edu/person/jim-glass" target="_blank">James Glass</a>.
                            <br/>
                            In <a href="https://2022.naacl.org/" target="_blank">
                                <b>
                                  Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</b></a>, 2022. <a style="color:#e22222"> (Oral Paper) </a>
                            <br/>
                            [<a href="bibs/chuang2022diffcse.bib" target="_blank" type="text/plain">bib</a>]
                            [<a href="#" onclick="$('#chuang2022diffcse_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://arxiv.org/pdf/2204.10298.pdf" target="_blank">pdf</a>]
                            [<a href="https://github.com/voidism/DiffCSE" target="_blank">code</a>]
                            [<a href="https://www.youtube.com/watch?v=9vx-HyzcXtU" target="_blank">video</a>]
                            <!-- [<a href="https://superbbenchmark.org/" target="_blank">leaderboard</a>] -->
                            <div id="chuang2022diffcse_abstract" class="abstract" style="display:none;">
                                <p>
                                    We propose DiffCSE, an unsupervised contrastive learning framework for learning sentence embeddings. DiffCSE learns sentence embeddings that are sensitive to the difference between the original sentence and an edited sentence, where the edited sentence is obtained by stochastically masking out the original sentence and then sampling from a masked language model. We show that DiffSCE is an instance of equivariant contrastive learning (Dangovski et al., 2021), which generalizes contrastive learning and learns representations that are insensitive to certain types of augmentations and sensitive to other "harmful" types of augmentations. Our experiments show that DiffCSE achieves state-of-the-art results among unsupervised sentence representation learning methods, outperforming unsupervised SimCSE by 2.3 absolute points on semantic textual similarity tasks.
                                </p>
                            </div>
                        </li>
                    </ul>
                    <ul class="pl">
                        <li>
                            <a href="https://arxiv.org/abs/2203.04911" target="_blank">
                                <b>DUAL: Discrete Spoken Unit Adaptive Learning for Textless Spoken Question Answering</b>
                            </a>
                            <br/>
                            <a href="https://daniellin94144.github.io/" target="_blank">Guan-Ting Lin</a>, 
                            <b>Yung-Sung Chuang</b>, 
                            <a href="https://github.com/voidful" target="_blank">Ho-Lam Chung</a>, 
                            <a href="https://scholar.google.com.tw/citations?user=R1mNI8QAAAAJ" target="_blank">Shu-wen Yang</a>, 
                            <a href="#">Hsuan-Jui Chen</a>, 
                            <a href="https://www.amazon.science/author/shuyan-dong" target="_blank">Shuyan Dong</a>, 
                            <a href="https://swdanielli.github.io/" target="_blank">Shang-Wen Li</a>, 
                            <a href="http://www.cs.toronto.edu/~asamir/" target="_blank">Abdelrahman Mohamed</a>, 
                            <a href="https://speech.ee.ntu.edu.tw/~hylee/index.html" target="_blank">Hung-yi Lee</a>, 
                            <a href="https://speech.ee.ntu.edu.tw/previous_version/lslNew.htm" target="_blank">Lin-shan Lee</a>.
                            <br/>
                                <b>
                                    Submitted to Interspeech 2022</b>.
                            <br/>
                            [<a href="bibs/lin2022dual.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#lin2022dual_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://arxiv.org/pdf/2203.04911.pdf" target="_blank">pdf</a>]
                            [<a href="https://github.com/daniellin94144/dual-textless-sqa" target="_blank">code</a>]
                            <!-- [<a href="https://www.youtube.com/watch?v=zd9fiVvej0k" target="_blank">video</a>] -->
                            <!-- [<a href="https://superbbenchmark.org/" target="_blank">leaderboard</a>] -->
                            <div id="lin2022dual_abstract" class="abstract" style="display:none;">
                                <p>
                                    Spoken Question Answering (SQA) is to find the answer from a spoken document given a question, which is crucial for personal assistants when replying to the queries from the users. Existing SQA methods all rely on Automatic Speech Recognition (ASR) transcripts. Not only does ASR need to be trained with massive annotated data that are time and cost-prohibitive to collect for low-resourced languages, but more importantly, very often the answers to the questions include name entities or out-of-vocabulary words that cannot be recognized correctly. Also, ASR aims to minimize recognition errors equally over all words, including many function words irrelevant to the SQA task. Therefore, SQA without ASR transcripts (textless) is always highly desired, although known to be very difficult. This work proposes Discrete Spoken Unit Adaptive Learning (DUAL), leveraging unlabeled data for pre-training and fine-tuned by the SQA downstream task. The time intervals of spoken answers can be directly predicted from spoken documents. We also release a new SQA benchmark corpus, NMSQA, for data with more realistic scenarios. We empirically showed that DUAL yields results comparable to those obtained by cascading ASR and text QA model and robust to real-world data. Our code and model will be open-sourced.
                                </p>
                            </div>
                        </li>
                    </ul>
                    <h3>2021</h3>
                    <ul class="pl">
                        <li>
                            <a href="https://arxiv.org/abs/2106.05933" target="_blank">
                                <b>PARP: Prune, Adjust and Re-Prune for Self-Supervised Speech Recognition</b>
                            </a>
                            <br/>
                            <a href="http://people.csail.mit.edu/clai24/" target="_blank">Cheng-I Jeff Lai</a>, 
                            <a href="https://mitibmwatsonailab.mit.edu/people/yang-zhang/" target="_blank">Yang Zhang</a>, <a href="https://alexander-h-liu.github.io/" target="_blank">Alexander H. Liu</a>, <a href="https://code-terminator.github.io/" target="_blank">Shiyu Chang</a>, <a href="https://tw.linkedin.com/in/yilunliao" target="_blank">Yi-Lun Liao</a>, 
                            <b>Yung-Sung Chuang</b>, 
                            <a href="https://scholar.google.com/citations?user=uEpr4C4AAAAJ&hl=en" target="_blank">Kaizhi Qian</a>, <a href="http://people.csail.mit.edu/sameerk/" target="_blank">Sameer Khurana</a>, <a href="https://mitibmwatsonailab.mit.edu/people/david-cox/" target="_blank">David Cox</a>, <a href="https://www.csail.mit.edu/person/jim-glass" target="_blank">James Glass</a>.
                            <br/>
                            In <a href="https://proceedings.neurips.cc/paper/2021/hash/b17c0907e67d868b4e0feb43dbbe6f11-Abstract.html" target="_blank">
                                <b>
                                    Thirty-fifth Conference on Neural Information Processing Systems (NeurIPS)</b></a>, 2021. <a style="color:#e22222">(Spotlight Paper)</a>
                            <br/>
                            [<a href="bibs/lai2021parp.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#lai2021parp_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://proceedings.neurips.cc/paper/2021/file/b17c0907e67d868b4e0feb43dbbe6f11-Paper.pdf" target="_blank">pdf</a>]
                            <!-- [<a href="https://github.com/s3prl/s3prl" target="_blank">code</a>] -->
                            <!-- [<a href="https://www.youtube.com/watch?v=zd9fiVvej0k" target="_blank">video</a>] -->
                            <!-- [<a href="https://superbbenchmark.org/" target="_blank">leaderboard</a>] -->
                            <div id="lai2021parp_abstract" class="abstract" style="display:none;">
                                <p>
                                    Recent work on speech self-supervised learning (speech SSL) demonstrated the benefits of scale in learning rich and transferable representations for Automatic Speech Recognition (ASR) with limited parallel data. It is then natural to investigate the existence of sparse and transferrable subnetworks in pre-trained speech SSL models that can achieve even better low-resource ASR performance. However, directly applying widely adopted pruning methods such as the Lottery Ticket Hypothesis (LTH) is suboptimal in the computational cost needed. Moreover, contrary to what LTH predicts, the discovered subnetworks yield minimal performance gain compared to the original dense network. In this work, we propose Prune-Adjust- Re-Prune (PARP), which discovers and finetunes subnetworks for much better ASR performance, while only requiring a single downstream finetuning run. PARP is inspired by our surprising observation that subnetworks pruned for pre-training tasks only needed to be slightly adjusted to achieve a sizeable performance boost in downstream ASR tasks. Extensive experiments on low-resource English and multi-lingual ASR show (1) sparse subnetworks exist in pre-trained speech SSL, and (2) the computational advantage and performance gain of PARP over baseline pruning methods. On the 10min Librispeech split without LM decoding, PARP discovers subnetworks from wav2vec 2.0 with an absolute 10.9%/12.6% WER decrease compared to the full model. We demonstrate PARP mitigates performance degradation in cross-lingual mask transfer, and investigate the possibility of discovering a single subnetwork for 10 spoken languages in one run.
                                </p>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <a href="https://arxiv.org/abs/2105.01051" target="_blank">
                                <b>SUPERB: Speech processing Universal PERformance Benchmark</b>
                            </a>
                            <br/>
                            <a href="https://scholar.google.com.tw/citations?user=R1mNI8QAAAAJ" target="_blank">Shu-wen Yang</a>, <a href="https://scholar.google.com/citations?user=SiyicoEAAAAJ&hl=zh-TW" target="_blank">Po-Han Chi*</a>, <b>Yung-Sung Chuang*</b>, 
                            <a href="http://people.csail.mit.edu/clai24/" target="_blank">Cheng-I Jeff Lai*</a>, 
                            <a href="https://ai.facebook.com/people/kushal-lakhotia/" target="_blank">Kushal Lakhotia*</a>, <a href="https://scholar.google.com/citations?user=0lrZq9MAAAAJ&hl=en" target="_blank">Yist Y. Lin*</a>, 
                            <a href="https://andi611.github.io/" target="_blank">Andy T. Liu*</a>, <a href="http://shijt.site/" target="_blank">Jiatong Shi*</a>, <a href="https://www.lti.cs.cmu.edu/people/222227473/xuankai-chang" target="_blank">Xuankai Chang</a>, <a href="https://github.com/DanielLin94144" target="_blank">Guan-Ting Lin</a>, 
                            <a href="https://tw.linkedin.com/in/tzu-hsien-huang-1686651b6" target="_blank">Tzu-Hsien Huang</a>, Wei-Cheng Tseng, <a href="https://tw.linkedin.com/in/ko-tik-lee-4747291a2/en?trk=people-guest_people_search-card" target="_blank">Ko-tik Lee</a>, <a href="https://scholar.google.com.tw/citations?user=qJ5zXNIAAAAJ" target="_blank">Da-Rong Liu</a>, 
                            <a href="https://dblp.org/pid/210/0905.html" target="_blank">Zili Huang</a>, <a href="https://www.amazon.science/author/shuyan-dong" target="_blank">Shuyan Dong</a>, <a href="https://swdanielli.github.io/" target="_blank">Shang-Wen Li</a>, 
                            <a href="https://sites.google.com/view/shinjiwatanabe" target="_blank">Shinji Watanabe</a>, 
                            <a href="http://www.cs.toronto.edu/~asamir/" target="_blank">Abdelrahman Mohamed</a>, <a href="https://speech.ee.ntu.edu.tw/~hylee/index.html" target="_blank">Hung-Yi Lee</a>.
                            <br/>
                            In <a href="https://www.interspeech2021.org/" target="_blank">
                                <b>
                                    Interspeech</b></a>, 2021.
                            <br/>
                            [<a href="bibs/yang2021superb.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#yang2020superb_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://arxiv.org/pdf/2105.01051.pdf" target="_blank">pdf</a>]
                            [<a href="https://github.com/s3prl/s3prl" target="_blank">code</a>]
                            [<a href="https://www.youtube.com/watch?v=zd9fiVvej0k" target="_blank">video</a>]
                            [<a href="https://superbbenchmark.org/" target="_blank">leaderboard</a>]
                            <div id="yang2020superb_abstract" class="abstract" style="display:none;">
                                <p>
                                    Self-supervised learning (SSL) has proven vital for advancing research in natural language processing (NLP) and computer vision (CV). The paradigm pretrains a shared model on large volumes of unlabeled data and achieves state-of-the-art (SOTA) for various tasks with minimal adaptation. However, the speech processing community lacks a similar setup to systematically explore the paradigm. To bridge this gap, we introduce Speech processing Universal PERformance Benchmark (SUPERB). SUPERB is a leaderboard to benchmark the performance of a shared model across a wide range of speech processing tasks with minimal architecture changes and labeled data. Among multiple usages of the shared model, we especially focus on extracting the representation learned from SSL due to its preferable re-usability. We present a simple framework to solve SUPERB tasks by learning task-specialized lightweight prediction heads on top of the frozen shared model. Our results demonstrate that the framework is promising as SSL representations show competitive generalizability and accessibility across SUPERB tasks. We release SUPERB as a challenge with a leaderboard and a benchmark toolkit to fuel the research in representation learning and general speech processing.
                                </p>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <a href="https://arxiv.org/abs/2105.04840" target="_blank">
                                <b>Investigating the Reordering Capability in CTC-based Non-Autoregressive End-to-End Speech Translation</b>
                            </a>
                            <br/>
                            <a href="https://scholar.google.com.hk/citations?user=CYDgtRoAAAAJ" target="_blank">Shun-Po Chuang*</a>, <b>Yung-Sung Chuang*</b>, <a href="https://aclanthology.org/people/c/chih-chiang-chang/" target="_blank">Chih-Chiang Chang*</a>,
                            <a href="https://speech.ee.ntu.edu.tw/~hylee/index.html" target="_blank">Hung-Yi Lee</a>.
                            <br/>
                            In <a href="https://aclanthology.org/volumes/2021.findings-acl/" target="_blank">
                                <b>
                                    The Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Findings</b></a>, 2021.
                            <br/>
                            [<a href="bibs/chuang2021investigating.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#chuang2021investigating_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://aclanthology.org/2021.findings-acl.92.pdf" target="_blank">pdf</a>]
                            [<a href="https://github.com/voidism/NAR-ST" target="_blank">code</a>]
                            [<a href="https://www.youtube.com/watch?v=I80xwWfswl8" target="_blank">video</a>]
                            <!-- [<a href="https://leaderboard.allenai.org/quoref" target="_blank">leaderboard</a>] -->
                            <div id="chuang2021investigating_abstract" class="abstract" style="display:none;">
                                <p>
                                    We study the possibilities of building a non-autoregressive speech-to-text translation model using connectionist temporal classification (CTC), and use CTC-based automatic speech recognition as an auxiliary task to improve the performance. CTC's success on translation is counter-intuitive due to its monotonicity assumption, so we analyze its reordering capability. Kendall's tau distance is introduced as the quantitative metric, and gradient-based visualization provides an intuitive way to take a closer look into the model. Our analysis shows that transformer encoders have the ability to change the word order and points out the future research direction that worth being explored more on non-autoregressive speech translation.
                                </p>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <a href="https://arxiv.org/abs/2106.07240" target="_blank">
                                <b>Mitigating Biases in Toxic Language Detection through Invariant Rationalization</b>
                            </a>
                            <br/>
                            <b>Yung-Sung Chuang</b>, <a href="https://www.linkedin.com/in/mingye94" target="_blank">Mingye Gao</a>,
                            <a href="http://people.csail.mit.edu/hyluo/" target="_blank">Hongyin Luo</a>,
                            <a href="https://www.csail.mit.edu/person/jim-glass" target="_blank">James Glass</a>,
                            <a href="https://speech.ee.ntu.edu.tw/~hylee/index.html" target="_blank">Hung-Yi Lee</a>,
                            <a href="https://www.csie.ntu.edu.tw/~yvchen/" target="_blank">Yun-Nung Chen</a>,
                            <a href="https://swdanielli.github.io/" target="_blank">Shang-Wen Li</a>.
                            <br/>
                            In <a href="https://aclanthology.org/events/woah-2021/" target="_blank">
                                <b>
                                    The 5th Workshop on Online Abuse and Harms at The Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing
                                    (WOAH@ACL-IJCNLP)</b></a>, 2021.
                            <br/>
                            [<a href="bibs/chuang2021mitigating.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#chuang2021mitigating_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://aclanthology.org/2021.woah-1.12.pdf" target="_blank">pdf</a>]
                            [<a href="https://github.com/voidism/invrat_debias" target="_blank">code</a>]
                            [<a href="https://www.youtube.com/watch?v=bAX8oL_ywpQ" target="_blank">video</a>]
                            <!-- [<a href="https://leaderboard.allenai.org/quoref" target="_blank">leaderboard</a>] -->
                            <div id="chuang2021mitigating_abstract" class="abstract" style="display:none;">
                                <p>
                                    Automatic detection of toxic language plays an essential role in protecting social media users, especially minority groups, from verbal abuse. However, biases toward some attributes, including gender, race, and dialect, exist in most training datasets for toxicity detection. The biases make the learned models unfair and can even exacerbate the marginalization of people. Considering that current debiasing methods for general natural language understanding tasks cannot effectively mitigate the biases in the toxicity detectors, we propose to use invariant rationalization (InvRat), a game-theoretic framework consisting of a rationale generator and a predictor, to rule out the spurious correlation of certain syntactic patterns (e.g., identity mentions, dialect) to toxicity labels. We empirically show that our method yields lower false positive rate in both lexical and dialectal attributes than previous debiasing methods.
                                </p>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <a href="https://arxiv.org/abs/2010.13826" target="_blank">
                                <b>Semi-Supervised Spoken Language Understanding via Self-Supervised Speech and Language Model Pretraining</b>
                            </a>
                            <br/>
                            <a href="http://people.csail.mit.edu/clai24/" target="_blank">Cheng-I Lai</a>,
                            <b>Yung-Sung Chuang</b>,
                            <a href="https://speech.ee.ntu.edu.tw/~hylee/index.html" target="_blank">Hung-Yi Lee</a>,
                            <a href="https://swdanielli.github.io/" target="_blank">Shang-Wen Li</a>,
                            <a href="https://www.csail.mit.edu/person/jim-glass" target="_blank">James Glass</a>.
                            <br/>
                            In <a href="https://2021.ieeeicassp.org/" target="_blank">
                                <b>
                                    IEEE International Conference on Acoustics, Speech and Signal Processing
                                    (ICASSP)</b></a>, 2021.
                            <br/>
                            [<a href="bibs/lai2020semi.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#lai2020semi_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://arxiv.org/pdf/2010.13826.pdf" target="_blank">pdf</a>]
                            [<a href="https://github.com/jefflai108/Semi-Supervsied-Spoken-Language-Understanding-PyTorch" target="_blank">code</a>]
                            [<a href="https://www.youtube.com/watch?v=5GOIPp9hWkY" target="_blank">video</a>]
                            <!-- [<a href="https://leaderboard.allenai.org/quoref" target="_blank">leaderboard</a>] -->
                            <div id="lai2020semi_abstract" class="abstract" style="display:none;">
                                <p>
                                    Much recent work on Spoken Language Understanding (SLU) is limited in at least one of three ways: models were trained on oracle text input and neglected ASR errors, models were trained to predict only intents without the slot values, or models were trained on a large amount of in-house data. In this paper, we propose a clean and general framework to learn semantics directly from speech with semi-supervision from transcribed or untranscribed speech to address these issues. Our framework is built upon pretrained end-to-end (E2E) ASR and self-supervised language models, such as BERT, and fine-tuned on a limited amount of target SLU data. We study two semi-supervised settings for the ASR component: supervised pretraining on transcribed speech, and unsupervised pretraining by replacing the ASR encoder with self-supervised speech representations, such as wav2vec. In parallel, we identify two essential criteria for evaluating SLU models: environmental noise-robustness and E2E semantics evaluation. Experiments on ATIS show that our SLU framework with speech as input can perform on par with those using oracle text as input in semantics understanding, even though environmental noise is present and a limited amount of labeled semantics data is available for training.
                                </p>
                            </div>
                        </li>
                    </ul>
                    <h3>2020</h3>
                    <ul class="pl">
                        <li>
                            <a href="https://arxiv.org/abs/2010.02123" target="_blank">
                                <b>Lifelong Language Knowledge Distillation</b>
                            </a>
                            <br/>
                            <b>Yung-Sung Chuang</b>, <a href="https://www.shangyusu.com/" target="_blank">Shang-Yu Su</a>,
                            <a href="https://www.csie.ntu.edu.tw/~yvchen/" target="_blank">Yun-Nung Chen</a>.
                            <br/>
                            In <a href="https://2020.emnlp.org/" target="_blank">
                                <b>
                                    The Conference on Empirical Methods in Natural Language Processing (EMNLP)</b></a>, 2020.
                            <br/>
                            [<a href="bibs/chuang2020lifelong.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#chuang2020lifelong_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://aclanthology.org/2020.emnlp-main.233.pdf" target="_blank">pdf</a>]
                            [<a href="https://github.com/voidism/L2KD" target="_blank">code</a>]
                            [<a href="https://slideslive.com/38938863/lifelong-language-knowledge-distillation" target="_blank">video</a>]
                            <!-- [<a href="https://leaderboard.allenai.org/quoref" target="_blank">leaderboard</a>] -->
                            <div id="chuang2020lifelong_abstract" class="abstract" style="display:none;">
                                <p>
                                    It is challenging to perform lifelong language learning (LLL) on a stream of different tasks without any performance degradation comparing to the multi-task counterparts. To address this issue, we present Lifelong Language Knowledge Distillation (L2KD), a simple but efficient method that can be easily applied to existing LLL architectures in order to mitigate the degradation. Specifically, when the LLL model is trained on a new task, we assign a teacher model to first learn the new task, and pass the knowledge to the LLL model via knowledge distillation. Therefore, the LLL model can better adapt to the new task while keeping the previously learned knowledge. Experiments show that the proposed L2KD consistently improves previous state-of-the-art models, and the degradation comparing to multi-task models in LLL tasks is well mitigated for both sequence generation and text classification tasks.
                                </p>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <a href="https://arxiv.org/abs/2010.04246" target="_blank">
                                <b>Dual Inference for Improving Language Understanding and Generation</b>
                            </a>
                            <br/>
                            <b>Yung-Sung Chuang*</b>, <a href="https://www.shangyusu.com/" target="_blank">Shang-Yu Su*</a>,
                            <a href="https://www.csie.ntu.edu.tw/~yvchen/" target="_blank">Yun-Nung Chen</a>.
                            <br/>
                            In <a href="https://2020.emnlp.org/" target="_blank">
                                <b>
                                    The Conference on Empirical Methods in Natural Language Processing: Findings (EMNLP)</b></a>, 2020.
                            <br/>
                            [<a href="bibs/su2020dual.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#su2020dual_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://aclanthology.org/2020.findings-emnlp.443.pdf" target="_blank">pdf</a>]
                            [<a href="https://github.com/MiuLab/DuaLUG" target="_blank">code</a>]
                            <!-- [<a href="https://slideslive.com/38938863/lifelong-language-knowledge-distillation" target="_blank">video</a>] -->
                            <!-- [<a href="https://leaderboard.allenai.org/quoref" target="_blank">leaderboard</a>] -->
                            <div id="su2020dual_abstract" class="abstract" style="display:none;">
                                <p>
                                    Natural language understanding (NLU) and Natural language generation (NLG) tasks hold a strong dual relationship, where NLU aims at predicting semantic labels based on natural language utterances and NLG does the opposite. The prior work mainly focused on exploiting the duality in model training in order to obtain the models with better performance. However, regarding the fast-growing scale of models in the current NLP area, sometimes we may have difficulty retraining whole NLU and NLG models. To better address the issue, this paper proposes to leverage the duality in the inference stage without the need of retraining. The experiments on three benchmark datasets demonstrate the effectiveness of the proposed method in both NLU and NLG, providing the great potential of practical usage.
                                </p>
                            </div>
                        </li>
                        <br/>
                        <li>
                            <a href="https://arxiv.org/abs/1910.11559" target="_blank">
                                <b>SpeechBERT: An Audio-and-text Jointly Learned Language Model for End-to-end Spoken Question Answering</b>
                            </a>
                            <br/>
                            <b>Yung-Sung Chuang</b>, <a href="https://liangtaiwan.github.io/" target="_blank">Chi-Liang Liu</a>,
                            <a href="https://speech.ee.ntu.edu.tw/~hylee/index.html" target="_blank"> Hung-Yi Lee</a>, <a href="https://speech.ee.ntu.edu.tw/previous_version/lslNew.htm" target="_blank">Lin-shan Lee</a>.
                            <br/>
                            In <a href="http://www.interspeech2020.org/" target="_blank">
                                <b>
                                    Interspeech</b></a>, 2020. <a style="color:#e22222">(Interspeech 2020 Travel Grant)</a>
                            <br/>
                            [<a href="bibs/chuang2020speechbert.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#chuang2020speechbert_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://www.isca-speech.org/archive/pdfs/interspeech_2020/chuang20b_interspeech.pdf" target="_blank">pdf</a>]
                            <!-- [<a href="https://github.com/MiuLab/DuaLUG" target="_blank">code</a>] -->
                            [<a href="https://www.youtube.com/watch?v=7mf7nSh8dGE" target="_blank">video</a>]
                            <!-- [<a href="https://leaderboard.allenai.org/quoref" target="_blank">leaderboard</a>] -->
                            <div id="chuang2020speechbert_abstract" class="abstract" style="display:none;">
                                <p>
                                    While various end-to-end models for spoken language understanding tasks have been explored recently, this paper is probably the first known attempt to challenge the very difficult task of end-to-end spoken question answering (SQA). Learning from the very successful BERT model for various text processing tasks, here we proposed an audio-and-text jointly learned SpeechBERT model. This model outperformed the conventional approach of cascading ASR with the following text question answering (TQA) model on datasets including ASR errors in answer spans, because the end-to-end model was shown to be able to extract information out of audio data before ASR produced errors. When ensembling the proposed end-to-end model with the cascade architecture, even better performance was achieved. In addition to the potential of end-to-end SQA, the SpeechBERT can also be considered for many other spoken language understanding tasks just as BERT for many text processing tasks.
                                </p>
                            </div>
                        </li>
                    </ul>
                    <h3>2019</h3>
                    <ul class="pl">
                        <li>
                            <a href="https://arxiv.org/abs/1910.01462" target="_blank">
                                <b>Towards Understanding of Medical Randomized Controlled Trials by Conclusion Generation</b>
                            </a>
                            <br/>
                            <a href="https://lipolysis.github.io/" target="_blank">Alexander Te-Wei Shieh*</a>, <b>Yung-Sung Chuang*</b>, <a href="https://www.shangyusu.com/" target="_blank">Shang-Yu Su</a>,
                            <a href="https://www.csie.ntu.edu.tw/~yvchen/" target="_blank">Yun-Nung Chen</a>.
                            <br/>
                            In <a href="https://2019.emnlp.org/" target="_blank">
                                <b>
                                    Proceedings of the Tenth International Workshop on Health Text Mining and Information Analysis (LOUHI 2019) at The 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing</b></a>, 2019.
                            <br/>
                            [<a href="bibs/shieh2019towards.bib" target="_blank">bib</a>]
                            [<a href="#" onclick="$('#shieh2019towards_abstract').toggle();return false;">abstract</a>]
                            [<a href="https://aclanthology.org/D19-6214.pdf" target="_blank">pdf</a>]
                            [<a href="https://github.com/MiuLab/RCT-Gen" target="_blank">code</a>]
                            <!-- [<a href="https://slideslive.com/38938863/lifelong-language-knowledge-distillation" target="_blank">video</a>] -->
                            <!-- [<a href="https://leaderboard.allenai.org/quoref" target="_blank">leaderboard</a>] -->
                            <div id="shieh2019towards_abstract" class="abstract" style="display:none;">
                                <p>
                                    Randomized controlled trials (RCTs) represent the paramount evidence of clinical medicine. Using machines to interpret the massive amount of RCTs has the potential of aiding clinical decision-making. We propose a RCT conclusion generation task from the PubMed 200k RCT sentence classification dataset to examine the effectiveness of sequence-to-sequence models on understanding RCTs. We first build a pointer-generator baseline model for conclusion generation. Then we fine-tune the state-of-the-art GPT-2 language model, which is pre-trained with general domain data, for this new medical domain task. Both automatic and human evaluation show that our GPT-2 fine-tuned models achieve improved quality and correctness in the generated conclusions compared to the baseline pointer-generator model. Further inspection points out the limitations of this current approach and future directions to explore.
                                </p>
                            </div>
                        </li>
                    </ul>
                </div>
            </div>
            <hr>
            <div class="row" id="talks">
                <div class="col">
                    <h2>Talks</h2>
                    <ul>
                        <li>
                            <b>Lectures (in Mandarin)</b><br/>
                            <a href="https://www.youtube.com/watch?v=jvyKmU4OM3c" target="_blank">
                                <b>Non-Autoregressive Conditional Sequence Modeling (DLHLP 2020 @ NTU, Taiwan)</b>
                            </a>
                            <br/>
                            <iframe width="560" height="315" src="https://www.youtube.com/embed/jvyKmU4OM3c" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                            <br/>
                            <a href="https://www.youtube.com/watch?v=YIuBHB9Ejok" target="_blank">
                                <b>Unsupervised Syntactic Parsing (Machine Learning 2019 @ NTU, Taiwan)</b>
                            </a>
                            <br/>
                            <iframe width="560" height="315" src="https://www.youtube.com/embed/YIuBHB9Ejok" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                        </li>
                        <li>
                            <b>Presentations</b><br/>
                            <a href="https://www.youtube.com/watch?v=9vx-HyzcXtU" target="_blank">
                                <b>[NAACL 2022] DiffCSE: Difference-based Contrastive Learning for Sentence Embeddings @ Seattle, WA</b>
                            </a>
                            <br/>
                            <iframe width="560" height="315" src="https://www.youtube.com/embed/9vx-HyzcXtU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                            <br/>
                            <a href="https://www.youtube.com/watch?v=bAX8oL_ywpQ" target="_blank">
                                <b>[ACL-IJCNLP 2021] Mitigating Biases in Toxic Language Detection through Invariant Rationalization @ Virtual</b>
                            </a>
                            <br/>
                            <iframe width="560" height="315" src="https://www.youtube.com/embed/bAX8oL_ywpQ" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                            <br/>
                            <a href="https://www.youtube.com/watch?v=I80xwWfswl8" target="_blank">
                                <b>[ACL-IJCNLP 2021] Investigating the Reordering Capability in CTC-based Non-Autoregressive End-to-End Speech Translation @ Virtual</b>
                            </a>
                            <br/>
                            <iframe width="560" height="315" src="https://www.youtube.com/embed/I80xwWfswl8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                            <!-- <br/>
                            <a href="https://www.youtube.com/watch?v=t3Ee5fA8mCo" target="_blank">
                                <b>[EMNLP 2020] Lifelong Language Knowledge Distillation</b>
                            </a>
                            <br/>
                            <iframe width="560" height="315" src="https://www.youtube.com/embed/t3Ee5fA8mCo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                            <br/>
                            <a href="https://www.youtube.com/watch?v=a6txVSmX7fI" target="_blank">
                                <b>[Interspeech 2020] SpeechBERT: An Audio-and-text Jointly Learned Language Model for End-to-end Spoken Question Answering</b>
                            </a>
                            <br/>
                            <iframe width="560" height="315" src="https://www.youtube.com/embed/a6txVSmX7fI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->
                        </li>
                    </ul>
                </div>
            </div>
            <hr>

            <div class="row" id="projects">
                <div class="col">
                    <h2>Projects</h2>
                    <ul>
                        <li>
                            <a href="https://drive.google.com/file/d/199N-2Up1SFS_BtM_nRVw8yCiYuepw1bg/view" target="_blank">
                                <b>Speech Recognition for Impaired Speaker</b>
                            </a>
                            <br/>
                            <!-- [<a href="#" onclick="$('#sris_abstract').toggle();return false;">abstract</a>] -->
                            [<a href="https://drive.google.com/file/d/199N-2Up1SFS_BtM_nRVw8yCiYuepw1bg/view" target="_blank">pdf</a>]
                            <!-- [<a href="https://aclanthology.org/2020.findings-emnlp.443.pdf" target="_blank">pdf</a>] -->
                            [<a href="https://github.com/voidism/ImpairedVoiceASR" target="_blank">code</a>]
                            <!-- [<a href="https://slideslive.com/38938863/lifelong-language-knowledge-distillation" target="_blank">video</a>] -->
                            <!-- [<a href="https://leaderboard.allenai.org/quoref" target="_blank">leaderboard</a>] -->
                            <div>
                                <p>
                                    Reducing WER from 80% to 20% for impaired voice speaker via personalized adaptation (in Mandarin). Final Project in <i>Introduction to Biomedical Engineering 2020 Spring</i>@NTU.
                                </p>
                            </div>
                        </li>
                        <li>
                            <a href="https://github.com/bchao1/108-2-NMLAB-Final" target="_blank">
                                <b>DPP: Decentralized Publishing Platform</b>
                            </a>
                            <br/>
                            <!-- [<a href="#" onclick="$('#sris_abstract').toggle();return false;">abstract</a>] -->
                            <!-- [<a href="https://drive.google.com/file/d/199N-2Up1SFS_BtM_nRVw8yCiYuepw1bg/view" target="_blank">pdf</a>] -->
                            <!-- [<a href="https://aclanthology.org/2020.findings-emnlp.443.pdf" target="_blank">pdf</a>] -->
                            [<a href="https://github.com/bchao1/108-2-NMLAB-Final" target="_blank">code</a>]
                            <!-- [<a href="https://slideslive.com/38938863/lifelong-language-knowledge-distillation" target="_blank">video</a>] -->
                            <!-- [<a href="https://leaderboard.allenai.org/quoref" target="_blank">leaderboard</a>] -->
                            <div>
                                <p>
                                    A Decentralized Publishing Platform created with Blockchain and Etheruem smart contract. Final Project in <i>Networking and Multinmedia Lab 2020 Spring</i>@NTU.
                                </p>
                            </div>
                        </li>
                        <li>
                            <a href="https://github.com/w4n9r3ntru3/MIPS-Processor" target="_blank">
                                <b>Single-Cycle MIPS Processor Implementation in Verilog</b>
                            </a>
                            <br/>
                            <!-- [<a href="#" onclick="$('#sris_abstract').toggle();return false;">abstract</a>] -->
                            <!-- [<a href="https://drive.google.com/file/d/199N-2Up1SFS_BtM_nRVw8yCiYuepw1bg/view" target="_blank">pdf</a>] -->
                            <!-- [<a href="https://aclanthology.org/2020.findings-emnlp.443.pdf" target="_blank">pdf</a>] -->
                            [<a href="https://github.com/w4n9r3ntru3/MIPS-Processor" target="_blank">code</a>]
                            <!-- [<a href="https://slideslive.com/38938863/lifelong-language-knowledge-distillation" target="_blank">video</a>] -->
                            <!-- [<a href="https://leaderboard.allenai.org/quoref" target="_blank">leaderboard</a>] -->
                            <div>
                                <p>
                                    Ranking 2nd place out of 44 groups by A*T value (Area * Clock Time). Final Project in <i>Computer Architecture 2019 Fall</i>@NTU.
                                </p>
                            </div>
                        </li>
                        <li>
                            <a href="https://github.com/raywu0123/ICCAD2019-Problem-E" target="_blank">
                                <b>Rectilinear Polygon Operations for Physical Design</b>
                            </a>
                            <br/>
                            <!-- [<a href="#" onclick="$('#sris_abstract').toggle();return false;">abstract</a>] -->
                            <!-- [<a href="https://drive.google.com/file/d/199N-2Up1SFS_BtM_nRVw8yCiYuepw1bg/view" target="_blank">pdf</a>] -->
                            <!-- [<a href="https://aclanthology.org/2020.findings-emnlp.443.pdf" target="_blank">pdf</a>] -->
                            [<a href="https://github.com/raywu0123/ICCAD2019-Problem-E" target="_blank">code</a>]
                            <!-- [<a href="https://slideslive.com/38938863/lifelong-language-knowledge-distillation" target="_blank">video</a>] -->
                            <!-- [<a href="https://leaderboard.allenai.org/quoref" target="_blank">leaderboard</a>] -->
                            <div>
                                <p>
                                    <a href="http://iccad-contest.org/2019/tw/problems.html" target="_blank">ICCAD 2019 CAD Contest - Problem E</a>. Final Project in <i>Algorithms 2019 Spring</i>@NTU.
                                </p>
                            </div>
                        </li>
                        <li>
                            <a href="https://voidism.github.io/pdfs/2019_DLCV_Final_Project_Poster_(final).pdf" target="_blank">
                                <b>Multi-Source Unsupervised Domain Adaptation Challenge</b>
                            </a>
                            <br/>
                            <!-- [<a href="#" onclick="$('#sris_abstract').toggle();return false;">abstract</a>] -->
                            <!-- [<a href="https://drive.google.com/file/d/199N-2Up1SFS_BtM_nRVw8yCiYuepw1bg/view" target="_blank">pdf</a>] -->
                            <!-- [<a href="https://aclanthology.org/2020.findings-emnlp.443.pdf" target="_blank">pdf</a>] -->
                            [<a href="https://voidism.github.io/pdfs/2019_DLCV_Final_Project_Poster_(final).pdf" target="_blank">pdf</a>]
                            <!-- [<a href="https://slideslive.com/38938863/lifelong-language-knowledge-distillation" target="_blank">video</a>] -->
                            <!-- [<a href="https://leaderboard.allenai.org/quoref" target="_blank">leaderboard</a>] -->
                            <div>
                                <p>
                                    Conducted experiments on unsupervised domain adaptation (UDA) for multi-source dataset from ICCV2019 Workshop Challenge. Final Project in <i>Deep Learning for Computer Vision 2019 Spring</i>@NTU.
                                </p>
                            </div>
                        </li>
                        <!-- <li> -->
                            <!-- <a href="https://github.com/voidism/Transformer_CycleGAN_Text_Style_Transfer-pytorch" target="_blank"> -->
                                <!-- <b>Transformer-based CycleGAN Text Style Transfer</b> -->
                            <!-- </a> -->
                            <!-- <br/> -->
                            <!-- [<a href="#" onclick="$('#sris_abstract').toggle();return false;">abstract</a>] -->
                            <!-- [<a href="https://drive.google.com/file/d/199N-2Up1SFS_BtM_nRVw8yCiYuepw1bg/view" target="_blank">pdf</a>] -->
                            <!-- [<a href="https://aclanthology.org/2020.findings-emnlp.443.pdf" target="_blank">pdf</a>] -->
                            <!-- [<a href="https://github.com/voidism/Transformer_CycleGAN_Text_Style_Transfer-pytorch" target="_blank">pdf</a>] -->
                            <!-- [<a href="https://slideslive.com/38938863/lifelong-language-knowledge-distillation" target="_blank">video</a>] -->
                            <!-- [<a href="https://leaderboard.allenai.org/quoref" target="_blank">leaderboard</a>] -->
                            <!-- <div> -->
                                <!-- <p> -->
                                    <!-- Building a transformer-based cycleGAN style transfer framework for text generation. -->
                                <!-- </p> -->
                            <!-- </div> -->
                        <!-- </li> -->
                        <li>
                            <a href="https://arxiv.org/abs/1901.05816" target="_blank">
                                <b>Robust Chinese Word Segmentation with Contextualized Word Representations</b>
                            </a>
                            <br/>
                            <!-- [<a href="#" onclick="$('#sris_abstract').toggle();return false;">abstract</a>] -->
                            <!-- [<a href="https://drive.google.com/file/d/199N-2Up1SFS_BtM_nRVw8yCiYuepw1bg/view" target="_blank">pdf</a>] -->
                            [<a href="https://arxiv.org/pdf/1901.05816.pdf" target="_blank">pdf</a>]
                            [<a href="https://pypi.org/project/pywordseg/" target="_blank">pypi</a>]
                            [<a href="https://github.com/voidism/pywordseg" target="_blank">code</a>]
                            <!-- [<a href="https://leaderboard.allenai.org/quoref" target="_blank">leaderboard</a>] -->
                            <div>
                                <p>
                                    Developed an open source state-of-the-art Chinese word segmentation system with BiLSTM and ELMo, helping the downstream Chinese NLP task. Final project in <i>Digital Speech Processing 2018 Fall</i>@NTU.
                                </p>
                            </div>
                        </li>
                        <li>
                            <a href="https://github.com/voidism/Big-Two" target="_blank">
                                <b>Big Two Game Environment and Agent in C++</b>
                            </a>
                            <br/>
                            <!-- [<a href="#" onclick="$('#sris_abstract').toggle();return false;">abstract</a>] -->
                            <!-- [<a href="https://drive.google.com/file/d/199N-2Up1SFS_BtM_nRVw8yCiYuepw1bg/view" target="_blank">pdf</a>] -->
                            <!-- [<a href="https://arxiv.org/pdf/1901.05816.pdf" target="_blank">pdf</a>] -->
                            <!-- [<a href="https://pypi.org/project/pywordseg/" target="_blank">pypi</a>] -->
                            [<a href="https://github.com/voidism/Big-Two" target="_blank">code</a>]
                            <!-- [<a href="https://leaderboard.allenai.org/quoref" target="_blank">leaderboard</a>] -->
                            <div>
                                <p>
                                    Developed a human-computer game program of the big-two game. Final Project in <i>Computer Programming 2017 Fall</i>@NTU.
                                </p>
                            </div>
                        </li>
                    </ul>
                </div>
            </div>
            <hr>
            <div class="row" id="honors">
                <div class="col">
                    <h2>Honors</h2>
                    <ul>
                        <li>
                            <b>Dean’s list (4 times)</b>, Electrical Engineering Dept. at NTU, Spring ’18, Spring ’19, Fall ’19, Spring ’20
                        </li>
                        <li>
                            <b>Irving T. Ho Memorial Scholarship (2 times)</b>, EECS at NTU, Fall ’18, Fall ’19
                        </li>
                        <li>
                            <b>Travel Grant</b>, INTERSPEECH 2020 conference, Sep. 2020
                        </li>
                        <li>
                            <b>Appier Best Application Award</b>, 2020 NTU CSIE Undergrad Special Research Exhibition, Jun. 2020
                        </li>
                        <li>
                            <b>2nd Place & Appier 1st Award</b>, 2019 NTU CSIE Undergrad Special Research Exhibition, Jun. 2019
                        </li>
                        <li>
                            <b>2nd Place</b>, 2019 NTUEE Undergraduate Innovation Award, Jun. 2019
                        </li>
                        <li>
                            <b>1st Place</b>, 2018 H. Spectrum Demo Day (out of 21 teams), Jul. 2018
                        </li>
                        <li>
                            <b>1st Place</b>, NCTS Health Hackathon 2018 (out of 18 teams), Jun. 2018
                        </li>
                        <li>
                            <b>Top 8 Finalist</b>, Microsoft Imagine Cup Taiwan National Final 2018, Apr. 2018
                        </li>
                        <li>
                            <b>Best Tech Award & Microsoft Enterprise Award</b>, MakeNTU 2018 (out of 50 teams), Mar. 2018
                        </li>
                        <li>
                            <b>1st place of Dept. of Transportation</b>, HackNTU 2017 (out of 100+ teams), Jul. 2017
                        </li>
                        <!-- <li>
                            <b>Teaching Assistant:</b> <br/>
                              Deep Learning for Human Language Processing, 2020 Spring@NTU (Prof. Hung-Yi Lee)
                        </li> -->
                    </ul>
                </div>
            </div>
            <hr>
            <div class="row" id="services">
                <div class="col">
                    <h2>Services</h2>
                    <ul>
                        <li>
                            <b>Reviewer:</b> <br/>
                              NeurIPS 2021, 2022, 2023<br/>
                              ICLR 2022, 2023<br/>
                              ICML 2022, 2023<br/>
                              EMNLP 2022, 2023<br/>
                              ACL 2023<br/>
                              AAAI 2023<br/>
                              ICASSP 2022 2023<br/>
                              TASL 2023<br/>
                        </li>
                        <!-- <li>
                            <b>Teaching Assistant:</b> <br/>
                              Deep Learning for Human Language Processing, 2020 Spring@NTU (Prof. Hung-Yi Lee)
                        </li> -->
                    </ul>
                </div>
            </div>
            <hr>
            <div class="row">
                <div class="col">
                    <!-- <h2>Miscellany</h2> -->
                    <ul>
                        <li>
                            This website is built from the <a href="https://github.com/nelson-liu/website">source code</a> of Nelson F. Liu's awesome website (<a href="https://nelsonliu.me/"
                            target="_blank">https://nelsonliu.me</a>).
                        </li>
                    </ul>
                </div>
            </div>
            <footer class="pt-2 my-md-2 pt-md-2 border-top">
                <div class="row justify-content-center">
                    <div class="col-6 col-md text-left align-self-center">
                        <p class="h5 text-muted">
                            © Yung-Sung Chuang, 2022
                        </p>
                    </div>
                    <div class="col-6 col-md text-right">
                        <a href="https://www.csail.mit.edu/" class="image-link">
                            <img class="mr-4" src="img/mit_csail_logo.svg" alt="MIT CSAIL logo." height="75">
                        </a>
                        <a href="https://www.mit.edu/" class="image-link">
                            <img src="img/mit_logo.svg" alt="MIT logo." height="50">
                        </a>
                    </div>
                </div>
            </footer>
        </div>
    </body>
</html>
